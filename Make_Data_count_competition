{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82370,"databundleVersionId":12656064,"sourceType":"competition"},{"sourceId":245382542,"sourceType":"kernelVersion"},{"sourceId":363124,"sourceType":"modelInstanceVersion","modelInstanceId":301506,"modelId":322000},{"sourceId":391615,"sourceType":"modelInstanceVersion","modelInstanceId":322452,"modelId":322000}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup and Imports\n- Copies `mdc_fine_grained_eval.py` for evaluation.\n- Imports libraries for PDF (`fitz`), XML (`lxml`), LLM (`vllm`), and data processing (`pandas`, `numpy`).\n- Ensures compatibility with Kaggle's GPU environment.","metadata":{}},{"cell_type":"code","source":"import fitz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:23:46.898199Z","iopub.execute_input":"2025-06-20T16:23:46.898675Z","iopub.status.idle":"2025-06-20T16:23:46.902034Z","shell.execute_reply.started":"2025-06-20T16:23:46.898653Z","shell.execute_reply":"2025-06-20T16:23:46.901309Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport re\ntry:\n    import fitz  # PyMuPDF\nexcept ImportError:\n    raise ImportError(\"PyMuPDF not installed. Please run '!pip install pymupdf' first.\")\nimport pandas as pd\nimport pickle\nimport json\nimport numpy as np\nfrom tqdm.auto import tqdm\ntry:\n    import vllm\n    import torch\nexcept ImportError:\n    print(\"vLLM not installed. Using rule-based classifier only.\")\ntry:\n    from lxml import etree\nexcept ImportError:\n    raise ImportError(\"lxml not installed. Please run '!pip install lxml' first.\")\ntry:\n    from mdc_fine_grained_eval import compute_metrics\n    eval_available = True\nexcept ImportError:\n    print(\"mdc_fine_grained_eval.py not found. Skipping evaluation for training mode.\")\n    eval_available = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:23:51.250496Z","iopub.execute_input":"2025-06-20T16:23:51.250782Z","iopub.status.idle":"2025-06-20T16:24:16.912715Z","shell.execute_reply.started":"2025-06-20T16:23:51.250762Z","shell.execute_reply":"2025-06-20T16:24:16.912157Z"}},"outputs":[{"name":"stdout","text":"INFO 06-20 16:24:01 [__init__.py:244] Automatically detected platform cuda.\n","output_type":"stream"},{"name":"stderr","text":"2025-06-20 16:24:03.149422: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750436643.302334      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750436643.346946      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Read PDFs and XMLs\n- Processes PDFs in `train/PDF` or `test/PDF`, stopping at \"References\" to avoid irrelevant DOIs.\n- Parses corresponding XML files in `train/XML` or `test/XML` to extract DOIs and accession IDs.\n- Stores results as `(article_id, pdf_text, xml_metadata)` tuples.\n- Handles cases where XML files are missing (~25% of articles).","metadata":{}},{"cell_type":"code","source":"# Define directories\nbase_dir = \"/kaggle/input/make-data-count-finding-data-references\"\npdf_dir = f\"{base_dir}/test/PDF\" if os.getenv('KAGGLE_IS_COMPETITION_RERUN') else f\"{base_dir}/train/PDF\"\nxml_dir = f\"{base_dir}/test/XML\" if os.getenv('KAGGLE_IS_COMPETITION_RERUN') else f\"{base_dir}/train/XML\"\n\ntexts = []\nfor filename in tqdm(os.listdir(pdf_dir), desc=\"Reading PDFs and XMLs\"):\n    if filename.endswith(\".pdf\"):\n        article_id = filename.split(\".pdf\")[0]\n        pdf_path = os.path.join(pdf_dir, filename)\n        xml_path = os.path.join(xml_dir, filename.replace(\".pdf\", \".xml\"))\n        \n        # Read PDF\n        pdf_text = \"\"\n        try:\n            doc = fitz.open(pdf_path)\n            for page in doc:\n                page_text = page.get_text().lower()\n                if 'references' in page_text:\n                    pdf_text += page_text.split(\"references\")[0]\n                    break\n                pdf_text += page_text\n            doc.close()\n        except Exception as e:\n            print(f\"Error processing PDF {filename}: {e}\")\n            continue\n        \n        # Read XML\n        xml_metadata = \"\"\n        if os.path.exists(xml_path):\n            try:\n                tree = etree.parse(xml_path)\n                root = tree.getroot()\n                for elem in root.iter('{*}article-id', '{*}ref', '{*}data'):\n                    if elem.text:\n                        xml_metadata += elem.text.lower() + \" \"\n                    for attr in elem.attrib.values():\n                        xml_metadata += attr.lower() + \" \"\n            except Exception as e:\n                print(f\"Error processing XML {filename.replace('.pdf', '.xml')}: {e}\")\n        \n        texts.append((article_id, pdf_text, xml_metadata))\n\nprint(f\"Processed {len(texts)} articles\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:24:20.673594Z","iopub.execute_input":"2025-06-20T16:24:20.674681Z","iopub.status.idle":"2025-06-20T16:25:40.435027Z","shell.execute_reply.started":"2025-06-20T16:24:20.674654Z","shell.execute_reply":"2025-06-20T16:25:40.433986Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Reading PDFs and XMLs:   0%|          | 0/524 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e620b19b85624f7981dd4892abccacfa"}},"metadata":{}},{"name":"stdout","text":"MuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nProcessed 524 articles\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Extract Identifier Contexts\n- Uses regex to detect DOIs and accession IDs (GEO: `GSE\\d+`, PDB: `PDB\\s+[0-9A-Z]{4}`, ArrayExpress: `E-[A-Z]{4}-\\d+`).\n- Extracts a 400-character context window around each identifier.\n- Stores results as `(article_id, identifier, id_type, context)` tuples.\n- Filters out identifiers matching the article ID.","metadata":{}},{"cell_type":"code","source":"# Cell 4: Extract Identifier Contexts\nchunks = []\ndoi_pattern = r\"10\\.\\d{4,}/[^\\s]+\"  # DOI regex\naccession_patterns = [\n    r\"GSE\\d{5,}\",  # GEO datasets\n    r\"PDB\\s+[0-9A-Z]{4}\",  # PDB datasets\n    r\"E-[A-Z]{4}-\\d+\"  # ArrayExpress datasets\n]\n\nskipped_identifiers = []\nfor article_id, pdf_text, xml_metadata in tqdm(texts, desc=\"Extracting identifiers\"):\n    combined_text = pdf_text + \" \" + xml_metadata\n    article_doi = article_id.lower() if article_id.startswith(\"10.\") else None\n    \n    # Extract DOIs\n    for match in re.finditer(doi_pattern, combined_text, re.IGNORECASE):\n        identifier = match.group().lower()\n        # Skip if identifier matches article_id or is part of article_doi\n        if identifier == article_id.lower() or (article_doi and identifier.startswith(article_doi)):\n            skipped_identifiers.append((article_id, identifier, \"self-referential DOI\"))\n            continue\n        start = max(0, match.start() - 200)\n        end = match.start() + 200\n        context = combined_text[start:end].strip()\n        if len(context) < 10:  # Skip short contexts\n            skipped_identifiers.append((article_id, identifier, \"context too short\"))\n            continue\n        chunks.append((article_id, identifier, \"doi\", context))\n    \n    # Extract accession IDs\n    for pattern in accession_patterns:\n        for match in re.finditer(pattern, combined_text, re.IGNORECASE):\n            identifier = match.group().lower()\n            start = max(0, match.start() - 200)\n            end = match.start() + 200\n            context = combined_text[start:end].strip()\n            if len(context) < 10:\n                skipped_identifiers.append((article_id, identifier, \"context too short\"))\n                continue\n            chunks.append((article_id, identifier, \"accession\", context))\n\n# Log skipped identifiers\nif skipped_identifiers:\n    print(f\"Skipped {len(skipped_identifiers)} identifiers:\")\n    for article_id, identifier, reason in skipped_identifiers[:5]:  # Log first 5 for brevity\n        print(f\"Article {article_id}: {identifier} ({reason})\")\n    if len(skipped_identifiers) > 5:\n        print(f\"...and {len(skipped_identifiers) - 5} more\")\n\n# Validate chunks\nvalid_chunks = [chunk for chunk in chunks if len(chunk) == 4 and isinstance(chunk[3], str) and chunk[3].strip()]\nif not valid_chunks:\n    print(\"Error: No valid chunks extracted. Check regex patterns or input data.\")\nelse:\n    print(f\"Extracted {len(valid_chunks)} valid identifier contexts\")\n\n# Save chunks\nwith open('chunks.pkl', 'wb') as file:\n    pickle.dump(valid_chunks, file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:34:24.319732Z","iopub.execute_input":"2025-06-20T16:34:24.320417Z","iopub.status.idle":"2025-06-20T16:34:25.071653Z","shell.execute_reply.started":"2025-06-20T16:34:24.320392Z","shell.execute_reply":"2025-06-20T16:34:25.071029Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Extracting identifiers:   0%|          | 0/524 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59875fc9f9534d7bba5f3e251d78708a"}},"metadata":{}},{"name":"stdout","text":"Extracted 3676 valid identifier contexts\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Rule-Based Classifier\n- Classifies identifiers as Primary, Secondary, or Not Relevant using keyword patterns.\n- Primary: Keywords like \"generated,\" \"produced,\" \"this study.\"\n- Secondary: Keywords like \"obtained from,\" \"available at,\" \"reused.\"\n- Not Relevant: Keywords like \"reference,\" \"citation,\" or lack of data context.\n- Used as a fallback if LLM fails or for ensemble scoring.","metadata":{}},{"cell_type":"code","source":"def rule_based_classifier(context):\n    context = context.lower()\n    primary_keywords = ['generated', 'produced', 'this study', 'our data', 'collected']\n    secondary_keywords = ['obtained from', 'available at', 'reused', 'derived', 'accessed']\n    not_relevant_keywords = ['reference', 'citation', 'bibliography']\n    \n    if any(keyword in context for keyword in not_relevant_keywords):\n        return \"Not Relevant\", 0.9\n    elif any(keyword in context for keyword in primary_keywords):\n        return \"Primary\", 0.8\n    elif any(keyword in context for keyword in secondary_keywords):\n        return \"Secondary\", 0.8\n    return \"Not Relevant\", 0.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:26:36.353770Z","iopub.execute_input":"2025-06-20T16:26:36.354437Z","iopub.status.idle":"2025-06-20T16:26:36.359089Z","shell.execute_reply.started":"2025-06-20T16:26:36.354417Z","shell.execute_reply":"2025-06-20T16:26:36.358431Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Cell 7: LLM Classification\nprompt_template = \"\"\"\nYou are given a piece of academic text containing a dataset identifier (DOI or accession ID). Your task is to:\n\n1. Identify the single dataset identifier (DOI or accession ID).\n2. Normalize DOIs to full URL format (https://doi.org/...). Keep accession IDs as-is (e.g., GSE12345).\n3. Classify the data associated with the identifier as:\n   - \"Primary\": data generated specifically for this study.\n   - \"Secondary\": data reused or derived from prior work.\n   - \"Not Relevant\": identifier is in References, not research data, or unrelated.\n4. Provide a confidence score (0.0 to 1.0) for the classification.\n\nIf no valid identifier is found, return an empty JSON object {}.\n\nReturn ONE dictionary within JSON backticks with three keys:\n```json\n{\n    \"dataset_id\": \"<normalized identifier>\",\n    \"classification\": \"<Primary, Secondary, or Not Relevant>\",\n    \"confidence\": <float between 0.0 and 1.0>\n}\n```\n\nAcademic Text: %s\n\"\"\"\n\n# Generate prompts with robust formatting\nprompts = []\nprompt_issues = []\nfor idx, chunk in enumerate(valid_chunks):\n    try:\n        context = chunk[3].strip()\n        if not context:\n            prompt_issues.append((idx, chunk, \"empty context\"))\n            prompts.append(\"\")\n            continue\n        \n        # Escape special characters and use % operator\n        prompt = prompt_template % context.replace('%', '%%')\n        prompts.append(prompt)\n    except Exception as e:\n        prompt_issues.append((idx, chunk, f\"formatting error: {e}\"))\n        prompts.append(\"\")\n\n# Log prompt issues\nif prompt_issues:\n    print(f\"Encountered {len(prompt_issues)} prompt generation issues:\")\n    for idx, chunk, reason in prompt_issues[:5]:  # Log first 5\n        print(f\"Chunk {idx}: {reason}\")\n    if len(prompt_issues) > 5:\n        print(f\"...and {len(prompt_issues) - 5} more\")\n\n# Filter valid prompts\nvalid_prompts = [p for p in prompts if p.strip()]\nif not valid_prompts:\n    print(\"Warning: No valid prompts generated. Using rule-based classifier for all chunks.\")\n\n# Initialize responses\nresponses = []\nllm_success_count = 0\nif llm_available and valid_prompts:\n    try:\n        responses = llm.generate(\n            valid_prompts,\n            vllm.SamplingParams(\n                n=1,\n                top_p=0.8,\n                temperature=0,\n                seed=777,\n                skip_special_tokens=False,\n                max_tokens=256,\n            ),\n            use_tqdm=True\n        )\n        responses = [(resp.outputs[0].text.lower(), 1.0) for resp in responses]\n        llm_success_count = len(responses)\n    except Exception as e:\n        print(f\"LLM inference failed: {e}. Using rule-based classifier.\")\n        responses = [(None, 0.0) for _ in valid_prompts]\nelse:\n    print(\"LLM unavailable or no valid prompts. Using rule-based classifier.\")\n    responses = [(None, 0.0) for _ in valid_prompts]\n\n# Align responses with valid_chunks\nfull_responses = [(None, 0.0) for _ in valid_chunks]\nprompt_idx = 0\nfor idx, prompt in enumerate(prompts):\n    if prompt.strip() and prompt_idx < len(responses):\n        full_responses[idx] = responses[prompt_idx]\n        prompt_idx += 1\n\n# Fallback to rule-based classifier\nfinal_responses = []\nfor idx, ((article_id, identifier, id_type, context), (llm_response, llm_conf)) in enumerate(zip(valid_chunks, full_responses)):\n    if llm_response and llm_conf > 0.5:\n        final_responses.append((llm_response, llm_conf))\n    else:\n        classification, conf = rule_based_classifier(context)\n        normalized_id = f\"https://doi.org/{identifier}\" if id_type == \"doi\" else identifier\n        json_response = f\"\"\"```json\n{{\n    \"dataset_id\": \"{normalized_id}\",\n    \"classification\": \"{classification}\",\n    \"confidence\": {conf}\n}}\n```\"\"\"\n        final_responses.append((json_response, conf))\n\nprint(f\"LLM succeeded for {llm_success_count}/{len(valid_chunks)} chunks; used rule-based classifier for {len(valid_chunks) - llm_success_count}\")\n\n# Save final_responses\nwith open('responses.pkl', 'wb') as file:\n    pickle.dump(final_responses, file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:36:11.845161Z","iopub.execute_input":"2025-06-20T16:36:11.845551Z","iopub.status.idle":"2025-06-20T17:54:38.613497Z","shell.execute_reply.started":"2025-06-20T16:36:11.845529Z","shell.execute_reply":"2025-06-20T17:54:38.612683Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/3676 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2e8140d1c8e48b3b5503d7a7331a26b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/3676 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d7a812d9f614a31a1a33b179477ec3a"}},"metadata":{}},{"name":"stdout","text":"WARNING 06-20 16:36:50 [scheduler.py:1801] Sequence group 113 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\nWARNING 06-20 16:38:54 [scheduler.py:1801] Sequence group 166 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\nWARNING 06-20 16:42:34 [scheduler.py:1801] Sequence group 351 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\nWARNING 06-20 16:45:11 [scheduler.py:1801] Sequence group 424 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\nWARNING 06-20 16:48:28 [scheduler.py:1801] Sequence group 609 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\nWARNING 06-20 16:51:55 [scheduler.py:1801] Sequence group 796 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\nWARNING 06-20 16:54:28 [scheduler.py:1801] Sequence group 864 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\nWARNING 06-20 16:57:43 [scheduler.py:1801] Sequence group 1048 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\nWARNING 06-20 17:01:14 [scheduler.py:1801] Sequence group 1228 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\nWARNING 06-20 17:03:49 [scheduler.py:1801] Sequence group 1301 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\nWARNING 06-20 17:07:10 [scheduler.py:1801] Sequence group 1480 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\nWARNING 06-20 17:10:36 [scheduler.py:1801] Sequence group 1661 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\nWARNING 06-20 17:14:07 [scheduler.py:1801] Sequence group 1845 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\nWARNING 06-20 17:16:36 [scheduler.py:1801] Sequence group 1912 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\nWARNING 06-20 17:20:04 [scheduler.py:1801] Sequence group 2081 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\nWARNING 06-20 17:23:30 [scheduler.py:1801] Sequence group 2264 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\nWARNING 06-20 17:26:56 [scheduler.py:1801] Sequence group 2450 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\nWARNING 06-20 17:29:22 [scheduler.py:1801] Sequence group 2516 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\nWARNING 06-20 17:32:42 [scheduler.py:1801] Sequence group 2705 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\nWARNING 06-20 17:36:14 [scheduler.py:1801] Sequence group 2892 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\nWARNING 06-20 17:38:43 [scheduler.py:1801] Sequence group 2959 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\nWARNING 06-20 17:41:56 [scheduler.py:1801] Sequence group 3148 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\nWARNING 06-20 17:44:38 [scheduler.py:1801] Sequence group 3218 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1101\nWARNING 06-20 17:47:50 [scheduler.py:1801] Sequence group 3400 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1151\nWARNING 06-20 17:51:20 [scheduler.py:1801] Sequence group 3589 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1201\nLLM succeeded for 3676/3676 chunks; used rule-based classifier for 0\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"with open('responses.pkl', 'rb') as file:\n    responses = pickle.load(file) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T17:59:01.582060Z","iopub.execute_input":"2025-06-20T17:59:01.582792Z","iopub.status.idle":"2025-06-20T17:59:01.592166Z","shell.execute_reply.started":"2025-06-20T17:59:01.582768Z","shell.execute_reply":"2025-06-20T17:59:01.591568Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import json\njson_block_pattern = r'```json\\s*(.*?)\\s*```'\ncitations = []\nfor (article_id, identifier, id_type, _), (response, conf) in zip(chunks, final_responses):\n    try:\n        match = re.search(json_block_pattern, response, re.DOTALL)\n        if match:\n            json_string = match.group(1).strip()\n            parsed_json = json.loads(json_string)\n            if not parsed_json:\n                continue\n            dataset_id = parsed_json['dataset_id'].replace('\\u200b', '').replace('\\n', '')\n            category = parsed_json['classification'].capitalize()\n            confidence = parsed_json.get('confidence', 0.5)\n            if category in ['Primary', 'Secondary'] and confidence > 0.7:\n                citations.append([article_id, dataset_id, category, confidence])\n    except Exception as e:\n        print(f\"Error parsing response for {article_id}: {e}\")\n\n# Create DataFrame\nsubmission = pd.DataFrame(citations, columns=['article_id', 'dataset_id', 'type', 'confidence'])\n\n# Ensemble: Resolve conflicts by highest confidence\nsubmission = submission.loc[submission.groupby(['article_id', 'dataset_id'])['confidence'].idxmax()]\n\n# Filter frequent dataset IDs\ndataset_id_counts = submission['dataset_id'].value_counts()\nfrequent_dataset_ids = dataset_id_counts[dataset_id_counts >= 3].index\nsubmission = submission[~submission['dataset_id'].isin(frequent_dataset_ids)]\n\n# Sort and deduplicate\nsubmission = submission.sort_values(by=['article_id', 'dataset_id', 'type'], ascending=True)\nsubmission = submission.drop_duplicates(subset=['article_id', 'dataset_id'])\n\n# Assign row IDs\nsubmission['row_id'] = range(len(submission))\n\n# Final submission\nsubmission = submission[['row_id', 'article_id', 'dataset_id', 'type']]\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file 'submission.csv' created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T17:59:03.109670Z","iopub.execute_input":"2025-06-20T17:59:03.110625Z","iopub.status.idle":"2025-06-20T17:59:03.184932Z","shell.execute_reply.started":"2025-06-20T17:59:03.110599Z","shell.execute_reply":"2025-06-20T17:59:03.184206Z"}},"outputs":[{"name":"stdout","text":"Submission file 'submission.csv' created\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T17:59:06.096810Z","iopub.execute_input":"2025-06-20T17:59:06.097334Z","iopub.status.idle":"2025-06-20T17:59:06.121268Z","shell.execute_reply.started":"2025-06-20T17:59:06.097309Z","shell.execute_reply":"2025-06-20T17:59:06.120690Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"     row_id              article_id  \\\n0         0    10.1002_2017jc013030   \n1         1    10.1002_2017jc013030   \n2         2  10.1002_anie.201916483   \n3         3  10.1002_anie.202005531   \n4         4  10.1002_anie.202007717   \n..      ...                     ...   \n850     850     10.7717_peerj.13193   \n851     851     10.7717_peerj.13193   \n852     852     10.7717_peerj.13193   \n853     853     10.7717_peerj.13193   \n854     854     10.7717_peerj.13193   \n\n                                            dataset_id       type  \n0                                 10.1002/2017jc013030  Secondary  \n1                 https://doi.org/10.1002/2017jc013030  Secondary  \n2               https://doi.org/10.1002/anie.201916483  Secondary  \n3               https://doi.org/10.1002/anie.202005531  Secondary  \n4                               10.1002/anie.202007717  Secondary  \n..                                                 ...        ...  \n850  https://doi.org/10.6073/pasta/b23deb8e1ccf1c14...    Primary  \n851                https://doi.org/10.7717/peerj.13193  Secondary  \n852          https://doi.org/10.7717/peerj.13193/fig-2    Primary  \n853          https://doi.org/10.7717/peerj.13193/fig-3    Primary  \n854          https://doi.org/10.7717/peerj.13193/fig-4    Primary  \n\n[855 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>article_id</th>\n      <th>dataset_id</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>10.1002_2017jc013030</td>\n      <td>10.1002/2017jc013030</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>10.1002_2017jc013030</td>\n      <td>https://doi.org/10.1002/2017jc013030</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>10.1002_anie.201916483</td>\n      <td>https://doi.org/10.1002/anie.201916483</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>10.1002_anie.202005531</td>\n      <td>https://doi.org/10.1002/anie.202005531</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>10.1002_anie.202007717</td>\n      <td>10.1002/anie.202007717</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>850</th>\n      <td>850</td>\n      <td>10.7717_peerj.13193</td>\n      <td>https://doi.org/10.6073/pasta/b23deb8e1ccf1c14...</td>\n      <td>Primary</td>\n    </tr>\n    <tr>\n      <th>851</th>\n      <td>851</td>\n      <td>10.7717_peerj.13193</td>\n      <td>https://doi.org/10.7717/peerj.13193</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>852</th>\n      <td>852</td>\n      <td>10.7717_peerj.13193</td>\n      <td>https://doi.org/10.7717/peerj.13193/fig-2</td>\n      <td>Primary</td>\n    </tr>\n    <tr>\n      <th>853</th>\n      <td>853</td>\n      <td>10.7717_peerj.13193</td>\n      <td>https://doi.org/10.7717/peerj.13193/fig-3</td>\n      <td>Primary</td>\n    </tr>\n    <tr>\n      <th>854</th>\n      <td>854</td>\n      <td>10.7717_peerj.13193</td>\n      <td>https://doi.org/10.7717/peerj.13193/fig-4</td>\n      <td>Primary</td>\n    </tr>\n  </tbody>\n</table>\n<p>855 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    pred_df = pd.read_csv(\"submission.csv\")\n    reference_df = pd.read_csv(\"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\")\n    reference_df = reference_df[reference_df['type'] != 'Missing'].reset_index(drop=True)\n    eval_dict = compute_metrics(pred_df, reference_df)\n    m = eval_dict['ents_f1']\n    print(f\"CV @ 524 train PDF = {round(m, 4)}\")\n    print(\"\\n\\n\")\n    print(json.dumps(eval_dict['ents_per_type'], indent=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T17:59:07.069930Z","iopub.execute_input":"2025-06-20T17:59:07.070558Z","iopub.status.idle":"2025-06-20T17:59:07.104389Z","shell.execute_reply.started":"2025-06-20T17:59:07.070534Z","shell.execute_reply":"2025-06-20T17:59:07.103534Z"}},"outputs":[{"name":"stdout","text":"CV @ 524 train PDF = 0.0765\n\n\n\n{\n    \"Secondary\": {\n        \"p\": 0.005649717514124294,\n        \"r\": 0.004454342984409799,\n        \"f1\": 0.0049813200498132\n    },\n    \"Primary\": {\n        \"p\": 0.11693548387096774,\n        \"r\": 0.21481481481481482,\n        \"f1\": 0.1514360313315927\n    }\n}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}