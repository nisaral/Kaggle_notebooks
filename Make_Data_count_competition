{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82370,"databundleVersionId":12656064,"sourceType":"competition"},{"sourceId":245382542,"sourceType":"kernelVersion"},{"sourceId":363124,"sourceType":"modelInstanceVersion","modelInstanceId":301506,"modelId":322000},{"sourceId":391615,"sourceType":"modelInstanceVersion","modelInstanceId":322452,"modelId":322000}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup and Imports\n- Copies `mdc_fine_grained_eval.py` for evaluation.\n- Imports libraries for PDF (`fitz`), XML (`lxml`), LLM (`vllm`), and data processing (`pandas`, `numpy`).\n- Ensures compatibility with Kaggle's GPU environment.","metadata":{}},{"cell_type":"code","source":"!pip install pymupdf lxml vllm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:34:43.346176Z","iopub.execute_input":"2025-06-23T09:34:43.346404Z","iopub.status.idle":"2025-06-23T09:38:37.379925Z","shell.execute_reply.started":"2025-06-23T09:34:43.346379Z","shell.execute_reply":"2025-06-23T09:38:37.379003Z"}},"outputs":[{"name":"stdout","text":"Collecting pymupdf\n  Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.3.1)\nCollecting vllm\n  Downloading vllm-0.9.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from vllm) (2024.11.6)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (7.0.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.4)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\nCollecting blake3 (from vllm)\n  Downloading blake3-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\nRequirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.51.3)\nCollecting huggingface-hub>=0.32.0 (from huggingface-hub[hf_xet]>=0.32.0->vllm)\n  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (3.20.3)\nCollecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n  Downloading fastapi-0.115.13-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.18)\nRequirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.70.0)\nRequirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.4)\nRequirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.1.0)\nCollecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\nCollecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\nCollecting llguidance<0.8.0,>=0.7.11 (from vllm)\n  Downloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting outlines==0.1.11 (from vllm)\n  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\nCollecting lark==1.2.2 (from vllm)\n  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\nCollecting xgrammar==0.1.19 (from vllm)\n  Downloading xgrammar-0.1.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.13.2)\nRequirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\nCollecting partial-json-parser (from vllm)\n  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\nCollecting pyzmq>=25.0.0 (from vllm)\n  Downloading pyzmq-27.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.0 kB)\nCollecting msgspec (from vllm)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting gguf>=0.13.0 (from vllm)\n  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n  Downloading mistral_common-1.6.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\nCollecting compressed-tensors==0.10.1 (from vllm)\n  Downloading compressed_tensors-0.10.1-py3-none-any.whl.metadata (7.0 kB)\nCollecting depyf==0.18.0 (from vllm)\n  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\nCollecting watchfiles (from vllm)\n  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.2)\nRequirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.11.1.4)\nRequirement already satisfied: opentelemetry-sdk>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.31.1)\nRequirement already satisfied: opentelemetry-api>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.31.1)\nCollecting opentelemetry-exporter-otlp>=1.26.0 (from vllm)\n  Downloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-semantic-conventions-ai>=0.4.1 (from vllm)\n  Downloading opentelemetry_semantic_conventions_ai-0.4.9-py3-none-any.whl.metadata (1.1 kB)\nCollecting numba==0.61.2 (from vllm)\n  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\nRequirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (2.46.0)\nCollecting torch==2.7.0 (from vllm)\n  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\nCollecting torchaudio==2.7.0 (from vllm)\n  Downloading torchaudio-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\nCollecting torchvision==0.22.0 (from vllm)\n  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nCollecting xformers==0.0.30 (from vllm)\n  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting astor (from depyf==0.18.0->vllm)\n  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.8)\nCollecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\nCollecting interegular (from outlines==0.1.11->vllm)\n  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\nCollecting diskcache (from outlines==0.1.11->vllm)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\nCollecting pycountry (from outlines==0.1.11->vllm)\n  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\nCollecting airportsdata (from outlines==0.1.11->vllm)\n  Downloading airportsdata-20250622-py3-none-any.whl.metadata (9.1 kB)\nCollecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting sympy>=1.13.3 (from torch==2.7.0->vllm)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->vllm) (3.4.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->vllm) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->vllm)\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->vllm)\n  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->vllm)\n  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->vllm)\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->vllm)\n  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->vllm)\n  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->vllm)\n  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->vllm)\n  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->vllm)\n  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->vllm)\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->vllm)\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->vllm)\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->vllm)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->vllm)\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.3.0 (from torch==2.7.0->vllm)\n  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->vllm) (75.2.0)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nCollecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\nCollecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\nCollecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.32.0->huggingface-hub[hf_xet]>=0.32.0->vllm) (25.0)\nCollecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.32.0->huggingface-hub[hf_xet]>=0.32.0->vllm)\n  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->vllm) (2.4.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.9.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.26.0->vllm) (1.2.18)\nCollecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.26.0->vllm)\n  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\nCollecting opentelemetry-exporter-otlp-proto-grpc==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-exporter-otlp-proto-http==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm) (1.70.0)\nRequirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm) (1.72.0rc1)\nCollecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-sdk>=1.26.0 (from vllm)\n  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\nCollecting protobuf (from vllm)\n  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nCollecting opentelemetry-api>=1.26.0 (from vllm)\n  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.26.0->vllm)\n  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.26.0->vllm) (8.7.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (0.4.0)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\nRequirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.4.26)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (0.5.3)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.20.0)\nRequirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\nRequirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.2)\nCollecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading rich_toolkit-0.14.7-py3-none-any.whl.metadata (999 bytes)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.26.0->vllm) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.24.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0->vllm) (1.3.0)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nCollecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vllm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->vllm) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->vllm) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->vllm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->vllm) (2024.2.0)\nRequirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.0.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.26.0->vllm) (1.17.2)\nDownloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading vllm-0.9.1-cp38-abi3-manylinux1_x86_64.whl (394.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.6/394.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading compressed_tensors-0.10.1-py3-none-any.whl (116 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\nDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchaudio-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xgrammar-0.1.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m957.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.13-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gguf-0.17.1-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.8/514.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mistral_common-1.6.2-py3-none-any.whl (6.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl (7.0 kB)\nDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl (17 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions_ai-0.4.9-py3-none-any.whl (5.6 kB)\nDownloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\nDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzmq-27.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (856 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.6/856.6 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading blake3-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.5/385.5 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\nDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\nDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\nDownloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading airportsdata-20250622-py3-none-any.whl (912 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nDownloading rich_toolkit-0.14.7-py3-none-any.whl (24 kB)\nDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, blake3, uvloop, uvicorn, triton, sympy, pyzmq, python-multipart, python-dotenv, pymupdf, pycountry, protobuf, partial-json-parser, opentelemetry-semantic-conventions-ai, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, llvmlite, llguidance, lark, interegular, httptools, hf-xet, diskcache, astor, airportsdata, watchfiles, starlette, opentelemetry-proto, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, depyf, rich-toolkit, prometheus-fastapi-instrumentator, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, nvidia-cusolver-cu12, lm-format-enforcer, fastapi, torch, outlines_core, opentelemetry-sdk, fastapi-cli, torchaudio, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-exporter-otlp, mistral_common, xgrammar, xformers, torchvision, outlines, numba, gguf, compressed-tensors, vllm\n  Attempting uninstall: nvidia-cusparselt-cu12\n    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: pyzmq\n    Found existing installation: pyzmq 24.0.1\n    Uninstalling pyzmq-24.0.1:\n      Successfully uninstalled pyzmq-24.0.1\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.43.0\n    Uninstalling llvmlite-0.43.0:\n      Successfully uninstalled llvmlite-0.43.0\n  Attempting uninstall: hf-xet\n    Found existing installation: hf-xet 1.1.0\n    Uninstalling hf-xet-1.1.0:\n      Successfully uninstalled hf-xet-1.1.0\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.31.1\n    Uninstalling opentelemetry-api-1.31.1:\n      Successfully uninstalled opentelemetry-api-1.31.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.31.1\n    Uninstalling huggingface-hub-0.31.1:\n      Successfully uninstalled huggingface-hub-0.31.1\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.52b1\n    Uninstalling opentelemetry-semantic-conventions-0.52b1:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.52b1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.31.1\n    Uninstalling opentelemetry-sdk-1.31.1:\n      Successfully uninstalled opentelemetry-sdk-1.31.1\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n  Attempting uninstall: numba\n    Found existing installation: numba 0.60.0\n    Uninstalling numba-0.60.0:\n      Successfully uninstalled numba-0.60.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ncudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ndistributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nydata-profiling 4.16.1 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed airportsdata-20250622 astor-0.8.1 blake3-1.0.5 compressed-tensors-0.10.1 depyf-0.18.0 diskcache-5.6.3 fastapi-0.115.13 fastapi-cli-0.0.7 gguf-0.17.1 hf-xet-1.1.5 httptools-0.6.4 huggingface-hub-0.33.0 interegular-0.3.3 lark-1.2.2 llguidance-0.7.30 llvmlite-0.44.0 lm-format-enforcer-0.10.11 mistral_common-1.6.2 msgspec-0.19.0 numba-0.61.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-exporter-otlp-proto-http-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 opentelemetry-semantic-conventions-ai-0.4.9 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.1.0 protobuf-5.29.5 pycountry-24.6.1 pymupdf-1.26.1 python-dotenv-1.1.0 python-multipart-0.0.20 pyzmq-27.0.0 rich-toolkit-0.14.7 starlette-0.46.2 sympy-1.14.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0 triton-3.3.0 uvicorn-0.34.3 uvloop-0.21.0 vllm-0.9.1 watchfiles-1.1.0 xformers-0.0.30 xgrammar-0.1.19\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import fitz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:38:37.381482Z","iopub.execute_input":"2025-06-23T09:38:37.381725Z","iopub.status.idle":"2025-06-23T09:38:37.461843Z","shell.execute_reply.started":"2025-06-23T09:38:37.381695Z","shell.execute_reply":"2025-06-23T09:38:37.461309Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport re\ntry:\n    import fitz  # PyMuPDF\nexcept ImportError:\n    raise ImportError(\"PyMuPDF not installed. Please run '!pip install pymupdf' first.\")\nimport pandas as pd\nimport pickle\nimport json\nimport numpy as np\nfrom tqdm.auto import tqdm\ntry:\n    import vllm\n    import torch\nexcept ImportError:\n    print(\"vLLM not installed. Using rule-based classifier only.\")\ntry:\n    from lxml import etree\nexcept ImportError:\n    raise ImportError(\"lxml not installed. Please run '!pip install lxml' first.\")\ntry:\n    from mdc_fine_grained_eval import compute_metrics\n    eval_available = True\nexcept ImportError:\n    print(\"mdc_fine_grained_eval.py not found. Skipping evaluation for training mode.\")\n    eval_available = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:38:37.462493Z","iopub.execute_input":"2025-06-23T09:38:37.462726Z","iopub.status.idle":"2025-06-23T09:39:03.403648Z","shell.execute_reply.started":"2025-06-23T09:38:37.462702Z","shell.execute_reply":"2025-06-23T09:39:03.403086Z"}},"outputs":[{"name":"stdout","text":"INFO 06-23 09:38:46 [__init__.py:244] Automatically detected platform cuda.\n","output_type":"stream"},{"name":"stderr","text":"2025-06-23 09:38:48.286436: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750671528.488092      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750671528.548994      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Read PDFs and XMLs\n- Processes PDFs in `train/PDF` or `test/PDF`, stopping at \"References\" to avoid irrelevant DOIs.\n- Parses corresponding XML files in `train/XML` or `test/XML` to extract DOIs and accession IDs.\n- Stores results as `(article_id, pdf_text, xml_metadata)` tuples.\n- Handles cases where XML files are missing (~25% of articles).","metadata":{}},{"cell_type":"code","source":"# Define directories\nbase_dir = \"/kaggle/input/make-data-count-finding-data-references\"\npdf_dir = f\"{base_dir}/test/PDF\" if os.getenv('KAGGLE_IS_COMPETITION_RERUN') else f\"{base_dir}/train/PDF\"\nxml_dir = f\"{base_dir}/test/XML\" if os.getenv('KAGGLE_IS_COMPETITION_RERUN') else f\"{base_dir}/train/XML\"\n\ntexts = []\nfor filename in tqdm(os.listdir(pdf_dir), desc=\"Reading PDFs and XMLs\"):\n    if filename.endswith(\".pdf\"):\n        article_id = filename.split(\".pdf\")[0]\n        pdf_path = os.path.join(pdf_dir, filename)\n        xml_path = os.path.join(xml_dir, filename.replace(\".pdf\", \".xml\"))\n        \n        # Read PDF\n        pdf_text = \"\"\n        try:\n            doc = fitz.open(pdf_path)\n            for page in doc:\n                page_text = page.get_text().lower()\n                if 'references' in page_text:\n                    pdf_text += page_text.split(\"references\")[0]\n                    break\n                pdf_text += page_text\n            doc.close()\n        except Exception as e:\n            print(f\"Error processing PDF {filename}: {e}\")\n            continue\n        \n        # Read XML\n        xml_metadata = \"\"\n        if os.path.exists(xml_path):\n            try:\n                tree = etree.parse(xml_path)\n                root = tree.getroot()\n                for elem in root.iter('{*}article-id', '{*}ref', '{*}data'):\n                    if elem.text:\n                        xml_metadata += elem.text.lower() + \" \"\n                    for attr in elem.attrib.values():\n                        xml_metadata += attr.lower() + \" \"\n            except Exception as e:\n                print(f\"Error processing XML {filename.replace('.pdf', '.xml')}: {e}\")\n        \n        texts.append((article_id, pdf_text, xml_metadata))\n\nprint(f\"Processed {len(texts)} articles\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:39:03.404376Z","iopub.execute_input":"2025-06-23T09:39:03.405053Z","iopub.status.idle":"2025-06-23T09:40:16.166346Z","shell.execute_reply.started":"2025-06-23T09:39:03.405023Z","shell.execute_reply":"2025-06-23T09:40:16.165606Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Reading PDFs and XMLs:   0%|          | 0/524 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac279853796341849971e1871e475a70"}},"metadata":{}},{"name":"stdout","text":"MuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nMuPDF error: unsupported error: cannot create appearance stream for  annotations\n\nProcessed 524 articles\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Extract Identifier Contexts\n- Uses regex to detect DOIs and accession IDs (GEO: `GSE\\d+`, PDB: `PDB\\s+[0-9A-Z]{4}`, ArrayExpress: `E-[A-Z]{4}-\\d+`).\n- Extracts a 400-character context window around each identifier.\n- Stores results as `(article_id, identifier, id_type, context)` tuples.\n- Filters out identifiers matching the article ID.","metadata":{}},{"cell_type":"code","source":"# Cell 4: Extract Identifier Contexts\nchunks = []\ndoi_pattern = r\"10\\.\\d{4,}/[^\\s]+\"  # DOI regex\naccession_patterns = [\n    r\"GSE\\d{5,}\",  # GEO datasets\n    r\"PDB\\s+[0-9A-Z]{4}\",  # PDB datasets\n    r\"E-[A-Z]{4}-\\d+\"  # ArrayExpress datasets\n]\n\nskipped_identifiers = []\nfor article_id, pdf_text, xml_metadata in tqdm(texts, desc=\"Extracting identifiers\"):\n    combined_text = pdf_text + \" \" + xml_metadata\n    article_doi = article_id.lower() if article_id.startswith(\"10.\") else None\n    \n    # Extract DOIs\n    for match in re.finditer(doi_pattern, combined_text, re.IGNORECASE):\n        identifier = match.group().lower()\n        # Skip if identifier matches article_id or is part of article_doi\n        if identifier == article_id.lower() or (article_doi and identifier.startswith(article_doi)):\n            skipped_identifiers.append((article_id, identifier, \"self-referential DOI\"))\n            continue\n        start = max(0, match.start() - 200)\n        end = match.start() + 200\n        context = combined_text[start:end].strip()\n        if len(context) < 10:  # Skip short contexts\n            skipped_identifiers.append((article_id, identifier, \"context too short\"))\n            continue\n        chunks.append((article_id, identifier, \"doi\", context))\n    \n    # Extract accession IDs\n    for pattern in accession_patterns:\n        for match in re.finditer(pattern, combined_text, re.IGNORECASE):\n            identifier = match.group().lower()\n            start = max(0, match.start() - 200)\n            end = match.start() + 200\n            context = combined_text[start:end].strip()\n            if len(context) < 10:\n                skipped_identifiers.append((article_id, identifier, \"context too short\"))\n                continue\n            chunks.append((article_id, identifier, \"accession\", context))\n\n# Log skipped identifiers\nif skipped_identifiers:\n    print(f\"Skipped {len(skipped_identifiers)} identifiers:\")\n    for article_id, identifier, reason in skipped_identifiers[:5]:  # Log first 5 for brevity\n        print(f\"Article {article_id}: {identifier} ({reason})\")\n    if len(skipped_identifiers) > 5:\n        print(f\"...and {len(skipped_identifiers) - 5} more\")\n\n# Validate chunks\nvalid_chunks = [chunk for chunk in chunks if len(chunk) == 4 and isinstance(chunk[3], str) and chunk[3].strip()]\nif not valid_chunks:\n    print(\"Error: No valid chunks extracted. Check regex patterns or input data.\")\nelse:\n    print(f\"Extracted {len(valid_chunks)} valid identifier contexts\")\n\n# Save chunks\nwith open('chunks.pkl', 'wb') as file:\n    pickle.dump(valid_chunks, file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:40:16.168331Z","iopub.execute_input":"2025-06-23T09:40:16.168561Z","iopub.status.idle":"2025-06-23T09:40:16.946679Z","shell.execute_reply.started":"2025-06-23T09:40:16.168544Z","shell.execute_reply":"2025-06-23T09:40:16.946113Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Extracting identifiers:   0%|          | 0/524 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eb5ca2fa1bd40559e8abfed66896a29"}},"metadata":{}},{"name":"stdout","text":"Extracted 3676 valid identifier contexts\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Rule-Based Classifier\n- Classifies identifiers as Primary, Secondary, or Not Relevant using keyword patterns.\n- Primary: Keywords like \"generated,\" \"produced,\" \"this study.\"\n- Secondary: Keywords like \"obtained from,\" \"available at,\" \"reused.\"\n- Not Relevant: Keywords like \"reference,\" \"citation,\" or lack of data context.\n- Used as a fallback if LLM fails or for ensemble scoring.","metadata":{}},{"cell_type":"code","source":"llm_available = False\ntry:\n    import vllm\n    llm = vllm.LLM(\n        \"/kaggle/input/qwen-3/transformers/8b-awq/1\",\n        quantization='awq',\n        tensor_parallel_size=torch.cuda.device_count(),\n        gpu_memory_utilization=0.9,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2048,\n        disable_log_stats=True\n    )\n    tokenizer = llm.get_tokenizer()\n    llm_available = True\nexcept Exception as e:\n    print(f\"LLM loading failed: {e}. Using rule-based classifier.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:40:16.947440Z","iopub.execute_input":"2025-06-23T09:40:16.947737Z","iopub.status.idle":"2025-06-23T09:42:41.396647Z","shell.execute_reply.started":"2025-06-23T09:40:16.947713Z","shell.execute_reply":"2025-06-23T09:42:41.395790Z"}},"outputs":[{"name":"stdout","text":"INFO 06-23 09:40:32 [config.py:823] This model supports multiple tasks: {'embed', 'classify', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\nWARNING 06-23 09:40:33 [config.py:931] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nWARNING 06-23 09:40:33 [arg_utils.py:1642] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \nINFO 06-23 09:40:33 [config.py:1946] Defaulting to use mp for distributed inference\nWARNING 06-23 09:40:33 [cuda.py:91] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nINFO 06-23 09:40:33 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.1) with config: model='/kaggle/input/qwen-3/transformers/8b-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen-3/transformers/8b-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/kaggle/input/qwen-3/transformers/8b-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \nWARNING 06-23 09:40:34 [utils.py:2597] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\nWARNING 06-23 09:40:34 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 06-23 09:40:34 [cuda.py:275] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 06-23 09:40:34 [cuda.py:324] Using XFormers backend.\nWARNING 06-23 09:40:35 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\nINFO 06-23 09:40:39 [__init__.py:244] Automatically detected platform cuda.\n","output_type":"stream"},{"name":"stderr","text":"2025-06-23 09:40:40.164601: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750671640.187427     164 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750671640.194289     164 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:40:46 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\n\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:40:47 [cuda.py:275] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:40:47 [cuda.py:324] Using XFormers backend.\n","output_type":"stream"},{"name":"stderr","text":"[W623 09:40:58.800062478 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W623 09:40:58.150880720 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W623 09:41:08.807434256 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"INFO 06-23 09:41:18 [utils.py:1126] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:41:18 [utils.py:1126] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:41:18 [pynccl.py:70] vLLM is using nccl==2.26.2\nINFO 06-23 09:41:18 [pynccl.py:70] vLLM is using nccl==2.26.2\n","output_type":"stream"},{"name":"stderr","text":"[W623 09:41:18.817817874 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"INFO 06-23 09:41:18 [custom_all_reduce_utils.py:208] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 06-23 09:41:46 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:41:46 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m WARNING 06-23 09:41:46 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nWARNING 06-23 09:41:46 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nINFO 06-23 09:41:46 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_85bed7fb'), local_subscribe_addr='ipc:///tmp/60c34f84-6269-4488-8e77-33c6fade5a91', remote_subscribe_addr=None, remote_addr_ipv6=False)\nINFO 06-23 09:41:46 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:41:46 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\nINFO 06-23 09:41:46 [model_runner.py:1171] Starting to load model /kaggle/input/qwen-3/transformers/8b-awq/1...\n\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:41:46 [model_runner.py:1171] Starting to load model /kaggle/input/qwen-3/transformers/8b-awq/1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"463b2c4b256c40d4bc7f9c9414b5abbf"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:42:28 [default_loader.py:272] Loading weights took 40.62 seconds\nINFO 06-23 09:42:28 [default_loader.py:272] Loading weights took 40.69 seconds\n\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:42:28 [model_runner.py:1203] Model loading took 2.8510 GiB and 40.862698 seconds\nINFO 06-23 09:42:29 [model_runner.py:1203] Model loading took 2.8510 GiB and 40.922685 seconds\n\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:42:35 [worker.py:294] Memory profiling takes 6.38 seconds\n\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:42:35 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n\u001b[1;36m(VllmWorkerProcess pid=164)\u001b[0;0m INFO 06-23 09:42:35 [worker.py:294] model weights take 2.85GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.20GiB; the rest of the memory reserved for KV Cache is 10.12GiB.\nINFO 06-23 09:42:36 [worker.py:294] Memory profiling takes 6.53 seconds\nINFO 06-23 09:42:36 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\nINFO 06-23 09:42:36 [worker.py:294] model weights take 2.85GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 8.91GiB.\nINFO 06-23 09:42:36 [executor_base.py:113] # cuda blocks: 8108, # CPU blocks: 3640\nINFO 06-23 09:42:36 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 63.34x\nINFO 06-23 09:42:41 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 12.12 seconds\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def rule_based_classifier(context):\n    context = context.lower()\n    primary_keywords = ['generated', 'produced', 'this study', 'our data', 'collected']\n    secondary_keywords = ['obtained from', 'available at', 'reused', 'derived', 'accessed']\n    not_relevant_keywords = ['reference', 'citation', 'bibliography']\n    \n    if any(keyword in context for keyword in not_relevant_keywords):\n        return \"Not Relevant\", 0.9\n    elif any(keyword in context for keyword in primary_keywords):\n        return \"Primary\", 0.8\n    elif any(keyword in context for keyword in secondary_keywords):\n        return \"Secondary\", 0.8\n    return \"Not Relevant\", 0.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:42:41.397437Z","iopub.execute_input":"2025-06-23T09:42:41.397675Z","iopub.status.idle":"2025-06-23T09:42:41.403360Z","shell.execute_reply.started":"2025-06-23T09:42:41.397658Z","shell.execute_reply":"2025-06-23T09:42:41.402575Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Cell 7: LLM Classification\nprompt_template = \"\"\"\nYou are given a piece of academic text containing a dataset identifier (DOI or accession ID). Your task is to:\n\n1. Identify the single dataset identifier (DOI or accession ID).\n2. Normalize DOIs to full URL format (https://doi.org/...). Keep accession IDs as-is (e.g., GSE12345).\n3. Classify the data associated with the identifier as:\n   - \"Primary\": data generated specifically for this study.\n   - \"Secondary\": data reused or derived from prior work.\n   - \"Not Relevant\": identifier is in References, not research data, or unrelated.\n4. Provide a confidence score (0.0 to 1.0) for the classification.\n\nIf no valid identifier is found, return an empty JSON object {}.\n\nReturn ONE dictionary within JSON backticks with three keys:\n```json\n{\n    \"dataset_id\": \"<normalized identifier>\",\n    \"classification\": \"<Primary, Secondary, or Not Relevant>\",\n    \"confidence\": <float between 0.0 and 1.0>\n}\n```\n\nAcademic Text: %s\n\"\"\"\n\n# Generate prompts with robust formatting\nprompts = []\nprompt_issues = []\nfor idx, chunk in enumerate(valid_chunks):\n    try:\n        context = chunk[3].strip()\n        if not context:\n            prompt_issues.append((idx, chunk, \"empty context\"))\n            prompts.append(\"\")\n            continue\n        \n        # Escape special characters and use % operator\n        prompt = prompt_template % context.replace('%', '%%')\n        prompts.append(prompt)\n    except Exception as e:\n        prompt_issues.append((idx, chunk, f\"formatting error: {e}\"))\n        prompts.append(\"\")\n\n# Log prompt issues\nif prompt_issues:\n    print(f\"Encountered {len(prompt_issues)} prompt generation issues:\")\n    for idx, chunk, reason in prompt_issues[:5]:  # Log first 5\n        print(f\"Chunk {idx}: {reason}\")\n    if len(prompt_issues) > 5:\n        print(f\"...and {len(prompt_issues) - 5} more\")\n\n# Filter valid prompts\nvalid_prompts = [p for p in prompts if p.strip()]\nif not valid_prompts:\n    print(\"Warning: No valid prompts generated. Using rule-based classifier for all chunks.\")\n\n# Initialize responses\nresponses = []\nllm_success_count = 0\nif llm_available and valid_prompts:\n    try:\n        responses = llm.generate(\n            valid_prompts,\n            vllm.SamplingParams(\n                n=1,\n                top_p=0.8,\n                temperature=0,\n                seed=777,\n                skip_special_tokens=False,\n                max_tokens=256,\n            ),\n            use_tqdm=True\n        )\n        responses = [(resp.outputs[0].text.lower(), 1.0) for resp in responses]\n        llm_success_count = len(responses)\n    except Exception as e:\n        print(f\"LLM inference failed: {e}. Using rule-based classifier.\")\n        responses = [(None, 0.0) for _ in valid_prompts]\nelse:\n    print(\"LLM unavailable or no valid prompts. Using rule-based classifier.\")\n    responses = [(None, 0.0) for _ in valid_prompts]\n\n# Align responses with valid_chunks\nfull_responses = [(None, 0.0) for _ in valid_chunks]\nprompt_idx = 0\nfor idx, prompt in enumerate(prompts):\n    if prompt.strip() and prompt_idx < len(responses):\n        full_responses[idx] = responses[prompt_idx]\n        prompt_idx += 1\n\n# Fallback to rule-based classifier\nfinal_responses = []\nfor idx, ((article_id, identifier, id_type, context), (llm_response, llm_conf)) in enumerate(zip(valid_chunks, full_responses)):\n    if llm_response and llm_conf > 0.5:\n        final_responses.append((llm_response, llm_conf))\n    else:\n        classification, conf = rule_based_classifier(context)\n        normalized_id = f\"https://doi.org/{identifier}\" if id_type == \"doi\" else identifier\n        json_response = f\"\"\"```json\n{{\n    \"dataset_id\": \"{normalized_id}\",\n    \"classification\": \"{classification}\",\n    \"confidence\": {conf}\n}}\n```\"\"\"\n        final_responses.append((json_response, conf))\n\nprint(f\"LLM succeeded for {llm_success_count}/{len(valid_chunks)} chunks; used rule-based classifier for {len(valid_chunks) - llm_success_count}\")\n\n# Save final_responses\nwith open('responses.pkl', 'wb') as file:\n    pickle.dump(final_responses, file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T09:42:41.404152Z","iopub.execute_input":"2025-06-23T09:42:41.404417Z","iopub.status.idle":"2025-06-23T10:22:17.528821Z","shell.execute_reply.started":"2025-06-23T09:42:41.404394Z","shell.execute_reply":"2025-06-23T10:22:17.528033Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/3676 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72613370a6d44feb9d55052036317bc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/3676 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fb2c302bffd4da984280b841ae47ac8"}},"metadata":{}},{"name":"stdout","text":"WARNING 06-23 09:43:58 [scheduler.py:1801] Sequence group 255 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\nWARNING 06-23 09:44:46 [scheduler.py:1801] Sequence group 204 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\nWARNING 06-23 09:47:21 [scheduler.py:1801] Sequence group 465 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\nWARNING 06-23 09:52:10 [scheduler.py:1801] Sequence group 957 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\nWARNING 06-23 09:55:07 [scheduler.py:1801] Sequence group 1194 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\nWARNING 06-23 09:59:48 [scheduler.py:1801] Sequence group 1697 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\nWARNING 06-23 10:02:43 [scheduler.py:1801] Sequence group 1933 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\nWARNING 06-23 10:05:43 [scheduler.py:1801] Sequence group 2158 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\nWARNING 06-23 10:10:22 [scheduler.py:1801] Sequence group 2665 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\nWARNING 06-23 10:13:22 [scheduler.py:1801] Sequence group 2896 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\nWARNING 06-23 10:18:07 [scheduler.py:1801] Sequence group 3403 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\nWARNING 06-23 10:20:59 [scheduler.py:1801] Sequence group 3637 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\nLLM succeeded for 3676/3676 chunks; used rule-based classifier for 0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"with open('responses.pkl', 'rb') as file:\n    responses = pickle.load(file) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T10:22:17.529716Z","iopub.execute_input":"2025-06-23T10:22:17.530037Z","iopub.status.idle":"2025-06-23T10:22:17.538288Z","shell.execute_reply.started":"2025-06-23T10:22:17.530010Z","shell.execute_reply":"2025-06-23T10:22:17.537690Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import json\njson_block_pattern = r'```json\\s*(.*?)\\s*```'\ncitations = []\nfor (article_id, identifier, id_type, _), (response, conf) in zip(chunks, final_responses):\n    try:\n        match = re.search(json_block_pattern, response, re.DOTALL)\n        if match:\n            json_string = match.group(1).strip()\n            parsed_json = json.loads(json_string)\n            if not parsed_json:\n                continue\n            dataset_id = parsed_json['dataset_id'].replace('\\u200b', '').replace('\\n', '')\n            category = parsed_json['classification'].capitalize()\n            confidence = parsed_json.get('confidence', 0.5)\n            if category in ['Primary', 'Secondary'] and confidence > 0.7:\n                citations.append([article_id, dataset_id, category, confidence])\n    except Exception as e:\n        print(f\"Error parsing response for {article_id}: {e}\")\n\n# Create DataFrame\nsubmission = pd.DataFrame(citations, columns=['article_id', 'dataset_id', 'type', 'confidence'])\n\n# Ensemble: Resolve conflicts by highest confidence\nsubmission = submission.loc[submission.groupby(['article_id', 'dataset_id'])['confidence'].idxmax()]\n\n# Filter frequent dataset IDs\ndataset_id_counts = submission['dataset_id'].value_counts()\nfrequent_dataset_ids = dataset_id_counts[dataset_id_counts >= 3].index\nsubmission = submission[~submission['dataset_id'].isin(frequent_dataset_ids)]\n\n# Sort and deduplicate\nsubmission = submission.sort_values(by=['article_id', 'dataset_id', 'type'], ascending=True)\nsubmission = submission.drop_duplicates(subset=['article_id', 'dataset_id'])\n\n# Assign row IDs\nsubmission['row_id'] = range(len(submission))\n\n# Final submission\nsubmission = submission[['row_id', 'article_id', 'dataset_id', 'type']]\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file 'submission.csv' created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T10:22:17.538971Z","iopub.execute_input":"2025-06-23T10:22:17.539197Z","iopub.status.idle":"2025-06-23T10:22:17.698542Z","shell.execute_reply.started":"2025-06-23T10:22:17.539175Z","shell.execute_reply":"2025-06-23T10:22:17.697810Z"}},"outputs":[{"name":"stdout","text":"Submission file 'submission.csv' created\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T10:50:33.377978Z","iopub.execute_input":"2025-06-23T10:50:33.378704Z","iopub.status.idle":"2025-06-23T10:50:33.409901Z","shell.execute_reply.started":"2025-06-23T10:50:33.378680Z","shell.execute_reply":"2025-06-23T10:50:33.409176Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"     row_id              article_id  \\\n0         0    10.1002_2017jc013030   \n1         1    10.1002_2017jc013030   \n2         2  10.1002_anie.201916483   \n3         3  10.1002_anie.202005531   \n4         4  10.1002_anie.202007717   \n..      ...                     ...   \n844     844     10.7717_peerj.13193   \n845     845     10.7717_peerj.13193   \n846     846     10.7717_peerj.13193   \n847     847     10.7717_peerj.13193   \n848     848     10.7717_peerj.13193   \n\n                                    dataset_id       type  \n0                         10.1002/2017jc013030  Secondary  \n1         https://doi.org/10.1002/2017jc013030  Secondary  \n2       https://doi.org/10.1002/anie.201916483  Secondary  \n3       https://doi.org/10.1002/anie.202005531  Secondary  \n4                       10.1002/anie.202007717  Secondary  \n..                                         ...        ...  \n844                        10.7717/peerj.13193  Secondary  \n845        https://doi.org/10.7717/peerj.13193  Secondary  \n846  https://doi.org/10.7717/peerj.13193/fig-2    Primary  \n847  https://doi.org/10.7717/peerj.13193/fig-3    Primary  \n848  https://doi.org/10.7717/peerj.13193/fig-4    Primary  \n\n[849 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_id</th>\n      <th>article_id</th>\n      <th>dataset_id</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>10.1002_2017jc013030</td>\n      <td>10.1002/2017jc013030</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>10.1002_2017jc013030</td>\n      <td>https://doi.org/10.1002/2017jc013030</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>10.1002_anie.201916483</td>\n      <td>https://doi.org/10.1002/anie.201916483</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>10.1002_anie.202005531</td>\n      <td>https://doi.org/10.1002/anie.202005531</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>10.1002_anie.202007717</td>\n      <td>10.1002/anie.202007717</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>844</th>\n      <td>844</td>\n      <td>10.7717_peerj.13193</td>\n      <td>10.7717/peerj.13193</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>845</th>\n      <td>845</td>\n      <td>10.7717_peerj.13193</td>\n      <td>https://doi.org/10.7717/peerj.13193</td>\n      <td>Secondary</td>\n    </tr>\n    <tr>\n      <th>846</th>\n      <td>846</td>\n      <td>10.7717_peerj.13193</td>\n      <td>https://doi.org/10.7717/peerj.13193/fig-2</td>\n      <td>Primary</td>\n    </tr>\n    <tr>\n      <th>847</th>\n      <td>847</td>\n      <td>10.7717_peerj.13193</td>\n      <td>https://doi.org/10.7717/peerj.13193/fig-3</td>\n      <td>Primary</td>\n    </tr>\n    <tr>\n      <th>848</th>\n      <td>848</td>\n      <td>10.7717_peerj.13193</td>\n      <td>https://doi.org/10.7717/peerj.13193/fig-4</td>\n      <td>Primary</td>\n    </tr>\n  </tbody>\n</table>\n<p>849 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    pred_df = pd.read_csv(\"submission.csv\")\n    reference_df = pd.read_csv(\"/kaggle/input/make-data-count-finding-data-references/train_labels.csv\")\n    reference_df = reference_df[reference_df['type'] != 'Missing'].reset_index(drop=True)\n    eval_dict = compute_metrics(pred_df, reference_df)\n    m = eval_dict['ents_f1']\n    print(f\"CV @ 524 train PDF = {round(m, 4)}\")\n    print(\"\\n\\n\")\n    print(json.dumps(eval_dict['ents_per_type'], indent=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T08:56:39.988403Z","iopub.status.idle":"2025-06-23T08:56:39.988632Z","shell.execute_reply.started":"2025-06-23T08:56:39.988519Z","shell.execute_reply":"2025-06-23T08:56:39.988533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}