{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":74608,"databundleVersionId":12609125,"sourceType":"competition"},{"sourceId":246274448,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/keyushnisar/openpolymerpred?scriptVersionId=246777723\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Open Polymer Prediction 2025: Enhanced Baseline Notebook\n\n<a href=\"https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025\" target=\"_blank\">\n  <img src=\"https://img.shields.io/badge/Kaggle-Competition-blue?style=for-the-badge&logo=kaggle\" alt=\"Kaggle Competition\">\n</a>\n\nThis notebook addresses the *NeurIPS - Open Polymer Prediction 2025* competition, which involves predicting five key polymer properties—glass transition temperature (Tg), fractional free volume (FFV), thermal conductivity (Tc), density, and radius of gyration (Rg)—from SMILES representations of polymer structures. The evaluation metric is a weighted Mean Absolute Error (wMAE), designed to balance the contribution of each property by accounting for their scale and data availability.\n\nThis enhanced baseline improves the original notebook by incorporating advanced feature engineering, a broader ensemble of machine learning models, and hyperparameter optimization. The notebook is structured for clarity, reproducibility, and scalability to handle the competition's hidden test set of approximately 1,500 polymers.\nIn this script, we'll:\n1. Find similar images to pair them up efficiently\n2. Detect key points (like corners or distinctive features) in each image\n3. Match these points between image pairs\n4. Verify matches to ensure accuracy\n5. Build a 3D model using COLMAP\n6. Create a submission file with camera poses\n\n\n\n**Approach Overview**\nThe approach consists of the following components:\n\n* Data Preprocessing: Load and clean the train and test datasets, handling missing values and outliers to ensure robust data quality.\n* Feature Engineering: Use RDKit to compute molecular descriptors and introduce custom polymer-specific features to capture structural properties.\n* Model Selection: Train an ensemble of gradient boosting models (XGBoost, LightGBM, CatBoost) and a simple neural network for each target property.\n* Hyperparameter Tuning: Use Optuna to optimize model hyperparameters, improving predictive performance.\n* Cross-Validation: Implement K-Fold cross-validation to ensure reliable model evaluation and generalization.\n* Ensemble Predictions: Combine predictions from multiple models to enhance accuracy and robustness.\n* Submission Preparation: Generate predictions for the test set and format them according to the competition requirements.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Configuration\nThe configuration class defines key parameters for data preprocessing, model training, and cross-validation. Key settings include:\n\n* Target Variables: The five properties to predict: Tg, FFV, Tc, Density, and Rg.\n* Cross-Validation: 5-fold K-Fold cross-validation for robust model evaluation.\n* Model Parameters: Settings for GPU usage, early stopping, and random state for reproducibility.\n* Feature Engineering: Enable feature importance analysis and preprocessing steps.\n* Evaluation Metric: Minimize the weighted Mean Absolute Error (wMAE).","metadata":{}},{"cell_type":"markdown","source":"%%time\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors\nfrom tqdm import tqdm\nimport gc\n\nclass CFG:\n    \"\"\"\n    Simple configuration class for paths and settings.\n    \"\"\"\n    data_path = \"/kaggle/input/neurips-open-polymer-prediction-2025\"\n    output_path = \"/kaggle/working\"\n    targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n    n_splits = 5\n    random_state = 42\n\n# Utility function for memory cleanup\ndef clean_memory():\n    gc.collect()\n\n# Utility function for wMAE metric\ndef score_metric(y_true, y_pred):\n    \"\"\"\n    Compute weighted Mean Absolute Error (wMAE) across targets.\n    \"\"\"\n    weights = {'Tg': 0.2, 'FFV': 0.2, 'Tc': 0.2, 'Density': 0.2, 'Rg': 0.2}  # Simplified equal weights\n    mae = 0\n    for col in y_true.columns:\n        valid_idx = y_true[col].notna()\n        if valid_idx.sum() > 0:\n            mae += weights[col] * mean_absolute_error(y_true[col][valid_idx], y_pred[col][valid_idx])\n    return mae\n\nprint(\"Setup complete\")","metadata":{"execution":{"iopub.status.busy":"2025-06-20T07:49:59.617411Z","iopub.execute_input":"2025-06-20T07:49:59.617573Z","iopub.status.idle":"2025-06-20T07:50:02.808543Z","shell.execute_reply.started":"2025-06-20T07:49:59.617558Z","shell.execute_reply":"2025-06-20T07:50:02.807851Z"}}},{"cell_type":"code","source":"!pip install optuna torch deepchem\n!pip install rdkit","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering\nFeature engineering is critical for capturing the chemical and structural properties of polymers. The FeatureMaker class uses RDKit to compute molecular descriptors from SMILES strings, such as molecular weight, topological polar surface area, and other chemical properties. Additional custom features are introduced to capture polymer-specific characteristics, including:\n\n* Chain Length: Number of repeating units inferred from SMILES.\n* Functional Group Counts: Counts of specific chemical groups (e.g., aromatic rings, hydroxyl groups).\n* Molecular Complexity: Metrics like the BertzCT index for structural complexity.\n  \nThe Ipc descriptor is log-transformed to handle its large range, and infinity values are replaced with NaN. Columns with near-zero variance or excessive missing values (>99.75%) are dropped to reduce noise.","metadata":{}},{"cell_type":"markdown","source":"%%writefile myfe.py\n\nimport pandas as pd\nimport numpy as np\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors, AllChem\n\nclass FeatureMaker:\n    \"\"\"\n    Generate RDKit descriptors and Morgan fingerprints from SMILES strings.\n    \"\"\"\n    def __init__(self):\n        self.feature_names = None\n\n    def _compute_descriptors(self, smiles):\n        \"\"\"\n        Compute RDKit descriptors and Morgan fingerprints for a SMILES string.\n        \"\"\"\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            return [None] * (len(Descriptors.descList) + 256)  # 256 for Morgan\n        desc_values = [desc[1](mol) for desc in Descriptors.descList]\n        # Compute Morgan fingerprint (ECFP4, radius=2, 256 bits)\n        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=256)\n        fp_values = list(fp)\n        return desc_values + fp_values\n\n    def fit_transform(self, X):\n        \"\"\"\n        Fit and transform the dataset to create features.\n        \"\"\"\n        desc_names = [desc[0] for desc in Descriptors.descList]\n        fp_names = [f'ECFP_{i}' for i in range(256)]\n        descriptors = [self._compute_descriptors(smi) for smi in X['SMILES']]\n        df = pd.DataFrame(descriptors, columns=desc_names + fp_names)\n        df = pd.concat([X, df], axis=1)\n        df[\"Ipc\"] = np.log1p(df[\"Ipc\"])  # Log-transform Ipc\n        self.feature_names = list(df.columns)\n        return df.replace([np.inf, -np.inf], np.nan)\n\n    def transform(self, X):\n        \"\"\"\n        Transform the dataset using the same features.\n        \"\"\"\n        desc_names = [desc[0] for desc in Descriptors.descList]\n        fp_names = [f'ECFP_{i}' for i in range(256)]\n        descriptors = [self._compute_descriptors(smi) for smi in X['SMILES']]\n        df = pd.DataFrame(descriptors, columns=desc_names + fp_names)\n        df = pd.concat([X, df], axis=1)\n        df[\"Ipc\"] = np.log1p(df[\"Ipc\"])\n        return df.replace([np.inf, -np.inf], np.nan)\n\n    def get_feature_names(self):\n        \"\"\"\n        Return feature names.\n        \"\"\"\n        return self.feature_names","metadata":{"execution":{"iopub.status.busy":"2025-06-20T07:50:21.63078Z","iopub.execute_input":"2025-06-20T07:50:21.631486Z","iopub.status.idle":"2025-06-20T07:50:21.637203Z","shell.execute_reply.started":"2025-06-20T07:50:21.631461Z","shell.execute_reply":"2025-06-20T07:50:21.63647Z"}}},{"cell_type":"markdown","source":"## Model Training Strategy\n\nThe model training strategy employs an ensemble of gradient boosting models (XGBoost, LightGBM, CatBoost) and a neural network implemented with PyTorch for each target property. Key aspects include:\n\n* FFV (Fractional Free Volume): As the most populated target (~90% fill rate), it uses a larger ensemble with tuned hyperparameters.\n* Other Targets (Tg, Tc, Density, Rg): These have higher missing rates, so simpler models with fewer iterations are used to avoid overfitting.\n* Hyperparameter Tuning: Optuna is used to optimize key parameters (e.g., learning rate, max depth) for each model.\n* Cross-Validation: 5-fold K-Fold cross-validation ensures robust evaluation.\nEnsemble: Predictions are averaged across models to improve robustness.\n\nThe weighted MAE metric is computed for each target, and the final score aggregates these errors with weights based on data availability and property ranges.","metadata":{}},{"cell_type":"markdown","source":"## Approach 1: Gradient Boosting Ensemble\n\nThis approach trains an ensemble of three gradient boosting models—XGBoost, LightGBM, and CatBoost—for each target property. These models are robust to missing data and handle non-linear relationships well. We use Optuna for hyperparameter tuning to optimize parameters like learning rate, max depth, and subsampling rates. FFV, with the highest data availability, uses larger models, while other targets use smaller models to prevent overfitting.","metadata":{}},{"cell_type":"markdown","source":"%%time\n\n# Install required packages\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom tqdm import tqdm\nimport optuna\nimport gc\nfrom myfe import FeatureMaker\n\n# Define configuration\nclass CFG:\n    data_path = \"/kaggle/input/neurips-open-polymer-prediction-2025/\"  # Kaggle input path\n    random_state = 42\n    n_splits = 3\n    targets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n\n# Memory cleanup function\ndef clean_memory():\n    gc.collect()\n\n# Data loading and preprocessing\nprint(\"Loading data...\")\ntrain_df = pd.read_csv(f\"{CFG.data_path}/train.csv\")\ntest_df = pd.read_csv(f\"{CFG.data_path}/test.csv\")\n\n# Initialize feature maker\nfeaturizer = FeatureMaker()\n\n# Generate features\nprint(\"Generating features...\")\nXtrain = featurizer.fit_transform(train_df)\nXtest = featurizer.transform(test_df)\n\n# Separate targets\nYtrain = Xtrain[CFG.targets].copy()\nXtrain = Xtrain.drop(CFG.targets, axis=1, errors='ignore')\n\n# Ensure test set has 'id' column\nif 'id' not in Xtest.columns:\n    Xtest['id'] = test_df['id']\n\n# Optuna objective function\ndef objective(trial, X, y, model_type, cv):\n    \"\"\"\n    Objective function for Optuna hyperparameter tuning.\n    \"\"\"\n    if model_type == 'XGB':\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n            'max_depth': trial.suggest_int('max_depth', 3, 8),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.8),\n            'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0),\n            'objective': 'reg:absoluteerror',\n            'random_state': CFG.random_state,\n            'early_stopping_rounds': 50,\n            'tree_method': 'hist',\n            'verbosity': 0\n        }\n        model = XGBRegressor(**params)\n    \n    elif model_type == 'LGBM':\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n            'max_depth': trial.suggest_int('max_depth', 3, 8),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.8),\n            'num_leaves': trial.suggest_int('num_leaves', 20, 50),\n            'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n            'objective': 'regression_l1',\n            'random_state': CFG.random_state,\n            'early_stopping_rounds': 50,\n            'verbosity': -1\n        }\n        model = LGBMRegressor(**params)\n    \n    elif model_type == 'CB':\n        params = {\n            'iterations': trial.suggest_int('iterations', 100, 1000),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n            'depth': trial.suggest_int('depth', 3, 8),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10.0),\n            'loss_function': 'MAE',\n            'eval_metric': 'MAE',\n            'random_state': CFG.random_state,\n            'early_stopping_rounds': 50,\n            'thread_count': -1,\n            'verbose': 0\n        }\n        model = CatBoostRegressor(**params)\n    \n    scores = []\n    for train_idx, dev_idx in cv.split(X, y):\n        X_tr, X_dev = X.iloc[train_idx], X.iloc[dev_idx]\n        y_tr, y_dev = y.iloc[train_idx], y.iloc[dev_idx]\n        if model_type in ['XGB', 'LGBM']:\n            model.fit(X_tr, y_tr, eval_set=[(X_dev, y_dev)], verbose_eval=False)\n        else:\n            model.fit(X_tr, y_tr, eval_set=(X_dev, y_dev), verbose=False)\n        preds = model.predict(X_dev)\n        scores.append(mean_absolute_error(y_dev, preds))\n    \n    return np.mean(scores)\n\ndef tune_model(X, y, model_type, cv, n_trials=5):\n    \"\"\"\n    Tune hyperparameters using Optuna.\n    \"\"\"\n    study = optuna.create_study(direction='minimize')\n    study.optimize(lambda trial: objective(trial, X, y, model_type, cv), n_trials=n_trials)\n    return study.best_params\n\n# Gradient Boosting Ensemble\nOOF_Preds = []\nTest_Preds = []\ndrop_cols = [\"Source\", \"id\", \"Id\", \"SMILES\"] + CFG.targets\n\nfor target in tqdm(CFG.targets):\n    print(f\"\\n=== Training Gradient Boosting Ensemble for {target} ===\\n\")\n    \n    # Prepare data\n    Xtrain_ = Xtrain.copy()\n    Xtrain_[target] = Ytrain[target]\n    Xtrain_ = Xtrain_.dropna(subset=[target])\n    idx = Xtrain_.index\n    Xtrain_.index = range(len(Xtrain_))\n    ytrain_ = Xtrain_[target]\n    \n    # Filter drop_cols\n    existing_drop_cols = [col for col in drop_cols if col in Xtrain_.columns and col != target]\n    X_features = Xtrain_.drop(existing_drop_cols + [target], axis=1, errors='ignore')\n    \n    # Cross-validation setup\n    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n    \n    # Tune hyperparameters\n    tuned_params = {}\n    for model_type in ['XGB', 'LGBM', 'CB']:\n        tuned_params[model_type] = tune_model(X_features, ytrain_, model_type, cv, n_trials=5)\n    \n    # Model definitions\n    models = {\n        'XGB': XGBRegressor(**tuned_params['XGB'], objective='reg:absoluteerror', random_state=CFG.random_state, early_stopping_rounds=50, tree_method='hist', verbosity=0),\n        'LGBM': LGBMRegressor(**tuned_params['LGBM'], objective='regression_l1', random_state=CFG.random_state, early_stopping_rounds=50, verbosity=-1),\n        'CB': CatBoostRegressor(**tuned_params['CB'], loss_function='MAE', eval_metric='MAE', random_state=CFG.random_state, early_stopping_rounds=50, thread_count=-1, verbose=0)\n    }\n    \n    oof_preds = np.zeros(len(Xtrain_))\n    test_preds = np.zeros(len(Xtest))\n    \n    # Train across folds\n    for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_, ytrain_)):\n        X_tr = Xtrain_.iloc[train_idx].drop(existing_drop_cols + [target], axis=1, errors='ignore')\n        X_dev = Xtrain_.iloc[dev_idx].drop(existing_drop_cols + [target], axis=1, errors='ignore')\n        y_tr, y_dev = ytrain_.iloc[train_idx], ytrain_.iloc[dev_idx]\n        \n        fold_preds = np.zeros(len(X_dev))\n        fold_test_preds = np.zeros(len(Xtest))\n        \n        for name, model in models.items():\n            if name in ['XGB', 'LGBM']:\n                model.fit(X_tr, y_tr, eval_set=[(X_dev, y_dev)], verbose_eval=False)\n            else:\n                model.fit(X_tr, y_tr, eval_set=(X_dev, y_dev), verbose=False)\n            fold_preds += model.predict(X_dev) / len(models)\n            fold_test_preds += model.predict(Xtest.drop([col for col in existing_drop_cols if col in Xtest.columns], axis=1, errors='ignore')) / len(models)\n        \n        oof_preds[dev_idx] = fold_preds\n        test_preds += fold_test_preds / CFG.n_splits\n    \n    # Store predictions\n    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n    Test_Preds.append(pd.DataFrame({target: test_preds}))\n    \n    # Compute score\n    score = mean_absolute_error(ytrain_, oof_preds)\n    print(f\"Gradient Boosting CV score for {target}: {score:.8f}\")\n    \n    clean_memory()\n\n# Combine predictions\ngb_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\ngb_test_preds = pd.concat(Test_Preds, axis=1)\n\n# Submission\nsub_fl = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\ngb_test_preds[\"id\"] = sub_fl[\"id\"]\ngb_test_preds[[\"id\"] + CFG.targets].to_csv(\"submission.csv\", index=False)\nprint(\"\\nGradient Boosting submission saved as submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-06-20T07:36:21.346501Z","iopub.execute_input":"2025-06-20T07:36:21.347376Z","iopub.status.idle":"2025-06-20T07:39:17.031167Z","shell.execute_reply.started":"2025-06-20T07:36:21.347344Z","shell.execute_reply":"2025-06-20T07:39:17.030204Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}}},{"cell_type":"markdown","source":"%%time\n\nOOF_Preds = []\nTest_Preds = []\nArtefacts = {}\ndrop_cols = [\"Source\", \"id\", \"Id\", \"SMILES\"] + CFG.targets\n\nfor target in tqdm(CFG.targets):\n    print(f\"\\n=== Training Gradient Boosting Ensemble for {target} ===\\n\")\n    \n    # Prepare data\n    Xtrain_ = Xtrain.copy()\n    Xtrain_[target] = Ytrain[target]\n    Xtrain_ = Xtrain_.dropna(subset=[target])\n    idx = Xtrain_.index\n    Xtrain_.index = range(len(Xtrain_))\n    ytrain_ = Xtrain_[target]\n    \n    # Filter drop_cols to include only existing columns, excluding the current target\n    existing_drop_cols = [col for col in drop_cols if col in Xtrain_.columns and col != target]\n    \n    # Features for hyperparameter tuning (exclude target)\n    X_features = Xtrain_.drop(existing_drop_cols + [target], axis=1, errors='ignore')\n    \n    # Cross-validation setup\n    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n    \n    # Tune hyperparameters\n    tuned_params = {}\n    for model_type in ['XGB', 'LGBM', 'CB']:\n        tuned_params[model_type] = tune_model(X_features, ytrain_, model_type, cv, n_trials=10)\n    \n    # Model definitions\n    models = {\n        'XGB': XGBRegressor(**tuned_params['XGB'], objective='reg:absoluteerror', random_state=CFG.random_state, verbosity=0),\n        'LGBM': LGBMRegressor(**tuned_params['LGBM'], objective='regression_l1', random_state=CFG.random_state, verbosity=-1),\n        'CB': CatBoostRegressor(**tuned_params['CB'], loss_function='MAE', eval_metric='MAE', random_state=CFG.random_state, verbose=0)\n    }\n    \n    oof_preds = np.zeros(len(Xtrain_))\n    test_preds = np.zeros(len(Xtest))\n    model_oof = {name: np.zeros(len(Xtrain_)) for name in models}\n    \n    # Train across folds\n    for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_, ytrain_)):\n        X_tr = Xtrain_.iloc[train_idx].drop(existing_drop_cols + [target], axis=1, errors='ignore')\n        X_dev = Xtrain_.iloc[dev_idx].drop(existing_drop_cols + [target], axis=1, errors='ignore')\n        y_tr, y_dev = ytrain_.iloc[train_idx], ytrain_.iloc[dev_idx]\n        \n        fold_preds = np.zeros(len(X_dev))\n        fold_test_preds = np.zeros(len(Xtest))\n        \n        for name, model in models.items():\n            model.fit(X_tr, y_tr)\n            model_oof[name][dev_idx] = model.predict(X_dev)\n            fold_preds += model.predict(X_dev) / len(models)\n            fold_test_preds += model.predict(Xtest.drop([col for col in existing_drop_cols if col in Xtest.columns], axis=1, errors='ignore')) / len(models)\n            Artefacts[f\"{target}_{name}_fold{fold}\"] = model\n        \n        oof_preds[dev_idx] = fold_preds\n        test_preds += fold_test_preds / CFG.n_splits\n    \n    # Store predictions\n    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n    Test_Preds.append(pd.DataFrame({target: test_preds}))\n    \n    # Compute score\n    score = mean_absolute_error(ytrain_, oof_preds)\n    print(f\"Ensemble CV score for {target}: {score:.8f}\")\n    \n    clean_memory()\n\n# Combine predictions\ngb_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\ngb_test_preds = pd.concat(Test_Preds, axis=1)","metadata":{"execution":{"iopub.status.busy":"2025-06-20T07:39:58.08172Z","iopub.execute_input":"2025-06-20T07:39:58.082637Z","iopub.status.idle":"2025-06-20T07:39:58.155101Z","shell.execute_reply.started":"2025-06-20T07:39:58.082598Z","shell.execute_reply":"2025-06-20T07:39:58.154203Z"}}},{"cell_type":"markdown","source":"## Approach 2: Neural Network\n\nA multilayer perceptron (MLP) implemented in PyTorch is used to capture complex non-linear relationships in the molecular descriptors. The MLP is trained with a mean absolute error (MAE) loss function, matching the competition’s metric. Early stopping and learning rate scheduling prevent overfitting. This approach is applied to all targets, with larger architectures for FFV due to its higher data availability.","metadata":{}},{"cell_type":"markdown","source":"%%time\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.feature_selection import VarianceThreshold\nfrom tqdm import tqdm\nimport optuna\nimport gc\nfrom torch.cuda.amp import GradScaler, autocast\nfrom myfe import FeatureMaker\n\n# Define configuration\nclass CFG:\n    data_path = \"/kaggle/input/neurips-open-polymer-prediction-2025/\" \n    random_state = 42\n    n_splits = 5  \n    targets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Memory cleanup function\ndef clean_memory():\n    gc.collect()\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\n# Data loading and preprocessing\nprint(\"Loading data...\")\ntrain_df = pd.read_csv(f\"{CFG.data_path}/train.csv\")\ntest_df = pd.read_csv(f\"{CFG.data_path}/test.csv\")\n\n# Initialize feature maker\nfeaturizer = FeatureMaker()\n\n# Generate features\nprint(\"Generating features...\")\nXtrain = featurizer.fit_transform(train_df)\nXtest = featurizer.transform(test_df)\n\n# Separate targets\nYtrain = Xtrain[CFG.targets].copy()\nXtrain = Xtrain.drop(CFG.targets, axis=1, errors='ignore')\n\n# Ensure test set has 'id' column\nif 'id' not in Xtest.columns:\n    Xtest['id'] = test_df['id']\n\nclass PolymerDataset(Dataset):\n    \"\"\"\n    Custom dataset for polymer data with NaN handling.\n    \"\"\"\n    def __init__(self, X, y=None):\n        self.X = torch.tensor(X.values, dtype=torch.float32).nan_to_num(0)\n        self.y = torch.tensor(y.values, dtype=torch.float32) if y is not None else None\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.X[idx], self.y[idx]\n        return self.X[idx]\n\nclass AttentionResidualMLP(nn.Module):\n    \"\"\"\n    Enhanced MLP with attention mechanism, residual connections, and layer normalization.\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, dropout):\n        super(AttentionResidualMLP, self).__init__()\n        self.input_layer = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        # Attention mechanism\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Softmax(dim=-1)\n        )\n        self.residual_block1 = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.residual_block2 = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.LayerNorm(hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.output_layer = nn.Linear(hidden_dim // 2, 1)\n        \n    def forward(self, x):\n        x1 = self.input_layer(x)\n        # Apply attention\n        attn_weights = self.attention(x1)\n        x1 = x1 * attn_weights\n        x2 = self.residual_block1(x1) + x1  # Residual connection\n        x3 = self.residual_block2(x2)\n        return self.output_layer(x3)\n\ndef weighted_mae_loss(outputs, targets, weights):\n    \"\"\"\n    Custom MAE loss with target-specific weights.\n    \"\"\"\n    return torch.mean(weights * torch.abs(outputs.squeeze() - targets))\n\ndef objective(trial, X, y, cv, target):\n    \"\"\"\n    Objective function for Optuna hyperparameter tuning.\n    \"\"\"\n    params = {\n        'hidden_dim': trial.suggest_int('hidden_dim', 128, 512, step=64),  # Wider range\n        'dropout': trial.suggest_float('dropout', 0.1, 0.4),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n    }\n    \n    scores = []\n    for train_idx, dev_idx in cv.split(X, y):\n        X_tr, X_dev = X[train_idx], X[dev_idx]\n        y_tr, y_dev = y[train_idx], y[dev_idx]\n        \n        train_dataset = PolymerDataset(pd.DataFrame(X_tr), pd.Series(y_tr))\n        dev_dataset = PolymerDataset(pd.DataFrame(X_dev), pd.Series(y_dev))\n        \n        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n        dev_loader = DataLoader(dev_dataset, batch_size=params['batch_size'])\n        \n        model = AttentionResidualMLP(input_dim=X_tr.shape[1], hidden_dim=params['hidden_dim'], dropout=params['dropout']).to(device)\n        criterion = nn.L1Loss()\n        optimizer = optim.AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)  # Longer cycle\n        scaler = GradScaler()\n        \n        best_loss = float('inf')\n        patience = 5  # Increased patience\n        counter = 0\n        for epoch in range(50):  # More epochs\n            model.train()\n            for X_batch, y_batch in train_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                optimizer.zero_grad()\n                with autocast():\n                    outputs = model(X_batch)\n                    loss = criterion(outputs.squeeze(), y_batch)\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n                scaler.step(optimizer)\n                scaler.update()\n            \n            model.eval()\n            val_loss = 0\n            with torch.no_grad():\n                for X_batch, y_batch in dev_loader:\n                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                    outputs = model(X_batch)\n                    val_loss += criterion(outputs.squeeze(), y_batch).item() * len(y_batch)\n            val_loss /= len(dev_dataset)\n            scheduler.step()\n            \n            if val_loss < best_loss:\n                best_loss = val_loss\n                counter = 0\n            else:\n                counter += 1\n                if counter >= patience:\n                    break\n        \n        model.eval()\n        preds = []\n        with torch.no_grad():\n            for X_batch, _ in dev_loader:\n                X_batch = X_batch.to(device)\n                preds.extend(model(X_batch).squeeze().cpu().numpy())\n        scores.append(mean_absolute_error(y_dev, preds))\n    \n    return np.mean(scores)\n\nOOF_Preds = []\nTest_Preds = []\ndrop_cols = [\"Source\", \"id\", \"Id\", \"SMILES\"] + CFG.targets\nseeds = [42, 123, 456, 789, 101]  # More seeds for bagging\ntarget_weights = {'Tg': 0.5, 'FFV': 2.0, 'Tc': 1.0, 'Density': 1.0, 'Rg': 0.8}  # wMAE weights\ntarget_scalers = {\n    'Tg': RobustScaler(),\n    'FFV': RobustScaler(),\n    'Tc': RobustScaler(),\n    'Density': RobustScaler(),\n    'Rg': RobustScaler()\n}  # Target-specific scaling\n\nfor target in tqdm(CFG.targets):\n    print(f\"\\n=== Training Neural Network for {target} ===\\n\")\n    \n    # Prepare data\n    Xtrain_ = Xtrain.copy()\n    Xtrain_[target] = Ytrain[target]\n    Xtrain_ = Xtrain_.dropna(subset=[target])\n    idx = Xtrain_.index\n    Xtrain_.index = range(len(Xtrain_))\n    ytrain_ = Xtrain_[target]\n    \n    # Scale target\n    ytrain_scaled = target_scalers[target].fit_transform(ytrain_.values.reshape(-1, 1)).flatten()\n    \n    # Filter drop_cols\n    existing_drop_cols = [col for col in drop_cols if col in Xtrain_.columns and col != target]\n    \n    # Feature selection\n    selector = VarianceThreshold(threshold=0.01)\n    Xtrain_features = Xtrain_.drop(existing_drop_cols + [target], axis=1, errors='ignore')\n    Xtrain_selected = selector.fit_transform(Xtrain_features.fillna(0))\n    Xtest_features = Xtest.drop([col for col in existing_drop_cols if col in Xtest.columns], axis=1, errors='ignore')\n    Xtest_selected = selector.transform(Xtest_features.fillna(0))\n    \n    # Scale features\n    scaler = RobustScaler()\n    Xtrain_scaled = scaler.fit_transform(Xtrain_selected)\n    Xtest_scaled = scaler.transform(Xtest_selected)\n    \n    # Cross-validation setup\n    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n    oof_preds = np.zeros(len(Xtrain_))\n    test_preds = np.zeros(len(Xtest))\n    \n    # Hyperparameter tuning\n    study = optuna.create_study(direction='minimize')\n    study.optimize(lambda trial: objective(trial, Xtrain_scaled, ytrain_scaled, cv, target), n_trials=10)  # More trials\n    best_params = study.best_params\n    \n    # Ensemble across seeds\n    for seed in seeds:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n        \n        for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_scaled, ytrain_scaled)):\n            X_tr, X_dev = Xtrain_scaled[train_idx], Xtrain_scaled[dev_idx]\n            y_tr, y_dev = ytrain_scaled[train_idx], ytrain_scaled[dev_idx]\n            \n            # Create datasets\n            train_dataset = PolymerDataset(pd.DataFrame(X_tr), pd.Series(y_tr))\n            dev_dataset = PolymerDataset(pd.DataFrame(X_dev), pd.Series(y_dev))\n            test_dataset = PolymerDataset(pd.DataFrame(Xtest_scaled))\n            \n            train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n            dev_loader = DataLoader(dev_dataset, batch_size=best_params['batch_size'])\n            test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n            \n            # Initialize model\n            model = AttentionResidualMLP(input_dim=X_tr.shape[1], hidden_dim=best_params['hidden_dim'], dropout=best_params['dropout']).to(device)\n            criterion = lambda x, y: weighted_mae_loss(x, y, torch.tensor(target_weights[target], device=device))\n            optimizer = optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n            scaler = GradScaler()\n            \n            # Training loop\n            best_loss = float('inf')\n            patience = 5\n            counter = 0\n            for epoch in range(50):\n                model.train()\n                for X_batch, y_batch in train_loader:\n                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                    optimizer.zero_grad()\n                    with autocast():\n                        outputs = model(X_batch)\n                        loss = criterion(outputs, y_batch)\n                    scaler.scale(loss).backward()\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                \n                # Validation\n                model.eval()\n                val_loss = 0\n                with torch.no_grad():\n                    for X_batch, y_batch in dev_loader:\n                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                        outputs = model(X_batch)\n                        val_loss += criterion(outputs, y_batch).item() * len(y_batch)\n                val_loss /= len(dev_dataset)\n                scheduler.step()\n                \n                # Early stopping\n                if val_loss < best_loss:\n                    best_loss = val_loss\n                    counter = 0\n                    best_model = model.state_dict()\n                else:\n                    counter += 1\n                    if counter >= patience:\n                        break\n            \n            # Load best model\n            model.load_state_dict(best_model)\n            model.eval()\n            with torch.no_grad():\n                dev_preds = []\n                for X_batch, _ in dev_loader:\n                    X_batch = X_batch.to(device)\n                    dev_preds.extend(model(X_batch).squeeze().cpu().numpy())\n                oof_preds[dev_idx] += target_scalers[target].inverse_transform(np.array(dev_preds).reshape(-1, 1)).flatten() / (CFG.n_splits * len(seeds))\n                \n                test_preds_fold = []\n                for X_batch in test_loader:\n                    X_batch = X_batch.to(device)\n                    test_preds_fold.extend(model(X_batch).squeeze().cpu().numpy())\n                test_preds += target_scalers[target].inverse_transform(np.array(test_preds_fold).reshape(-1, 1)).flatten() / (CFG.n_splits * len(seeds))\n    \n    # Store predictions\n    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n    Test_Preds.append(pd.DataFrame({target: test_preds}))\n    \n    # Compute score\n    score = mean_absolute_error(ytrain_, oof_preds)\n    print(f\"Neural Network CV score for {target}: {score:.8f}\")\n    \n    clean_memory()\n\n# Combine predictions\nnn_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\nnn_test_preds = pd.concat(Test_Preds, axis=1)\n\n# Submission\nsub_fl = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\nnn_test_preds[\"id\"] = sub_fl[\"id\"]\nnn_test_preds[[\"id\"] + CFG.targets].to_csv(\"submission.csv\", index=False)\nprint(\"\\nNeural Network submission saved as submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-06-20T07:53:10.036265Z","iopub.execute_input":"2025-06-20T07:53:10.036897Z","iopub.status.idle":"2025-06-20T08:40:06.652168Z","shell.execute_reply.started":"2025-06-20T07:53:10.036866Z","shell.execute_reply":"2025-06-20T08:40:06.651312Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}}},{"cell_type":"markdown","source":"## Approach 3: Graph Neural Network\n\nGraph Neural Networks (GNNs) model polymers as molecular graphs, where atoms are nodes and bonds are edges. This captures spatial and structural relationships better than descriptor-based methods. We use DeepChem’s GraphConvModel, which is well-suited for molecular data. Due to computational constraints, we apply this to FFV and optionally to other targets with reduced epochs for sparse data.","metadata":{}},{"cell_type":"markdown","source":"%%time\n\nimport deepchem as dc\nfrom deepchem.models import GraphConvModel\n\nOOF_Preds = []\nTest_Preds = []\ndrop_cols = [\"Source\", \"id\", \"Id\", \"SMILES\"] + CFG.targets\n\nfor target in tqdm(['FFV']):\n    print(f\"\\n=== Training Graph Neural Network for {target} ===\\n\")\n    \n    # Prepare data\n    Xtrain_ = Xtrain.copy()\n    Xtrain_[target] = Ytrain[target]\n    Xtrain_ = Xtrain_.dropna(subset=[target])\n    idx = Xtrain_.index\n    Xtrain_.index = range(len(Xtrain_))\n    ytrain_ = Xtrain_[target]\n    \n    # Convert SMILES to molecular graphs\n    featurizer = dc.feat.ConvMolFeaturizer()\n    train_mols = [Chem.MolFromSmiles(smi) for smi in Xtrain_['SMILES']]\n    test_mols = [Chem.MolFromSmiles(smi) for smi in Xtest['SMILES']]\n    \n    train_features = featurizer.featurize(train_mols)\n    test_features = featurizer.featurize(test_mols)\n    \n    # Filter invalid molecules\n    valid_train_idx = [i for i, feat in enumerate(train_features) if isinstance(feat, dc.feat.mol_graphs.ConvMol)]\n    valid_test_idx = [i for i, feat in enumerate(test_features) if isinstance(feat, dc.feat.mol_graphs.ConvMol)]\n    \n    train_features = [train_features[i] for i in valid_train_idx]\n    ytrain_ = ytrain_.iloc[valid_train_idx]\n    idx = idx[valid_train_idx]\n    test_features = [test_features[i] for i in valid_test_idx]\n    \n    # Cross-validation setup\n    cv = KFold(n_splits=3, shuffle=True, random_state=CFG.random_state)\n    oof_preds = np.zeros(len(ytrain_))\n    test_preds = np.zeros(len(test_mols))\n    \n    for fold, (train_idx, dev_idx) in enumerate(cv.split(train_features, ytrain_)):\n        X_tr = [train_features[i] for i in train_idx]\n        X_dev = [train_features[i] for i in dev_idx]\n        y_tr = ytrain_.iloc[train_idx]\n        y_dev = ytrain_.iloc[dev_idx]\n        \n        # Create datasets\n        train_dataset = dc.data.NumpyDataset(X_tr, y_tr.values)\n        dev_dataset = dc.data.NumpyDataset(X_dev, y_dev.values)\n        test_dataset = dc.data.NumpyDataset(test_features)\n        \n        # Initialize model\n        model = GraphConvModel(n_tasks=1, mode='regression', batch_size=16, learning_rate=0.001, graph_conv_layers=[32, 32])\n        \n        # Train model\n        model.fit(train_dataset, nb_epoch=30)\n        \n        # Predict\n        oof_preds[dev_idx] = model.predict(dev_dataset).flatten()\n        test_preds[valid_test_idx] += model.predict(test_dataset).flatten() / 3\n    \n    # Store predictions\n    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n    Test_Preds.append(pd.DataFrame({target: test_preds}))\n    \n    # Compute score\n    score = mean_absolute_error(ytrain_, oof_preds)\n    print(f\"GNN CV score for {target}: {score:.8f}\")\n    \n    clean_memory()\n\n# Combine predictions (fill others with NaN)\ngnn_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\ngnn_test_preds = pd.concat(Test_Preds, axis=1)\nfor t in CFG.targets:\n    if t not in gnn_oof_preds.columns:\n        gnn_oof_preds[t] = np.nan\n        gnn_test_preds[t] = np.nan\n\n# Submission\nsub_fl = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\ngnn_test_preds[\"id\"] = sub_fl[\"id\"]\ngnn_test_preds[[\"id\"] + CFG.targets].to_csv(\"submission_gnn.csv\", index=False)\nprint(\"\\nGNN submission saved as submission_gnn.csv\")","metadata":{}},{"cell_type":"markdown","source":"## Approach 4: Stacking Ensemble\nThe stacking ensemble combines predictions from the gradient boosting models and neural network using a meta-learner (Linear Regression). This approach leverages the strengths of individual models to improve overall accuracy. First-level predictions are generated via cross-validation, and the meta-learner is trained on these predictions to produce the final output.","metadata":{}},{"cell_type":"code","source":"\n%%time\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Install dependencies\n#!pip install pandas numpy torch scikit-learn optuna tqdm rdkit xgboost lightgbm\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.linear_model import Ridge\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom rdkit import Chem, rdBase\nfrom rdkit.Chem import Descriptors, AllChem\nfrom rdkit.ML.Descriptors import MoleculeDescriptors\nfrom tqdm import tqdm\nimport optuna\nimport gc\nfrom torch.cuda.amp import GradScaler, autocast\nrdBase.DisableLog('rdApp.warning')\n\n# Configuration\nclass CFG:\n    data_path = \"/kaggle/input/neurips-open-polymer-prediction-2025/\"\n    targets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n    n_splits = 5\n    random_state = 42\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Memory cleanup\ndef clean_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n# === SCRIPT 1: GBDT STACKING MODEL ===\nprint(\"--- RUNNING SCRIPT 1: GBDT STACKING MODEL ---\")\n\n# Feature Engineering\ndef generate_rdkit_features(smiles_str: str):\n    mol = Chem.MolFromSmiles(smiles_str)\n    desc_list = [d[0] for d in Descriptors._descList]\n    morgan_fp_size = 256  # Reduced for efficiency\n    if mol is None:\n        return np.full(len(desc_list) + morgan_fp_size, np.nan)\n    calculator = MoleculeDescriptors.MolecularDescriptorCalculator(desc_list)\n    descriptors = np.array(calculator.CalcDescriptors(mol))\n    descriptors[desc_list.index(\"Ipc\")] = np.log1p(descriptors[desc_list.index(\"Ipc\")])  # Log-transform Ipc\n    mfp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=morgan_fp_size)\n    mfp_array = np.array(list(mfp.ToBitString())).astype(int)\n    return np.concatenate([descriptors, mfp_array])\n\n# Load data\ntrain_df = pd.read_csv(f\"{CFG.data_path}/train.csv\")\ntest_df = pd.read_csv(f\"{CFG.data_path}/test.csv\")\nsample_df = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\n\n# Generate features\ndesc_list_names = [d[0] for d in Descriptors._descList]\nfp_morgan_cols = [f'mfp_{i}' for i in range(256)]\nfeature_columns = desc_list_names + fp_morgan_cols\nX = pd.DataFrame(np.vstack([generate_rdkit_features(s) for s in train_df['SMILES']]), columns=feature_columns)\nX_test = pd.DataFrame(np.vstack([generate_rdkit_features(s) for s in test_df['SMILES']]), columns=feature_columns)\n\n# Preprocess features\nf32_max = np.finfo(np.float32).max\nfor df in [X, X_test]:\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df[df > f32_max] = np.nan\n    df[df < -f32_max] = np.nan\nimpute_values = X.mean()\nX.fillna(impute_values, inplace=True)\nX_test = X_test.reindex(columns=X.columns).fillna(impute_values)\n\n# GBDT parameters\nXGB_PARAMS = {'n_estimators': 2000, 'learning_rate': 0.02, 'max_depth': 6, 'subsample': 0.7, 'colsample_bytree': 0.6, 'random_state': CFG.random_state, 'n_jobs': -1, 'tree_method': 'hist'}\nLGBM_PARAMS = {'n_estimators': 2000, 'learning_rate': 0.02, 'max_depth': 7, 'subsample': 0.7, 'colsample_bytree': 0.6, 'random_state': CFG.random_state, 'n_jobs': -1, 'verbosity': -1}\nMETA_MODEL = Ridge(alpha=1.0, random_state=CFG.random_state)\n\ngbdt_oof_df = pd.DataFrame(index=train_df.index)\ngbdt_predictions_df = pd.DataFrame({'id': test_df['id']})\n\nfor target in CFG.targets:\n    print(f\"  Training GBDT for {target}...\")\n    y = train_df[target].dropna()\n    X_subset = X.loc[y.index]\n    \n    oof_preds_xgb = pd.Series(np.zeros(len(X_subset)), index=X_subset.index)\n    oof_preds_lgb = pd.Series(np.zeros(len(X_subset)), index=X_subset.index)\n    test_preds_xgb_folds = np.zeros((len(X_test), CFG.n_splits))\n    test_preds_lgb_folds = np.zeros((len(X_test), CFG.n_splits))\n    \n    kf = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X_subset, y)):\n        X_train_fold, y_train_fold = X_subset.iloc[train_idx], y.iloc[train_idx]\n        X_val_fold, y_val_fold = X_subset.iloc[val_idx], y.iloc[val_idx]\n        \n        # XGBoost\n        xgb_model = xgb.XGBRegressor(**XGB_PARAMS)\n        xgb_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], callbacks=[xgb.callback.EarlyStopping(50, save_best=True)], verbose=0)\n        oof_preds_xgb.iloc[val_idx] = xgb_model.predict(X_val_fold)\n        test_preds_xgb_folds[:, fold] = xgb_model.predict(X_test)\n        \n        # LightGBM\n        lgb_model = lgb.LGBMRegressor(**LGBM_PARAMS)\n        lgb_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], callbacks=[lgb.early_stopping(50, verbose=False)])\n        oof_preds_lgb.iloc[val_idx] = lgb_model.predict(X_val_fold)\n        test_preds_lgb_folds[:, fold] = lgb_model.predict(X_test)\n    \n    # Meta-model\n    X_meta_train = pd.concat([oof_preds_xgb, oof_preds_lgb], axis=1)\n    X_meta_test = pd.DataFrame({'xgb': np.mean(test_preds_xgb_folds, axis=1), 'lgb': np.mean(test_preds_lgb_folds, axis=1)})\n    meta_model = META_MODEL\n    meta_model.fit(X_meta_train, y)\n    gbdt_predictions_df[target] = meta_model.predict(X_meta_test)\n    gbdt_oof_df.loc[X_meta_train.index, target] = meta_model.predict(X_meta_train)\n\ngbdt_predictions_df.to_csv('submission_gbdt.csv', index=False)\ngbdt_oof_df.to_csv('oof_gbdt.csv')\nprint(\"--- SCRIPT 1 COMPLETE: GBDT predictions and OOF file saved. ---\")\nclean_memory()\n\n# === SCRIPT 2: NEURAL NETWORK MODEL ===\nprint(\"\\n--- RUNNING SCRIPT 2: NEURAL NETWORK MODEL ---\")\n\nclass PolymerDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.tensor(X.values, dtype=torch.float32).nan_to_num(0)\n        self.y = torch.tensor(y.values, dtype=torch.float32) if y is not None else None\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.X[idx], self.y[idx]\n        return self.X[idx]\n\nclass AttentionResidualMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, dropout):\n        super(AttentionResidualMLP, self).__init__()\n        self.input_layer = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Softmax(dim=-1)\n        )\n        self.residual_block1 = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.residual_block2 = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.LayerNorm(hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.output_layer = nn.Linear(hidden_dim // 2, 1)\n        \n    def forward(self, x):\n        x1 = self.input_layer(x)\n        attn_weights = self.attention(x1)\n        x1 = x1 * attn_weights\n        x2 = self.residual_block1(x1) + x1\n        x3 = self.residual_block2(x2)\n        return self.output_layer(x3)\n\ndef weighted_mae_loss(outputs, targets, weights):\n    return torch.mean(weights * torch.abs(outputs.squeeze() - targets))\n\ndef objective(trial, X, y, cv, target):\n    params = {\n        'hidden_dim': trial.suggest_int('hidden_dim', 128, 512, step=64),\n        'dropout': trial.suggest_float('dropout', 0.1, 0.4),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n    }\n    \n    scores = []\n    for train_idx, dev_idx in cv.split(X, y):\n        X_tr, X_dev = X[train_idx], X[dev_idx]\n        y_tr, y_dev = y[train_idx], y[dev_idx]\n        \n        train_dataset = PolymerDataset(pd.DataFrame(X_tr), pd.Series(y_tr))\n        dev_dataset = PolymerDataset(pd.DataFrame(X_dev), pd.Series(y_dev))\n        \n        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n        dev_loader = DataLoader(dev_dataset, batch_size=params['batch_size'])\n        \n        model = AttentionResidualMLP(input_dim=X_tr.shape[1], hidden_dim=params['hidden_dim'], dropout=params['dropout']).to(CFG.device)\n        criterion = nn.L1Loss()\n        optimizer = optim.AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n        scaler = GradScaler()\n        \n        best_loss = float('inf')\n        patience = 5\n        counter = 0\n        for epoch in range(50):\n            model.train()\n            for X_batch, y_batch in train_loader:\n                X_batch, y_batch = X_batch.to(CFG.device), y_batch.to(CFG.device)\n                optimizer.zero_grad()\n                with autocast():\n                    outputs = model(X_batch)\n                    loss = criterion(outputs.squeeze(), y_batch)\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            \n            model.eval()\n            val_loss = 0\n            with torch.no_grad():\n                for X_batch, y_batch in dev_loader:\n                    X_batch, y_batch = X_batch.to(CFG.device), y_batch.to(CFG.device)\n                    outputs = model(X_batch)\n                    val_loss += criterion(outputs.squeeze(), y_batch).item() * len(y_batch)\n            val_loss /= len(dev_dataset)\n            scheduler.step()\n            \n            if val_loss < best_loss:\n                best_loss = val_loss\n                counter = 0\n            else:\n                counter += 1\n                if counter >= patience:\n                    break\n        \n        model.eval()\n        preds = []\n        with torch.no_grad():\n            for X_batch, _ in dev_loader:\n                X_batch = X_batch.to(CFG.device)\n                preds.extend(model(X_batch).squeeze().cpu().numpy())\n        scores.append(mean_absolute_error(y_dev, preds))\n    \n    return np.mean(scores)\n\nnn_oof_df = pd.DataFrame(index=train_df.index)\nnn_predictions_df = pd.DataFrame({'id': test_df['id']})\ntarget_weights = {'Tg': 0.5, 'FFV': 2.0, 'Tc': 1.0, 'Density': 1.0, 'Rg': 0.8}\ntarget_scalers = {t: RobustScaler() for t in CFG.targets}\n\nfor target in tqdm(CFG.targets, desc=\"NN Targets\"):\n    print(f\"  Training Neural Network for {target}...\")\n    \n    # Prepare data\n    Xtrain_ = X.loc[train_df[target].dropna().index]\n    ytrain_ = train_df.loc[Xtrain_.index, target]\n    \n    # Scale target\n    ytrain_scaled = target_scalers[target].fit_transform(ytrain_.values.reshape(-1, 1)).flatten()\n    \n    # Feature selection\n    selector = VarianceThreshold(threshold=0.01)\n    Xtrain_selected = selector.fit_transform(Xtrain_.fillna(0))\n    Xtest_selected = selector.transform(X_test.fillna(0))\n    \n    # Scale features\n    scaler = RobustScaler()\n    Xtrain_scaled = scaler.fit_transform(Xtrain_selected)\n    Xtest_scaled = scaler.transform(Xtest_selected)\n    \n    # Hyperparameter tuning\n    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n    study = optuna.create_study(direction='minimize')\n    study.optimize(lambda trial: objective(trial, Xtrain_scaled, ytrain_scaled, cv, target), n_trials=10)\n    best_params = study.best_params\n    \n    # Train final model\n    oof_preds = np.zeros(len(Xtrain_))\n    test_preds = np.zeros(len(X_test))\n    seeds = [42, 123, 456]  # Reduced for speed\n    \n    for seed in seeds:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n        \n        for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_scaled, ytrain_scaled)):\n            X_tr, X_dev = Xtrain_scaled[train_idx], Xtrain_scaled[dev_idx]\n            y_tr, y_dev = ytrain_scaled[train_idx], ytrain_scaled[dev_idx]\n            \n            train_dataset = PolymerDataset(pd.DataFrame(X_tr), pd.Series(y_tr))\n            dev_dataset = PolymerDataset(pd.DataFrame(X_dev), pd.Series(y_dev))\n            test_dataset = PolymerDataset(pd.DataFrame(Xtest_scaled))\n            \n            train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n            dev_loader = DataLoader(dev_dataset, batch_size=best_params['batch_size'])\n            test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n            \n            model = AttentionResidualMLP(input_dim=X_tr.shape[1], hidden_dim=best_params['hidden_dim'], dropout=best_params['dropout']).to(CFG.device)\n            criterion = lambda x, y: weighted_mae_loss(x, y, torch.tensor(target_weights[target], device=CFG.device))\n            optimizer = optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n            scaler = GradScaler()\n            \n            best_loss = float('inf')\n            patience = 5\n            counter = 0\n            for epoch in range(50):\n                model.train()\n                for X_batch, y_batch in train_loader:\n                    X_batch, y_batch = X_batch.to(CFG.device), y_batch.to(CFG.device)\n                    optimizer.zero_grad()\n                    with autocast():\n                        outputs = model(X_batch)\n                        loss = criterion(outputs, y_batch)\n                    scaler.scale(loss).backward()\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                \n                model.eval()\n                val_loss = 0\n                with torch.no_grad():\n                    for X_batch, y_batch in dev_loader:\n                        X_batch, y_batch = X_batch.to(CFG.device), y_batch.to(CFG.device)\n                        outputs = model(X_batch)\n                        val_loss += criterion(outputs, y_batch).item() * len(y_batch)\n                val_loss /= len(dev_dataset)\n                scheduler.step()\n                \n                if val_loss < best_loss:\n                    best_loss = val_loss\n                    counter = 0\n                    best_model = model.state_dict()\n                else:\n                    counter += 1\n                    if counter >= patience:\n                        break\n            \n            model.load_state_dict(best_model)\n            model.eval()\n            with torch.no_grad():\n                dev_preds = []\n                for X_batch, _ in dev_loader:\n                    X_batch = X_batch.to(CFG.device)\n                    dev_preds.extend(model(X_batch).squeeze().cpu().numpy())\n                oof_preds[dev_idx] += target_scalers[target].inverse_transform(np.array(dev_preds).reshape(-1, 1)).flatten() / (CFG.n_splits * len(seeds))\n                \n                test_preds_fold = []\n                for X_batch in test_loader:\n                    X_batch = X_batch.to(CFG.device)\n                    test_preds_fold.extend(model(X_batch).squeeze().cpu().numpy())\n                test_preds += target_scalers[target].inverse_transform(np.array(test_preds_fold).reshape(-1, 1)).flatten() / (CFG.n_splits * len(seeds))\n    \n    nn_oof_df.loc[Xtrain_.index, target] = oof_preds\n    nn_predictions_df[target] = test_preds\n    \n    score = mean_absolute_error(ytrain_, oof_preds)\n    print(f\"  Neural Network CV score for {target}: {score:.8f}\")\n\nnn_predictions_df.to_csv('submission_nn.csv', index=False)\nnn_oof_df.to_csv('oof_nn.csv')\nprint(\"--- SCRIPT 2 COMPLETE: Neural Network predictions and OOF file saved. ---\")\nclean_memory()\n\n# === SCRIPT 3: OPTIMAL BLENDING ===\nprint(\"\\n--- RUNNING SCRIPT 3: OPTIMAL BLENDING ---\")\n\nnn_test_preds = pd.read_csv('submission_nn.csv')\ngbdt_test_preds = pd.read_csv('submission_gbdt.csv')\nnn_oof_df = pd.read_csv('oof_nn.csv', index_col=0)\ngbdt_oof_df = pd.read_csv('oof_gbdt.csv', index_col=0)\n\nfinal_submission = pd.DataFrame({'id': nn_test_preds['id']})\nbest_weights = {}\n\nprint(\"Finding optimal blend weights...\")\nfor target in CFG.targets:\n    oof_df = pd.concat([train_df[target], nn_oof_df[target], gbdt_oof_df[target]], axis=1)\n    oof_df.columns = ['true', 'nn', 'gbdt']\n    oof_df.dropna(inplace=True)\n    \n    best_rmse = float('inf')\n    best_w = 0.5\n    for w in np.arange(0.0, 1.01, 0.01):\n        blend_preds = w * oof_df['nn'] + (1 - w) * oof_df['gbdt']\n        rmse = np.sqrt(mean_squared_error(oof_df['true'], blend_preds))\n        if rmse < best_rmse:\n            best_rmse = rmse\n            best_w = w\n    \n    best_weights[target] = best_w\n    print(f\"  Best weight for {target}: {best_w:.2f} (Local RMSE: {best_rmse:.5f})\")\n\nprint(\"\\nBlending test predictions...\")\nfor col in nn_test_preds.columns:\n    if col != 'id':\n        w = best_weights.get(col, 0.5)\n        final_submission[col] = (w * nn_test_preds[col]) + ((1 - w) * gbdt_test_preds[col])\n\nfinal_submission = final_submission[sample_df.columns]\nfinal_submission.to_csv('submission.csv', index=False)\nprint(\"\\nFinal submission saved as submission.csv\")\nprint(\"Preview:\")\nprint(final_submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T08:40:43.926471Z","iopub.execute_input":"2025-06-22T08:40:43.927323Z","iopub.status.idle":"2025-06-22T09:34:49.817625Z","shell.execute_reply.started":"2025-06-22T08:40:43.927289Z","shell.execute_reply":"2025-06-22T09:34:49.817026Z"}},"outputs":[{"name":"stdout","text":"--- RUNNING SCRIPT 1: GBDT STACKING MODEL ---\n  Training GBDT for Tg...\n  Training GBDT for FFV...\n  Training GBDT for Tc...\n  Training GBDT for Density...\n  Training GBDT for Rg...\n--- SCRIPT 1 COMPLETE: GBDT predictions and OOF file saved. ---\n\n--- RUNNING SCRIPT 2: NEURAL NETWORK MODEL ---\n","output_type":"stream"},{"name":"stderr","text":"NN Targets:   0%|          | 0/5 [00:00<?, ?it/s][I 2025-06-22 08:46:47,705] A new study created in memory with name: no-name-3ece7190-7cf9-4516-a071-6b5dbb18d9cc\n","output_type":"stream"},{"name":"stdout","text":"  Training Neural Network for Tg...\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-22 08:47:04,966] Trial 0 finished with value: 0.35420426825301615 and parameters: {'hidden_dim': 256, 'dropout': 0.3517739860684512, 'learning_rate': 0.00025277362113591367, 'batch_size': 16, 'weight_decay': 0.00037240233239859696}. Best is trial 0 with value: 0.35420426825301615.\n[I 2025-06-22 08:47:08,255] Trial 1 finished with value: 0.36776235010910346 and parameters: {'hidden_dim': 320, 'dropout': 0.20953154421286027, 'learning_rate': 0.008926245272316684, 'batch_size': 64, 'weight_decay': 6.704318119738742e-05}. Best is trial 0 with value: 0.35420426825301615.\n[I 2025-06-22 08:47:18,668] Trial 2 finished with value: 0.3498868459001646 and parameters: {'hidden_dim': 128, 'dropout': 0.3929243300162585, 'learning_rate': 0.0020151433698454963, 'batch_size': 16, 'weight_decay': 2.349635085026228e-06}. Best is trial 2 with value: 0.3498868459001646.\n[I 2025-06-22 08:47:29,850] Trial 3 finished with value: 0.3649879094034607 and parameters: {'hidden_dim': 320, 'dropout': 0.1821111419977161, 'learning_rate': 0.00014968742444952123, 'batch_size': 16, 'weight_decay': 2.2143170610694144e-06}. Best is trial 2 with value: 0.3498868459001646.\n[I 2025-06-22 08:47:36,011] Trial 4 finished with value: 0.565615625787119 and parameters: {'hidden_dim': 448, 'dropout': 0.16579986296153754, 'learning_rate': 1.586702913429803e-05, 'batch_size': 64, 'weight_decay': 8.802379196081185e-06}. Best is trial 2 with value: 0.3498868459001646.\n[I 2025-06-22 08:47:44,023] Trial 5 finished with value: 0.3663624198979787 and parameters: {'hidden_dim': 256, 'dropout': 0.39162667144169017, 'learning_rate': 0.00022356034512772694, 'batch_size': 32, 'weight_decay': 6.66441693142654e-05}. Best is trial 2 with value: 0.3498868459001646.\n[I 2025-06-22 08:47:52,288] Trial 6 finished with value: 0.4793514962918528 and parameters: {'hidden_dim': 384, 'dropout': 0.26190139013068126, 'learning_rate': 4.547250020852945e-05, 'batch_size': 64, 'weight_decay': 1.5927029179938144e-05}. Best is trial 2 with value: 0.3498868459001646.\n[I 2025-06-22 08:47:56,963] Trial 7 finished with value: 0.4268594852557978 and parameters: {'hidden_dim': 512, 'dropout': 0.39601662477816346, 'learning_rate': 0.009810806673013676, 'batch_size': 32, 'weight_decay': 4.115725358484298e-05}. Best is trial 2 with value: 0.3498868459001646.\n[I 2025-06-22 08:48:01,417] Trial 8 finished with value: 0.3592472792611214 and parameters: {'hidden_dim': 192, 'dropout': 0.3007004585587357, 'learning_rate': 0.0033616198708741277, 'batch_size': 32, 'weight_decay': 0.0007009486008284659}. Best is trial 2 with value: 0.3498868459001646.\n[I 2025-06-22 08:48:06,713] Trial 9 finished with value: 0.35651234375285384 and parameters: {'hidden_dim': 256, 'dropout': 0.14148230976915074, 'learning_rate': 0.00032814562214291125, 'batch_size': 32, 'weight_decay': 4.166499848198983e-05}. Best is trial 2 with value: 0.3498868459001646.\nNN Targets:  20%|██        | 1/5 [01:46<07:04, 106.24s/it]","output_type":"stream"},{"name":"stdout","text":"  Neural Network CV score for Tg: 92.93586760\n  Training Neural Network for FFV...\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-22 08:48:34,111] A new study created in memory with name: no-name-04287224-5171-412f-9cbb-7112ae244d8b\n[I 2025-06-22 08:53:02,994] Trial 0 finished with value: 0.17797374710038025 and parameters: {'hidden_dim': 448, 'dropout': 0.14615668800426612, 'learning_rate': 0.00025591273001754713, 'batch_size': 16, 'weight_decay': 0.0003701465886412841}. Best is trial 0 with value: 0.17797374710038025.\n[I 2025-06-22 08:54:52,195] Trial 1 finished with value: 0.1935374603788433 and parameters: {'hidden_dim': 192, 'dropout': 0.2673043670854641, 'learning_rate': 0.0035401199049082633, 'batch_size': 32, 'weight_decay': 0.00013373181205698917}. Best is trial 0 with value: 0.17797374710038025.\n[I 2025-06-22 08:55:55,233] Trial 2 finished with value: 0.21325790737006942 and parameters: {'hidden_dim': 128, 'dropout': 0.33348747717824334, 'learning_rate': 0.0003435473265071812, 'batch_size': 64, 'weight_decay': 0.00021627285188061986}. Best is trial 0 with value: 0.17797374710038025.\n[I 2025-06-22 09:00:37,421] Trial 3 finished with value: 0.2289514527063928 and parameters: {'hidden_dim': 128, 'dropout': 0.2321057209903799, 'learning_rate': 6.5012783617985e-05, 'batch_size': 16, 'weight_decay': 0.0002981934614906535}. Best is trial 0 with value: 0.17797374710038025.\n[I 2025-06-22 09:03:18,727] Trial 4 finished with value: 0.2171084879105552 and parameters: {'hidden_dim': 128, 'dropout': 0.21163587856582092, 'learning_rate': 0.0001048443052079252, 'batch_size': 32, 'weight_decay': 0.0006141919050287876}. Best is trial 0 with value: 0.17797374710038025.\n[I 2025-06-22 09:07:46,049] Trial 5 finished with value: 0.1835332816061141 and parameters: {'hidden_dim': 256, 'dropout': 0.1887630260854718, 'learning_rate': 0.00024452723874802837, 'batch_size': 16, 'weight_decay': 0.0002032725204269442}. Best is trial 0 with value: 0.17797374710038025.\n[I 2025-06-22 09:08:54,970] Trial 6 finished with value: 0.19217839404818962 and parameters: {'hidden_dim': 256, 'dropout': 0.31948707900387724, 'learning_rate': 0.0033413962824700616, 'batch_size': 64, 'weight_decay': 4.072495661358327e-05}. Best is trial 0 with value: 0.17797374710038025.\n[I 2025-06-22 09:10:06,985] Trial 7 finished with value: 0.1907799218384078 and parameters: {'hidden_dim': 256, 'dropout': 0.13268593985654736, 'learning_rate': 0.0001978357233187318, 'batch_size': 64, 'weight_decay': 0.000627802093380283}. Best is trial 0 with value: 0.17797374710038025.\n[I 2025-06-22 09:12:11,310] Trial 8 finished with value: 0.24210513919054483 and parameters: {'hidden_dim': 128, 'dropout': 0.26988099778223185, 'learning_rate': 0.006092715654503768, 'batch_size': 16, 'weight_decay': 1.4377851834357982e-06}. Best is trial 0 with value: 0.17797374710038025.\n[I 2025-06-22 09:13:24,040] Trial 9 finished with value: 0.19053989201682325 and parameters: {'hidden_dim': 512, 'dropout': 0.21811498344899932, 'learning_rate': 0.00018224373469773742, 'batch_size': 64, 'weight_decay': 6.201801436204832e-06}. Best is trial 0 with value: 0.17797374710038025.\nNN Targets:  40%|████      | 2/5 [40:47<1:11:03, 1421.20s/it][I 2025-06-22 09:27:35,601] A new study created in memory with name: no-name-a19f537b-8218-4b18-9a29-12880395c56e\n","output_type":"stream"},{"name":"stdout","text":"  Neural Network CV score for FFV: 0.29388978\n  Training Neural Network for Tc...\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-22 09:27:42,168] Trial 0 finished with value: 0.19758240843273678 and parameters: {'hidden_dim': 448, 'dropout': 0.1784073049301355, 'learning_rate': 0.0005654008849468417, 'batch_size': 32, 'weight_decay': 0.00033286445959572236}. Best is trial 0 with value: 0.19758240843273678.\n[I 2025-06-22 09:27:51,471] Trial 1 finished with value: 0.18772998726599538 and parameters: {'hidden_dim': 128, 'dropout': 0.2539643095973735, 'learning_rate': 0.0003803100587851109, 'batch_size': 32, 'weight_decay': 0.00028944986952618104}. Best is trial 1 with value: 0.18772998726599538.\n[I 2025-06-22 09:28:01,023] Trial 2 finished with value: 0.3912989623406075 and parameters: {'hidden_dim': 256, 'dropout': 0.38808925583218057, 'learning_rate': 3.240115406203966e-05, 'batch_size': 64, 'weight_decay': 0.0006520249145757331}. Best is trial 1 with value: 0.18772998726599538.\n[I 2025-06-22 09:28:04,615] Trial 3 finished with value: 0.19030474712889153 and parameters: {'hidden_dim': 384, 'dropout': 0.35341175848104067, 'learning_rate': 0.0022897071450540146, 'batch_size': 64, 'weight_decay': 5.6669577199562827e-05}. Best is trial 1 with value: 0.18772998726599538.\n[I 2025-06-22 09:28:21,589] Trial 4 finished with value: 0.18310733671139995 and parameters: {'hidden_dim': 384, 'dropout': 0.2970754361711304, 'learning_rate': 0.0020824035024843454, 'batch_size': 16, 'weight_decay': 0.00046010309207773835}. Best is trial 4 with value: 0.18310733671139995.\n[I 2025-06-22 09:28:43,410] Trial 5 finished with value: 0.19125371350851914 and parameters: {'hidden_dim': 448, 'dropout': 0.36867781155384105, 'learning_rate': 0.004451065610740957, 'batch_size': 16, 'weight_decay': 0.000498603965593442}. Best is trial 4 with value: 0.18310733671139995.\n[I 2025-06-22 09:28:57,262] Trial 6 finished with value: 0.19253182030021349 and parameters: {'hidden_dim': 320, 'dropout': 0.350132351729709, 'learning_rate': 0.0009855631397353297, 'batch_size': 16, 'weight_decay': 1.8776552874961573e-05}. Best is trial 4 with value: 0.18310733671139995.\n[I 2025-06-22 09:29:11,883] Trial 7 finished with value: 0.19033985797951364 and parameters: {'hidden_dim': 384, 'dropout': 0.3309975709302061, 'learning_rate': 0.00014063491906257027, 'batch_size': 32, 'weight_decay': 1.7308078495590793e-05}. Best is trial 4 with value: 0.18310733671139995.\n[I 2025-06-22 09:29:19,934] Trial 8 finished with value: 0.19267784740768953 and parameters: {'hidden_dim': 512, 'dropout': 0.18420032326385927, 'learning_rate': 0.000953528917772435, 'batch_size': 32, 'weight_decay': 1.0816914512177965e-05}. Best is trial 4 with value: 0.18310733671139995.\n[I 2025-06-22 09:29:55,757] Trial 9 finished with value: 0.23781770694206789 and parameters: {'hidden_dim': 384, 'dropout': 0.3244932130437667, 'learning_rate': 2.4315163510553127e-05, 'batch_size': 16, 'weight_decay': 0.00018026446575867048}. Best is trial 4 with value: 0.18310733671139995.\nNN Targets:  60%|██████    | 3/5 [43:47<28:28, 854.26s/it]   [I 2025-06-22 09:30:35,202] A new study created in memory with name: no-name-b6672ed4-f220-4c25-8171-d8dc8e6b5ff9\n","output_type":"stream"},{"name":"stdout","text":"  Neural Network CV score for Tc: 0.20548976\n  Training Neural Network for Density...\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-22 09:31:08,115] Trial 0 finished with value: 0.47919665573810555 and parameters: {'hidden_dim': 320, 'dropout': 0.3409274955799171, 'learning_rate': 1.4594504430773801e-05, 'batch_size': 16, 'weight_decay': 0.0005680228783561642}. Best is trial 0 with value: 0.47919665573810555.\n[I 2025-06-22 09:31:15,821] Trial 1 finished with value: 0.22221082683234342 and parameters: {'hidden_dim': 192, 'dropout': 0.16290246627573507, 'learning_rate': 0.00024283495185195182, 'batch_size': 64, 'weight_decay': 1.0605803996144505e-05}. Best is trial 1 with value: 0.22221082683234342.\n[I 2025-06-22 09:31:20,375] Trial 2 finished with value: 0.19925122587802357 and parameters: {'hidden_dim': 128, 'dropout': 0.2656683676451411, 'learning_rate': 0.0058768279563026745, 'batch_size': 64, 'weight_decay': 3.0169359551993246e-06}. Best is trial 2 with value: 0.19925122587802357.\n[I 2025-06-22 09:31:29,460] Trial 3 finished with value: 0.30791617301080343 and parameters: {'hidden_dim': 128, 'dropout': 0.2747817166337029, 'learning_rate': 9.617971640542002e-05, 'batch_size': 64, 'weight_decay': 8.108112796015347e-06}. Best is trial 2 with value: 0.19925122587802357.\n[I 2025-06-22 09:31:38,886] Trial 4 finished with value: 0.3453992002103796 and parameters: {'hidden_dim': 384, 'dropout': 0.29666764733762396, 'learning_rate': 4.233733478216454e-05, 'batch_size': 64, 'weight_decay': 0.00034241489963409506}. Best is trial 2 with value: 0.19925122587802357.\n[I 2025-06-22 09:31:41,956] Trial 5 finished with value: 0.20796364568944253 and parameters: {'hidden_dim': 128, 'dropout': 0.138872960660067, 'learning_rate': 0.007363692181946434, 'batch_size': 64, 'weight_decay': 0.00039564319255058994}. Best is trial 2 with value: 0.19925122587802357.\n[I 2025-06-22 09:31:59,337] Trial 6 finished with value: 0.38273821087843857 and parameters: {'hidden_dim': 320, 'dropout': 0.1210589710858295, 'learning_rate': 1.624414226792132e-05, 'batch_size': 32, 'weight_decay': 4.222226635204877e-05}. Best is trial 2 with value: 0.19925122587802357.\n[I 2025-06-22 09:32:06,340] Trial 7 finished with value: 0.23763901336021404 and parameters: {'hidden_dim': 256, 'dropout': 0.2232078664972164, 'learning_rate': 0.00018831163504401928, 'batch_size': 64, 'weight_decay': 0.00022913662275691628}. Best is trial 2 with value: 0.19925122587802357.\n[I 2025-06-22 09:32:13,039] Trial 8 finished with value: 0.2064394964235794 and parameters: {'hidden_dim': 384, 'dropout': 0.17110518735525793, 'learning_rate': 0.0012240558888371273, 'batch_size': 32, 'weight_decay': 2.5638100478830827e-06}. Best is trial 2 with value: 0.19925122587802357.\n[I 2025-06-22 09:32:45,695] Trial 9 finished with value: 0.3472515155787285 and parameters: {'hidden_dim': 512, 'dropout': 0.3931809599971774, 'learning_rate': 1.7464117139401183e-05, 'batch_size': 16, 'weight_decay': 1.8094347671902627e-05}. Best is trial 2 with value: 0.19925122587802357.\nNN Targets:  80%|████████  | 4/5 [46:10<09:33, 573.37s/it][I 2025-06-22 09:32:57,973] A new study created in memory with name: no-name-1d8fdd7c-fdfa-48b4-9765-763364a6fcb3\n","output_type":"stream"},{"name":"stdout","text":"  Neural Network CV score for Density: 0.78971468\n  Training Neural Network for Rg...\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-22 09:33:15,624] Trial 0 finished with value: 0.24079397234845792 and parameters: {'hidden_dim': 384, 'dropout': 0.3029348047613252, 'learning_rate': 0.00011494313986364161, 'batch_size': 16, 'weight_decay': 4.170371938779283e-06}. Best is trial 0 with value: 0.24079397234845792.\n[I 2025-06-22 09:33:18,243] Trial 1 finished with value: 0.243598280565049 and parameters: {'hidden_dim': 192, 'dropout': 0.11415244263832774, 'learning_rate': 0.008286930993242606, 'batch_size': 64, 'weight_decay': 2.0949953615650084e-05}. Best is trial 0 with value: 0.24079397234845792.\n[I 2025-06-22 09:33:22,601] Trial 2 finished with value: 0.22365401361444204 and parameters: {'hidden_dim': 256, 'dropout': 0.24100841843827145, 'learning_rate': 0.0004809731363306527, 'batch_size': 64, 'weight_decay': 6.164836399445577e-06}. Best is trial 2 with value: 0.22365401361444204.\n[I 2025-06-22 09:33:30,077] Trial 3 finished with value: 0.233387535360861 and parameters: {'hidden_dim': 448, 'dropout': 0.17948861824542128, 'learning_rate': 0.00019987698913254004, 'batch_size': 32, 'weight_decay': 1.739059198578217e-05}. Best is trial 2 with value: 0.22365401361444204.\n[I 2025-06-22 09:33:34,435] Trial 4 finished with value: 0.22768444820253647 and parameters: {'hidden_dim': 192, 'dropout': 0.10117989052530593, 'learning_rate': 0.00037681217696138166, 'batch_size': 64, 'weight_decay': 3.131170934142226e-06}. Best is trial 2 with value: 0.22365401361444204.\n[I 2025-06-22 09:34:07,183] Trial 5 finished with value: 0.3369193900758169 and parameters: {'hidden_dim': 320, 'dropout': 0.3522006165353868, 'learning_rate': 2.5312557858584103e-05, 'batch_size': 16, 'weight_decay': 2.6350252996327067e-06}. Best is trial 2 with value: 0.22365401361444204.\n[I 2025-06-22 09:34:13,487] Trial 6 finished with value: 0.48311492254083604 and parameters: {'hidden_dim': 320, 'dropout': 0.3394466475465949, 'learning_rate': 1.0434820020970715e-05, 'batch_size': 64, 'weight_decay': 5.136704672931344e-06}. Best is trial 2 with value: 0.22365401361444204.\n[I 2025-06-22 09:34:16,823] Trial 7 finished with value: 0.2314426797300805 and parameters: {'hidden_dim': 448, 'dropout': 0.16404827397127145, 'learning_rate': 0.0006789865577353241, 'batch_size': 64, 'weight_decay': 7.952701915238111e-05}. Best is trial 2 with value: 0.22365401361444204.\n[I 2025-06-22 09:34:21,544] Trial 8 finished with value: 0.2391510827866213 and parameters: {'hidden_dim': 256, 'dropout': 0.14927606390185888, 'learning_rate': 0.0002725965381063088, 'batch_size': 64, 'weight_decay': 4.2621399707230344e-05}. Best is trial 2 with value: 0.22365401361444204.\n[I 2025-06-22 09:34:35,324] Trial 9 finished with value: 0.2564031219520287 and parameters: {'hidden_dim': 256, 'dropout': 0.36490624560484286, 'learning_rate': 0.0063913464154319866, 'batch_size': 16, 'weight_decay': 0.0007445490419786596}. Best is trial 2 with value: 0.22365401361444204.\nNN Targets: 100%|██████████| 5/5 [48:01<00:00, 576.34s/it]","output_type":"stream"},{"name":"stdout","text":"  Neural Network CV score for Rg: 13.17756098\n--- SCRIPT 2 COMPLETE: Neural Network predictions and OOF file saved. ---\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n--- RUNNING SCRIPT 3: OPTIMAL BLENDING ---\nFinding optimal blend weights...\n  Best weight for Tg: 0.00 (Local RMSE: 64.75848)\n  Best weight for FFV: 0.00 (Local RMSE: 0.01279)\n  Best weight for Tc: 0.00 (Local RMSE: 0.04069)\n  Best weight for Density: 0.00 (Local RMSE: 0.07008)\n  Best weight for Rg: 0.00 (Local RMSE: 2.46020)\n\nBlending test predictions...\n\nFinal submission saved as submission.csv\nPreview:\n           id          Tg       FFV        Tc   Density         Rg\n0  1109053969  156.259104  0.375796  0.186964  1.178455  21.283007\n1  1422188626  171.046137  0.378355  0.242433  1.097039  21.010584\n2  2032016830   95.031012  0.353134  0.262752  1.140527  21.157024\nCPU times: user 58min 48s, sys: 29.8 s, total: 59min 18s\nWall time: 54min 5s\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Approach 5: Feature Selection with Recursive Feature Elimination\nTo reduce noise and overfitting, we apply Recursive Feature Elimination (RFE) with XGBoost to select the top 50 features for each target. This approach trains an XGBoost model on the selected features, improving efficiency and potentially accuracy by focusing on the most relevant descriptors.","metadata":{}},{"cell_type":"markdown","source":"%%time\n\nfrom sklearn.feature_selection import RFE\n\nOOF_Preds = []\nTest_Preds = []\n\nfor target in tqdm(CFG.targets):\n    print(f\"\\n=== Training with Feature Selection for {target} ===\\n\")\n    \n    # Prepare data\n    Xtrain_ = Xtrain.copy()\n    Xtrain_[target] = Ytrain[target]\n    Xtrain_ = Xtrain_.dropna(subset=[target])\n    idx = Xtrain_.index\n    Xtrain_.index = range(len(Xtrain_))\n    ytrain_ = Xtrain_[target]\n    \n    # Feature selection\n    X_tr = Xtrain_.drop(drop_cols, axis=1)\n    model = XGBRegressor(objective='reg:absoluteerror', random_state=CFG.random_state, n_estimators=100)\n    rfe = RFE(estimator=model, n_features_to_select=50)\n    rfe.fit(X_tr, ytrain_)\n    selected_cols = X_tr.columns[rfe.support_].tolist()\n    \n    # Cross-validation setup\n    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n    oof_preds = np.zeros(len(Xtrain_))\n    test_preds = np.zeros(len(Xtest))\n    \n    # Train model on selected features\n    params = {\n        'objective': 'reg:absoluteerror',\n        'n_estimators': 1000 if target == 'FFV' else 300,\n        'learning_rate': 0.02,\n        'max_depth': 6 if target == 'FFV' else 4,\n        'colsample_bytree': 0.3,\n        'random_state': CFG.random_state,\n        'verbosity': 0\n    }\n    model = XGBRegressor(**params)\n    \n    for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_, ytrain_)):\n        X_tr_fold = Xtrain_.iloc[train_idx][selected_cols]\n        X_dev_fold = Xtrain_.iloc[dev_idx][selected_cols]\n        y_tr, y_dev = ytrain_.iloc[train_idx], ytrain_.iloc[dev_idx]\n        \n        model.fit(X_tr_fold, y_tr)\n        oof_preds[dev_idx] = model.predict(X_dev_fold)\n        test_preds += model.predict(Xtest[selected_cols]) / CFG.n_splits\n    \n    # Store predictions\n    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n    Test_Preds.append(pd.DataFrame({target: test_preds}))\n    \n    # Compute score\n    score = mean_absolute_error(ytrain_, oof_preds)\n    print(f\"RFE CV score for {target}: {score:.8f}\")\n    \n    clean_memory()\n\n# Combine predictions\nrfe_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\nrfe_test_preds = pd.concat(Test_Preds, axis=1)","metadata":{}},{"cell_type":"markdown","source":"## Final Ensemble and Submission\nThe final predictions are obtained by averaging the OOF and test predictions from all approaches (Gradient Boosting, Neural Network, GNN, Stacking, and RFE). This ensemble leverages the strengths of each method to maximize accuracy. The wMAE score is computed on the OOF predictions, and the test predictions are formatted for submission.","metadata":{}},{"cell_type":"markdown","source":"%%time\n\n# Combine all predictions\nall_oof_preds = [\n    gb_oof_preds.fillna(0),\n    nn_oof_preds.fillna(0),\n    gnn_oof_preds.fillna(0),\n    stack_oof_preds.fillna(0),\n    rfe_oof_preds.fillna(0)\n]\nall_test_preds = [\n    gb_test_preds.fillna(0),\n    nn_test_preds.fillna(0),\n    gnn_test_preds.fillna(0),\n    stack_test_preds.fillna(0),\n    rfe_test_preds.fillna(0)\n]\n\nfinal_oof_preds = pd.DataFrame(0, index=gb_oof_preds.index, columns=CFG.targets)\nfinal_test_preds = pd.DataFrame(0, index=gb_test_preds.index, columns=CFG.targets)\n\nfor oof, test in zip(all_oof_preds, all_test_preds):\n    for target in CFG.targets:\n        final_oof_preds[target] += oof[target] / len(all_oof_preds)\n        final_test_preds[target] += test[target] / len(all_test_preds)\n\n# Compute final CV score\nscore = score_metric(Ytrain, final_oof_preds)\nprint(f\"\\nFinal Ensemble CV score: {score:.8f}\\n\")\n\n# Prepare submission\nsub_fl = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\nfinal_test_preds[\"id\"] = sub_fl[\"id\"]\nfinal_test_preds[[\"id\"] + CFG.targets].to_csv(\"submission.csv\", index=False)\n\n# Display first few rows\n!head submission.csv","metadata":{}}]}