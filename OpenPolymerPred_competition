{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/keyushnisar/openpolymerpred?scriptVersionId=246536551\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"2aee3f48","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.00469,"end_time":"2025-06-20T15:03:11.088963","exception":false,"start_time":"2025-06-20T15:03:11.084273","status":"completed"},"tags":[]},"source":["# Open Polymer Prediction 2025: Enhanced Baseline Notebook\n","\n","<a href=\"https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025\" target=\"_blank\">\n","  <img src=\"https://img.shields.io/badge/Kaggle-Competition-blue?style=for-the-badge&logo=kaggle\" alt=\"Kaggle Competition\">\n","</a>\n","\n","This notebook addresses the *NeurIPS - Open Polymer Prediction 2025* competition, which involves predicting five key polymer properties—glass transition temperature (Tg), fractional free volume (FFV), thermal conductivity (Tc), density, and radius of gyration (Rg)—from SMILES representations of polymer structures. The evaluation metric is a weighted Mean Absolute Error (wMAE), designed to balance the contribution of each property by accounting for their scale and data availability.\n","\n","This enhanced baseline improves the original notebook by incorporating advanced feature engineering, a broader ensemble of machine learning models, and hyperparameter optimization. The notebook is structured for clarity, reproducibility, and scalability to handle the competition's hidden test set of approximately 1,500 polymers.\n","In this script, we'll:\n","1. Find similar images to pair them up efficiently\n","2. Detect key points (like corners or distinctive features) in each image\n","3. Match these points between image pairs\n","4. Verify matches to ensure accuracy\n","5. Build a 3D model using COLMAP\n","6. Create a submission file with camera poses\n","\n","\n","\n","**Approach Overview**\n","The approach consists of the following components:\n","\n","* Data Preprocessing: Load and clean the train and test datasets, handling missing values and outliers to ensure robust data quality.\n","* Feature Engineering: Use RDKit to compute molecular descriptors and introduce custom polymer-specific features to capture structural properties.\n","* Model Selection: Train an ensemble of gradient boosting models (XGBoost, LightGBM, CatBoost) and a simple neural network for each target property.\n","* Hyperparameter Tuning: Use Optuna to optimize model hyperparameters, improving predictive performance.\n","* Cross-Validation: Implement K-Fold cross-validation to ensure reliable model evaluation and generalization.\n","* Ensemble Predictions: Combine predictions from multiple models to enhance accuracy and robustness.\n","* Submission Preparation: Generate predictions for the test set and format them according to the competition requirements."]},{"cell_type":"markdown","id":"9332670e","metadata":{"papermill":{"duration":0.003371,"end_time":"2025-06-20T15:03:11.096065","exception":false,"start_time":"2025-06-20T15:03:11.092694","status":"completed"},"tags":[]},"source":["## Configuration\n","The configuration class defines key parameters for data preprocessing, model training, and cross-validation. Key settings include:\n","\n","* Target Variables: The five properties to predict: Tg, FFV, Tc, Density, and Rg.\n","* Cross-Validation: 5-fold K-Fold cross-validation for robust model evaluation.\n","* Model Parameters: Settings for GPU usage, early stopping, and random state for reproducibility.\n","* Feature Engineering: Enable feature importance analysis and preprocessing steps.\n","* Evaluation Metric: Minimize the weighted Mean Absolute Error (wMAE)."]},{"cell_type":"markdown","id":"26e41c95","metadata":{"execution":{"iopub.execute_input":"2025-06-20T07:49:59.617573Z","iopub.status.busy":"2025-06-20T07:49:59.617411Z","iopub.status.idle":"2025-06-20T07:50:02.808543Z","shell.execute_reply":"2025-06-20T07:50:02.807851Z","shell.execute_reply.started":"2025-06-20T07:49:59.617558Z"},"papermill":{"duration":0.003286,"end_time":"2025-06-20T15:03:11.102862","exception":false,"start_time":"2025-06-20T15:03:11.099576","status":"completed"},"tags":[]},"source":["%%time\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import mean_absolute_error\n","from xgboost import XGBRegressor\n","from rdkit import Chem\n","from rdkit.Chem import Descriptors\n","from tqdm import tqdm\n","import gc\n","\n","class CFG:\n","    \"\"\"\n","    Simple configuration class for paths and settings.\n","    \"\"\"\n","    data_path = \"/kaggle/input/neurips-open-polymer-prediction-2025\"\n","    output_path = \"/kaggle/working\"\n","    targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n","    n_splits = 5\n","    random_state = 42\n","\n","# Utility function for memory cleanup\n","def clean_memory():\n","    gc.collect()\n","\n","# Utility function for wMAE metric\n","def score_metric(y_true, y_pred):\n","    \"\"\"\n","    Compute weighted Mean Absolute Error (wMAE) across targets.\n","    \"\"\"\n","    weights = {'Tg': 0.2, 'FFV': 0.2, 'Tc': 0.2, 'Density': 0.2, 'Rg': 0.2}  # Simplified equal weights\n","    mae = 0\n","    for col in y_true.columns:\n","        valid_idx = y_true[col].notna()\n","        if valid_idx.sum() > 0:\n","            mae += weights[col] * mean_absolute_error(y_true[col][valid_idx], y_pred[col][valid_idx])\n","    return mae\n","\n","print(\"Setup complete\")"]},{"cell_type":"code","execution_count":1,"id":"c5ab4a98","metadata":{"execution":{"iopub.execute_input":"2025-06-20T15:03:11.110641Z","iopub.status.busy":"2025-06-20T15:03:11.110413Z","iopub.status.idle":"2025-06-20T15:04:33.291846Z","shell.execute_reply":"2025-06-20T15:04:33.291142Z"},"papermill":{"duration":82.187192,"end_time":"2025-06-20T15:04:33.293479","exception":false,"start_time":"2025-06-20T15:03:11.106287","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\r\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n","Collecting deepchem\r\n","  Downloading deepchem-2.8.0-py3-none-any.whl.metadata (2.0 kB)\r\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\r\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\r\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\r\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\r\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\r\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\r\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\r\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\r\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\r\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\r\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\r\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\r\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\r\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\r\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\r\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from deepchem) (1.5.0)\r\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from deepchem) (2.2.3)\r\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from deepchem) (1.2.2)\r\n","Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from deepchem) (1.15.2)\r\n","Collecting rdkit (from deepchem)\r\n","  Downloading rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\r\n","Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\r\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.3.8)\r\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.2.4)\r\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (0.1.1)\r\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2025.1.0)\r\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2022.1.0)\r\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2.4.1)\r\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\r\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->deepchem) (2.9.0.post0)\r\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->deepchem) (2025.2)\r\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->deepchem) (2025.2)\r\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit->deepchem) (11.1.0)\r\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->deepchem) (3.6.0)\r\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.17.0)\r\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2024.2.0)\r\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2022.1.0)\r\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.3.0)\r\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna) (2024.2.0)\r\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading deepchem-2.8.0-py3-none-any.whl (1.0 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl (34.9 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, rdkit, deepchem\r\n","  Attempting uninstall: nvidia-nvjitlink-cu12\r\n","    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n","    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n","  Attempting uninstall: nvidia-curand-cu12\r\n","    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n","    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n","      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n","  Attempting uninstall: nvidia-cufft-cu12\r\n","    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n","    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n","      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n","  Attempting uninstall: nvidia-cublas-cu12\r\n","    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n","    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n","      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n","  Attempting uninstall: nvidia-cusparse-cu12\r\n","    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n","    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n","  Attempting uninstall: nvidia-cudnn-cu12\r\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n","  Attempting uninstall: nvidia-cusolver-cu12\r\n","    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n","    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n","Successfully installed deepchem-2.8.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rdkit-2025.3.3\r\n","Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2025.3.3)\r\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\r\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\r\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (1.3.8)\r\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (1.2.4)\r\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (0.1.1)\r\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2025.1.0)\r\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2022.1.0)\r\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2.4.1)\r\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit) (2024.2.0)\r\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit) (2022.1.0)\r\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit) (1.3.0)\r\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit) (2024.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit) (2024.2.0)\r\n"]}],"source":["!pip install optuna torch deepchem\n","!pip install rdkit"]},{"cell_type":"markdown","id":"1c3e14a7","metadata":{"papermill":{"duration":0.022041,"end_time":"2025-06-20T15:04:33.338918","exception":false,"start_time":"2025-06-20T15:04:33.316877","status":"completed"},"tags":[]},"source":["## Feature Engineering\n","Feature engineering is critical for capturing the chemical and structural properties of polymers. The FeatureMaker class uses RDKit to compute molecular descriptors from SMILES strings, such as molecular weight, topological polar surface area, and other chemical properties. Additional custom features are introduced to capture polymer-specific characteristics, including:\n","\n","* Chain Length: Number of repeating units inferred from SMILES.\n","* Functional Group Counts: Counts of specific chemical groups (e.g., aromatic rings, hydroxyl groups).\n","* Molecular Complexity: Metrics like the BertzCT index for structural complexity.\n","  \n","The Ipc descriptor is log-transformed to handle its large range, and infinity values are replaced with NaN. Columns with near-zero variance or excessive missing values (>99.75%) are dropped to reduce noise."]},{"cell_type":"markdown","id":"e839f5f5","metadata":{"execution":{"iopub.execute_input":"2025-06-20T07:50:21.631486Z","iopub.status.busy":"2025-06-20T07:50:21.63078Z","iopub.status.idle":"2025-06-20T07:50:21.637203Z","shell.execute_reply":"2025-06-20T07:50:21.63647Z","shell.execute_reply.started":"2025-06-20T07:50:21.631461Z"},"papermill":{"duration":0.022047,"end_time":"2025-06-20T15:04:33.38315","exception":false,"start_time":"2025-06-20T15:04:33.361103","status":"completed"},"tags":[]},"source":["%%writefile myfe.py\n","\n","import pandas as pd\n","import numpy as np\n","from rdkit import Chem\n","from rdkit.Chem import Descriptors, AllChem\n","\n","class FeatureMaker:\n","    \"\"\"\n","    Generate RDKit descriptors and Morgan fingerprints from SMILES strings.\n","    \"\"\"\n","    def __init__(self):\n","        self.feature_names = None\n","\n","    def _compute_descriptors(self, smiles):\n","        \"\"\"\n","        Compute RDKit descriptors and Morgan fingerprints for a SMILES string.\n","        \"\"\"\n","        mol = Chem.MolFromSmiles(smiles)\n","        if mol is None:\n","            return [None] * (len(Descriptors.descList) + 256)  # 256 for Morgan\n","        desc_values = [desc[1](mol) for desc in Descriptors.descList]\n","        # Compute Morgan fingerprint (ECFP4, radius=2, 256 bits)\n","        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=256)\n","        fp_values = list(fp)\n","        return desc_values + fp_values\n","\n","    def fit_transform(self, X):\n","        \"\"\"\n","        Fit and transform the dataset to create features.\n","        \"\"\"\n","        desc_names = [desc[0] for desc in Descriptors.descList]\n","        fp_names = [f'ECFP_{i}' for i in range(256)]\n","        descriptors = [self._compute_descriptors(smi) for smi in X['SMILES']]\n","        df = pd.DataFrame(descriptors, columns=desc_names + fp_names)\n","        df = pd.concat([X, df], axis=1)\n","        df[\"Ipc\"] = np.log1p(df[\"Ipc\"])  # Log-transform Ipc\n","        self.feature_names = list(df.columns)\n","        return df.replace([np.inf, -np.inf], np.nan)\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Transform the dataset using the same features.\n","        \"\"\"\n","        desc_names = [desc[0] for desc in Descriptors.descList]\n","        fp_names = [f'ECFP_{i}' for i in range(256)]\n","        descriptors = [self._compute_descriptors(smi) for smi in X['SMILES']]\n","        df = pd.DataFrame(descriptors, columns=desc_names + fp_names)\n","        df = pd.concat([X, df], axis=1)\n","        df[\"Ipc\"] = np.log1p(df[\"Ipc\"])\n","        return df.replace([np.inf, -np.inf], np.nan)\n","\n","    def get_feature_names(self):\n","        \"\"\"\n","        Return feature names.\n","        \"\"\"\n","        return self.feature_names"]},{"cell_type":"markdown","id":"fa673246","metadata":{"papermill":{"duration":0.021949,"end_time":"2025-06-20T15:04:33.427356","exception":false,"start_time":"2025-06-20T15:04:33.405407","status":"completed"},"tags":[]},"source":["## Model Training Strategy\n","\n","The model training strategy employs an ensemble of gradient boosting models (XGBoost, LightGBM, CatBoost) and a neural network implemented with PyTorch for each target property. Key aspects include:\n","\n","* FFV (Fractional Free Volume): As the most populated target (~90% fill rate), it uses a larger ensemble with tuned hyperparameters.\n","* Other Targets (Tg, Tc, Density, Rg): These have higher missing rates, so simpler models with fewer iterations are used to avoid overfitting.\n","* Hyperparameter Tuning: Optuna is used to optimize key parameters (e.g., learning rate, max depth) for each model.\n","* Cross-Validation: 5-fold K-Fold cross-validation ensures robust evaluation.\n","Ensemble: Predictions are averaged across models to improve robustness.\n","\n","The weighted MAE metric is computed for each target, and the final score aggregates these errors with weights based on data availability and property ranges."]},{"cell_type":"markdown","id":"81aa3d86","metadata":{"papermill":{"duration":0.022011,"end_time":"2025-06-20T15:04:33.471859","exception":false,"start_time":"2025-06-20T15:04:33.449848","status":"completed"},"tags":[]},"source":["## Approach 1: Gradient Boosting Ensemble\n","\n","This approach trains an ensemble of three gradient boosting models—XGBoost, LightGBM, and CatBoost—for each target property. These models are robust to missing data and handle non-linear relationships well. We use Optuna for hyperparameter tuning to optimize parameters like learning rate, max depth, and subsampling rates. FFV, with the highest data availability, uses larger models, while other targets use smaller models to prevent overfitting."]},{"cell_type":"markdown","id":"18b1df94","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2025-06-20T07:36:21.347376Z","iopub.status.busy":"2025-06-20T07:36:21.346501Z","iopub.status.idle":"2025-06-20T07:39:17.031167Z","shell.execute_reply":"2025-06-20T07:39:17.030204Z","shell.execute_reply.started":"2025-06-20T07:36:21.347344Z"},"jupyter":{"outputs_hidden":true},"papermill":{"duration":0.022161,"end_time":"2025-06-20T15:04:33.51613","exception":false,"start_time":"2025-06-20T15:04:33.493969","status":"completed"},"tags":[]},"source":["%%time\n","\n","# Install required packages\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import mean_absolute_error\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from catboost import CatBoostRegressor\n","from tqdm import tqdm\n","import optuna\n","import gc\n","from myfe import FeatureMaker\n","\n","# Define configuration\n","class CFG:\n","    data_path = \"/kaggle/input/neurips-open-polymer-prediction-2025/\"  # Kaggle input path\n","    random_state = 42\n","    n_splits = 3\n","    targets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n","\n","# Memory cleanup function\n","def clean_memory():\n","    gc.collect()\n","\n","# Data loading and preprocessing\n","print(\"Loading data...\")\n","train_df = pd.read_csv(f\"{CFG.data_path}/train.csv\")\n","test_df = pd.read_csv(f\"{CFG.data_path}/test.csv\")\n","\n","# Initialize feature maker\n","featurizer = FeatureMaker()\n","\n","# Generate features\n","print(\"Generating features...\")\n","Xtrain = featurizer.fit_transform(train_df)\n","Xtest = featurizer.transform(test_df)\n","\n","# Separate targets\n","Ytrain = Xtrain[CFG.targets].copy()\n","Xtrain = Xtrain.drop(CFG.targets, axis=1, errors='ignore')\n","\n","# Ensure test set has 'id' column\n","if 'id' not in Xtest.columns:\n","    Xtest['id'] = test_df['id']\n","\n","# Optuna objective function\n","def objective(trial, X, y, model_type, cv):\n","    \"\"\"\n","    Objective function for Optuna hyperparameter tuning.\n","    \"\"\"\n","    if model_type == 'XGB':\n","        params = {\n","            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n","            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n","            'max_depth': trial.suggest_int('max_depth', 3, 8),\n","            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.8),\n","            'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0),\n","            'objective': 'reg:absoluteerror',\n","            'random_state': CFG.random_state,\n","            'early_stopping_rounds': 50,\n","            'tree_method': 'hist',\n","            'verbosity': 0\n","        }\n","        model = XGBRegressor(**params)\n","    \n","    elif model_type == 'LGBM':\n","        params = {\n","            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n","            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n","            'max_depth': trial.suggest_int('max_depth', 3, 8),\n","            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.8),\n","            'num_leaves': trial.suggest_int('num_leaves', 20, 50),\n","            'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n","            'objective': 'regression_l1',\n","            'random_state': CFG.random_state,\n","            'early_stopping_rounds': 50,\n","            'verbosity': -1\n","        }\n","        model = LGBMRegressor(**params)\n","    \n","    elif model_type == 'CB':\n","        params = {\n","            'iterations': trial.suggest_int('iterations', 100, 1000),\n","            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n","            'depth': trial.suggest_int('depth', 3, 8),\n","            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10.0),\n","            'loss_function': 'MAE',\n","            'eval_metric': 'MAE',\n","            'random_state': CFG.random_state,\n","            'early_stopping_rounds': 50,\n","            'thread_count': -1,\n","            'verbose': 0\n","        }\n","        model = CatBoostRegressor(**params)\n","    \n","    scores = []\n","    for train_idx, dev_idx in cv.split(X, y):\n","        X_tr, X_dev = X.iloc[train_idx], X.iloc[dev_idx]\n","        y_tr, y_dev = y.iloc[train_idx], y.iloc[dev_idx]\n","        if model_type in ['XGB', 'LGBM']:\n","            model.fit(X_tr, y_tr, eval_set=[(X_dev, y_dev)], verbose_eval=False)\n","        else:\n","            model.fit(X_tr, y_tr, eval_set=(X_dev, y_dev), verbose=False)\n","        preds = model.predict(X_dev)\n","        scores.append(mean_absolute_error(y_dev, preds))\n","    \n","    return np.mean(scores)\n","\n","def tune_model(X, y, model_type, cv, n_trials=5):\n","    \"\"\"\n","    Tune hyperparameters using Optuna.\n","    \"\"\"\n","    study = optuna.create_study(direction='minimize')\n","    study.optimize(lambda trial: objective(trial, X, y, model_type, cv), n_trials=n_trials)\n","    return study.best_params\n","\n","# Gradient Boosting Ensemble\n","OOF_Preds = []\n","Test_Preds = []\n","drop_cols = [\"Source\", \"id\", \"Id\", \"SMILES\"] + CFG.targets\n","\n","for target in tqdm(CFG.targets):\n","    print(f\"\\n=== Training Gradient Boosting Ensemble for {target} ===\\n\")\n","    \n","    # Prepare data\n","    Xtrain_ = Xtrain.copy()\n","    Xtrain_[target] = Ytrain[target]\n","    Xtrain_ = Xtrain_.dropna(subset=[target])\n","    idx = Xtrain_.index\n","    Xtrain_.index = range(len(Xtrain_))\n","    ytrain_ = Xtrain_[target]\n","    \n","    # Filter drop_cols\n","    existing_drop_cols = [col for col in drop_cols if col in Xtrain_.columns and col != target]\n","    X_features = Xtrain_.drop(existing_drop_cols + [target], axis=1, errors='ignore')\n","    \n","    # Cross-validation setup\n","    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n","    \n","    # Tune hyperparameters\n","    tuned_params = {}\n","    for model_type in ['XGB', 'LGBM', 'CB']:\n","        tuned_params[model_type] = tune_model(X_features, ytrain_, model_type, cv, n_trials=5)\n","    \n","    # Model definitions\n","    models = {\n","        'XGB': XGBRegressor(**tuned_params['XGB'], objective='reg:absoluteerror', random_state=CFG.random_state, early_stopping_rounds=50, tree_method='hist', verbosity=0),\n","        'LGBM': LGBMRegressor(**tuned_params['LGBM'], objective='regression_l1', random_state=CFG.random_state, early_stopping_rounds=50, verbosity=-1),\n","        'CB': CatBoostRegressor(**tuned_params['CB'], loss_function='MAE', eval_metric='MAE', random_state=CFG.random_state, early_stopping_rounds=50, thread_count=-1, verbose=0)\n","    }\n","    \n","    oof_preds = np.zeros(len(Xtrain_))\n","    test_preds = np.zeros(len(Xtest))\n","    \n","    # Train across folds\n","    for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_, ytrain_)):\n","        X_tr = Xtrain_.iloc[train_idx].drop(existing_drop_cols + [target], axis=1, errors='ignore')\n","        X_dev = Xtrain_.iloc[dev_idx].drop(existing_drop_cols + [target], axis=1, errors='ignore')\n","        y_tr, y_dev = ytrain_.iloc[train_idx], ytrain_.iloc[dev_idx]\n","        \n","        fold_preds = np.zeros(len(X_dev))\n","        fold_test_preds = np.zeros(len(Xtest))\n","        \n","        for name, model in models.items():\n","            if name in ['XGB', 'LGBM']:\n","                model.fit(X_tr, y_tr, eval_set=[(X_dev, y_dev)], verbose_eval=False)\n","            else:\n","                model.fit(X_tr, y_tr, eval_set=(X_dev, y_dev), verbose=False)\n","            fold_preds += model.predict(X_dev) / len(models)\n","            fold_test_preds += model.predict(Xtest.drop([col for col in existing_drop_cols if col in Xtest.columns], axis=1, errors='ignore')) / len(models)\n","        \n","        oof_preds[dev_idx] = fold_preds\n","        test_preds += fold_test_preds / CFG.n_splits\n","    \n","    # Store predictions\n","    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n","    Test_Preds.append(pd.DataFrame({target: test_preds}))\n","    \n","    # Compute score\n","    score = mean_absolute_error(ytrain_, oof_preds)\n","    print(f\"Gradient Boosting CV score for {target}: {score:.8f}\")\n","    \n","    clean_memory()\n","\n","# Combine predictions\n","gb_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\n","gb_test_preds = pd.concat(Test_Preds, axis=1)\n","\n","# Submission\n","sub_fl = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\n","gb_test_preds[\"id\"] = sub_fl[\"id\"]\n","gb_test_preds[[\"id\"] + CFG.targets].to_csv(\"submission.csv\", index=False)\n","print(\"\\nGradient Boosting submission saved as submission.csv\")"]},{"cell_type":"markdown","id":"d581ebc9","metadata":{"execution":{"iopub.execute_input":"2025-06-20T07:39:58.082637Z","iopub.status.busy":"2025-06-20T07:39:58.08172Z","iopub.status.idle":"2025-06-20T07:39:58.155101Z","shell.execute_reply":"2025-06-20T07:39:58.154203Z","shell.execute_reply.started":"2025-06-20T07:39:58.082598Z"},"papermill":{"duration":0.0642,"end_time":"2025-06-20T15:04:33.60253","exception":false,"start_time":"2025-06-20T15:04:33.53833","status":"completed"},"tags":[]},"source":["%%time\n","\n","OOF_Preds = []\n","Test_Preds = []\n","Artefacts = {}\n","drop_cols = [\"Source\", \"id\", \"Id\", \"SMILES\"] + CFG.targets\n","\n","for target in tqdm(CFG.targets):\n","    print(f\"\\n=== Training Gradient Boosting Ensemble for {target} ===\\n\")\n","    \n","    # Prepare data\n","    Xtrain_ = Xtrain.copy()\n","    Xtrain_[target] = Ytrain[target]\n","    Xtrain_ = Xtrain_.dropna(subset=[target])\n","    idx = Xtrain_.index\n","    Xtrain_.index = range(len(Xtrain_))\n","    ytrain_ = Xtrain_[target]\n","    \n","    # Filter drop_cols to include only existing columns, excluding the current target\n","    existing_drop_cols = [col for col in drop_cols if col in Xtrain_.columns and col != target]\n","    \n","    # Features for hyperparameter tuning (exclude target)\n","    X_features = Xtrain_.drop(existing_drop_cols + [target], axis=1, errors='ignore')\n","    \n","    # Cross-validation setup\n","    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n","    \n","    # Tune hyperparameters\n","    tuned_params = {}\n","    for model_type in ['XGB', 'LGBM', 'CB']:\n","        tuned_params[model_type] = tune_model(X_features, ytrain_, model_type, cv, n_trials=10)\n","    \n","    # Model definitions\n","    models = {\n","        'XGB': XGBRegressor(**tuned_params['XGB'], objective='reg:absoluteerror', random_state=CFG.random_state, verbosity=0),\n","        'LGBM': LGBMRegressor(**tuned_params['LGBM'], objective='regression_l1', random_state=CFG.random_state, verbosity=-1),\n","        'CB': CatBoostRegressor(**tuned_params['CB'], loss_function='MAE', eval_metric='MAE', random_state=CFG.random_state, verbose=0)\n","    }\n","    \n","    oof_preds = np.zeros(len(Xtrain_))\n","    test_preds = np.zeros(len(Xtest))\n","    model_oof = {name: np.zeros(len(Xtrain_)) for name in models}\n","    \n","    # Train across folds\n","    for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_, ytrain_)):\n","        X_tr = Xtrain_.iloc[train_idx].drop(existing_drop_cols + [target], axis=1, errors='ignore')\n","        X_dev = Xtrain_.iloc[dev_idx].drop(existing_drop_cols + [target], axis=1, errors='ignore')\n","        y_tr, y_dev = ytrain_.iloc[train_idx], ytrain_.iloc[dev_idx]\n","        \n","        fold_preds = np.zeros(len(X_dev))\n","        fold_test_preds = np.zeros(len(Xtest))\n","        \n","        for name, model in models.items():\n","            model.fit(X_tr, y_tr)\n","            model_oof[name][dev_idx] = model.predict(X_dev)\n","            fold_preds += model.predict(X_dev) / len(models)\n","            fold_test_preds += model.predict(Xtest.drop([col for col in existing_drop_cols if col in Xtest.columns], axis=1, errors='ignore')) / len(models)\n","            Artefacts[f\"{target}_{name}_fold{fold}\"] = model\n","        \n","        oof_preds[dev_idx] = fold_preds\n","        test_preds += fold_test_preds / CFG.n_splits\n","    \n","    # Store predictions\n","    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n","    Test_Preds.append(pd.DataFrame({target: test_preds}))\n","    \n","    # Compute score\n","    score = mean_absolute_error(ytrain_, oof_preds)\n","    print(f\"Ensemble CV score for {target}: {score:.8f}\")\n","    \n","    clean_memory()\n","\n","# Combine predictions\n","gb_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\n","gb_test_preds = pd.concat(Test_Preds, axis=1)"]},{"cell_type":"markdown","id":"8ca50420","metadata":{"papermill":{"duration":0.022182,"end_time":"2025-06-20T15:04:33.647786","exception":false,"start_time":"2025-06-20T15:04:33.625604","status":"completed"},"tags":[]},"source":["## Approach 2: Neural Network\n","\n","A multilayer perceptron (MLP) implemented in PyTorch is used to capture complex non-linear relationships in the molecular descriptors. The MLP is trained with a mean absolute error (MAE) loss function, matching the competition’s metric. Early stopping and learning rate scheduling prevent overfitting. This approach is applied to all targets, with larger architectures for FFV due to its higher data availability."]},{"cell_type":"markdown","id":"b45b1e9c","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2025-06-20T07:53:10.036897Z","iopub.status.busy":"2025-06-20T07:53:10.036265Z","iopub.status.idle":"2025-06-20T08:40:06.652168Z","shell.execute_reply":"2025-06-20T08:40:06.651312Z","shell.execute_reply.started":"2025-06-20T07:53:10.036866Z"},"jupyter":{"outputs_hidden":true},"papermill":{"duration":0.021945,"end_time":"2025-06-20T15:04:33.692182","exception":false,"start_time":"2025-06-20T15:04:33.670237","status":"completed"},"tags":[]},"source":["%%time\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.feature_selection import VarianceThreshold\n","from tqdm import tqdm\n","import optuna\n","import gc\n","from torch.cuda.amp import GradScaler, autocast\n","from myfe import FeatureMaker\n","\n","# Define configuration\n","class CFG:\n","    data_path = \"/kaggle/input/neurips-open-polymer-prediction-2025/\" \n","    random_state = 42\n","    n_splits = 5  \n","    targets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Memory cleanup function\n","def clean_memory():\n","    gc.collect()\n","    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n","\n","# Data loading and preprocessing\n","print(\"Loading data...\")\n","train_df = pd.read_csv(f\"{CFG.data_path}/train.csv\")\n","test_df = pd.read_csv(f\"{CFG.data_path}/test.csv\")\n","\n","# Initialize feature maker\n","featurizer = FeatureMaker()\n","\n","# Generate features\n","print(\"Generating features...\")\n","Xtrain = featurizer.fit_transform(train_df)\n","Xtest = featurizer.transform(test_df)\n","\n","# Separate targets\n","Ytrain = Xtrain[CFG.targets].copy()\n","Xtrain = Xtrain.drop(CFG.targets, axis=1, errors='ignore')\n","\n","# Ensure test set has 'id' column\n","if 'id' not in Xtest.columns:\n","    Xtest['id'] = test_df['id']\n","\n","class PolymerDataset(Dataset):\n","    \"\"\"\n","    Custom dataset for polymer data with NaN handling.\n","    \"\"\"\n","    def __init__(self, X, y=None):\n","        self.X = torch.tensor(X.values, dtype=torch.float32).nan_to_num(0)\n","        self.y = torch.tensor(y.values, dtype=torch.float32) if y is not None else None\n","    \n","    def __len__(self):\n","        return len(self.X)\n","    \n","    def __getitem__(self, idx):\n","        if self.y is not None:\n","            return self.X[idx], self.y[idx]\n","        return self.X[idx]\n","\n","class AttentionResidualMLP(nn.Module):\n","    \"\"\"\n","    Enhanced MLP with attention mechanism, residual connections, and layer normalization.\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dim, dropout):\n","        super(AttentionResidualMLP, self).__init__()\n","        self.input_layer = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.LayerNorm(hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout)\n","        )\n","        # Attention mechanism\n","        self.attention = nn.Sequential(\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.Softmax(dim=-1)\n","        )\n","        self.residual_block1 = nn.Sequential(\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.LayerNorm(hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout)\n","        )\n","        self.residual_block2 = nn.Sequential(\n","            nn.Linear(hidden_dim, hidden_dim // 2),\n","            nn.LayerNorm(hidden_dim // 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout)\n","        )\n","        self.output_layer = nn.Linear(hidden_dim // 2, 1)\n","        \n","    def forward(self, x):\n","        x1 = self.input_layer(x)\n","        # Apply attention\n","        attn_weights = self.attention(x1)\n","        x1 = x1 * attn_weights\n","        x2 = self.residual_block1(x1) + x1  # Residual connection\n","        x3 = self.residual_block2(x2)\n","        return self.output_layer(x3)\n","\n","def weighted_mae_loss(outputs, targets, weights):\n","    \"\"\"\n","    Custom MAE loss with target-specific weights.\n","    \"\"\"\n","    return torch.mean(weights * torch.abs(outputs.squeeze() - targets))\n","\n","def objective(trial, X, y, cv, target):\n","    \"\"\"\n","    Objective function for Optuna hyperparameter tuning.\n","    \"\"\"\n","    params = {\n","        'hidden_dim': trial.suggest_int('hidden_dim', 128, 512, step=64),  # Wider range\n","        'dropout': trial.suggest_float('dropout', 0.1, 0.4),\n","        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n","        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n","        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n","    }\n","    \n","    scores = []\n","    for train_idx, dev_idx in cv.split(X, y):\n","        X_tr, X_dev = X[train_idx], X[dev_idx]\n","        y_tr, y_dev = y[train_idx], y[dev_idx]\n","        \n","        train_dataset = PolymerDataset(pd.DataFrame(X_tr), pd.Series(y_tr))\n","        dev_dataset = PolymerDataset(pd.DataFrame(X_dev), pd.Series(y_dev))\n","        \n","        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n","        dev_loader = DataLoader(dev_dataset, batch_size=params['batch_size'])\n","        \n","        model = AttentionResidualMLP(input_dim=X_tr.shape[1], hidden_dim=params['hidden_dim'], dropout=params['dropout']).to(device)\n","        criterion = nn.L1Loss()\n","        optimizer = optim.AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n","        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)  # Longer cycle\n","        scaler = GradScaler()\n","        \n","        best_loss = float('inf')\n","        patience = 5  # Increased patience\n","        counter = 0\n","        for epoch in range(50):  # More epochs\n","            model.train()\n","            for X_batch, y_batch in train_loader:\n","                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","                optimizer.zero_grad()\n","                with autocast():\n","                    outputs = model(X_batch)\n","                    loss = criterion(outputs.squeeze(), y_batch)\n","                scaler.scale(loss).backward()\n","                scaler.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n","                scaler.step(optimizer)\n","                scaler.update()\n","            \n","            model.eval()\n","            val_loss = 0\n","            with torch.no_grad():\n","                for X_batch, y_batch in dev_loader:\n","                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","                    outputs = model(X_batch)\n","                    val_loss += criterion(outputs.squeeze(), y_batch).item() * len(y_batch)\n","            val_loss /= len(dev_dataset)\n","            scheduler.step()\n","            \n","            if val_loss < best_loss:\n","                best_loss = val_loss\n","                counter = 0\n","            else:\n","                counter += 1\n","                if counter >= patience:\n","                    break\n","        \n","        model.eval()\n","        preds = []\n","        with torch.no_grad():\n","            for X_batch, _ in dev_loader:\n","                X_batch = X_batch.to(device)\n","                preds.extend(model(X_batch).squeeze().cpu().numpy())\n","        scores.append(mean_absolute_error(y_dev, preds))\n","    \n","    return np.mean(scores)\n","\n","OOF_Preds = []\n","Test_Preds = []\n","drop_cols = [\"Source\", \"id\", \"Id\", \"SMILES\"] + CFG.targets\n","seeds = [42, 123, 456, 789, 101]  # More seeds for bagging\n","target_weights = {'Tg': 0.5, 'FFV': 2.0, 'Tc': 1.0, 'Density': 1.0, 'Rg': 0.8}  # wMAE weights\n","target_scalers = {\n","    'Tg': RobustScaler(),\n","    'FFV': RobustScaler(),\n","    'Tc': RobustScaler(),\n","    'Density': RobustScaler(),\n","    'Rg': RobustScaler()\n","}  # Target-specific scaling\n","\n","for target in tqdm(CFG.targets):\n","    print(f\"\\n=== Training Neural Network for {target} ===\\n\")\n","    \n","    # Prepare data\n","    Xtrain_ = Xtrain.copy()\n","    Xtrain_[target] = Ytrain[target]\n","    Xtrain_ = Xtrain_.dropna(subset=[target])\n","    idx = Xtrain_.index\n","    Xtrain_.index = range(len(Xtrain_))\n","    ytrain_ = Xtrain_[target]\n","    \n","    # Scale target\n","    ytrain_scaled = target_scalers[target].fit_transform(ytrain_.values.reshape(-1, 1)).flatten()\n","    \n","    # Filter drop_cols\n","    existing_drop_cols = [col for col in drop_cols if col in Xtrain_.columns and col != target]\n","    \n","    # Feature selection\n","    selector = VarianceThreshold(threshold=0.01)\n","    Xtrain_features = Xtrain_.drop(existing_drop_cols + [target], axis=1, errors='ignore')\n","    Xtrain_selected = selector.fit_transform(Xtrain_features.fillna(0))\n","    Xtest_features = Xtest.drop([col for col in existing_drop_cols if col in Xtest.columns], axis=1, errors='ignore')\n","    Xtest_selected = selector.transform(Xtest_features.fillna(0))\n","    \n","    # Scale features\n","    scaler = RobustScaler()\n","    Xtrain_scaled = scaler.fit_transform(Xtrain_selected)\n","    Xtest_scaled = scaler.transform(Xtest_selected)\n","    \n","    # Cross-validation setup\n","    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n","    oof_preds = np.zeros(len(Xtrain_))\n","    test_preds = np.zeros(len(Xtest))\n","    \n","    # Hyperparameter tuning\n","    study = optuna.create_study(direction='minimize')\n","    study.optimize(lambda trial: objective(trial, Xtrain_scaled, ytrain_scaled, cv, target), n_trials=10)  # More trials\n","    best_params = study.best_params\n","    \n","    # Ensemble across seeds\n","    for seed in seeds:\n","        torch.manual_seed(seed)\n","        np.random.seed(seed)\n","        \n","        for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_scaled, ytrain_scaled)):\n","            X_tr, X_dev = Xtrain_scaled[train_idx], Xtrain_scaled[dev_idx]\n","            y_tr, y_dev = ytrain_scaled[train_idx], ytrain_scaled[dev_idx]\n","            \n","            # Create datasets\n","            train_dataset = PolymerDataset(pd.DataFrame(X_tr), pd.Series(y_tr))\n","            dev_dataset = PolymerDataset(pd.DataFrame(X_dev), pd.Series(y_dev))\n","            test_dataset = PolymerDataset(pd.DataFrame(Xtest_scaled))\n","            \n","            train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n","            dev_loader = DataLoader(dev_dataset, batch_size=best_params['batch_size'])\n","            test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n","            \n","            # Initialize model\n","            model = AttentionResidualMLP(input_dim=X_tr.shape[1], hidden_dim=best_params['hidden_dim'], dropout=best_params['dropout']).to(device)\n","            criterion = lambda x, y: weighted_mae_loss(x, y, torch.tensor(target_weights[target], device=device))\n","            optimizer = optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n","            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n","            scaler = GradScaler()\n","            \n","            # Training loop\n","            best_loss = float('inf')\n","            patience = 5\n","            counter = 0\n","            for epoch in range(50):\n","                model.train()\n","                for X_batch, y_batch in train_loader:\n","                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","                    optimizer.zero_grad()\n","                    with autocast():\n","                        outputs = model(X_batch)\n","                        loss = criterion(outputs, y_batch)\n","                    scaler.scale(loss).backward()\n","                    scaler.unscale_(optimizer)\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                    scaler.step(optimizer)\n","                    scaler.update()\n","                \n","                # Validation\n","                model.eval()\n","                val_loss = 0\n","                with torch.no_grad():\n","                    for X_batch, y_batch in dev_loader:\n","                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","                        outputs = model(X_batch)\n","                        val_loss += criterion(outputs, y_batch).item() * len(y_batch)\n","                val_loss /= len(dev_dataset)\n","                scheduler.step()\n","                \n","                # Early stopping\n","                if val_loss < best_loss:\n","                    best_loss = val_loss\n","                    counter = 0\n","                    best_model = model.state_dict()\n","                else:\n","                    counter += 1\n","                    if counter >= patience:\n","                        break\n","            \n","            # Load best model\n","            model.load_state_dict(best_model)\n","            model.eval()\n","            with torch.no_grad():\n","                dev_preds = []\n","                for X_batch, _ in dev_loader:\n","                    X_batch = X_batch.to(device)\n","                    dev_preds.extend(model(X_batch).squeeze().cpu().numpy())\n","                oof_preds[dev_idx] += target_scalers[target].inverse_transform(np.array(dev_preds).reshape(-1, 1)).flatten() / (CFG.n_splits * len(seeds))\n","                \n","                test_preds_fold = []\n","                for X_batch in test_loader:\n","                    X_batch = X_batch.to(device)\n","                    test_preds_fold.extend(model(X_batch).squeeze().cpu().numpy())\n","                test_preds += target_scalers[target].inverse_transform(np.array(test_preds_fold).reshape(-1, 1)).flatten() / (CFG.n_splits * len(seeds))\n","    \n","    # Store predictions\n","    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n","    Test_Preds.append(pd.DataFrame({target: test_preds}))\n","    \n","    # Compute score\n","    score = mean_absolute_error(ytrain_, oof_preds)\n","    print(f\"Neural Network CV score for {target}: {score:.8f}\")\n","    \n","    clean_memory()\n","\n","# Combine predictions\n","nn_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\n","nn_test_preds = pd.concat(Test_Preds, axis=1)\n","\n","# Submission\n","sub_fl = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\n","nn_test_preds[\"id\"] = sub_fl[\"id\"]\n","nn_test_preds[[\"id\"] + CFG.targets].to_csv(\"submission.csv\", index=False)\n","print(\"\\nNeural Network submission saved as submission.csv\")"]},{"cell_type":"markdown","id":"4648c5f8","metadata":{"papermill":{"duration":0.022415,"end_time":"2025-06-20T15:04:33.736915","exception":false,"start_time":"2025-06-20T15:04:33.7145","status":"completed"},"tags":[]},"source":["## Approach 3: Graph Neural Network\n","\n","Graph Neural Networks (GNNs) model polymers as molecular graphs, where atoms are nodes and bonds are edges. This captures spatial and structural relationships better than descriptor-based methods. We use DeepChem’s GraphConvModel, which is well-suited for molecular data. Due to computational constraints, we apply this to FFV and optionally to other targets with reduced epochs for sparse data."]},{"cell_type":"markdown","id":"d417402d","metadata":{"papermill":{"duration":0.02207,"end_time":"2025-06-20T15:04:33.781832","exception":false,"start_time":"2025-06-20T15:04:33.759762","status":"completed"},"tags":[]},"source":["%%time\n","\n","import deepchem as dc\n","from deepchem.models import GraphConvModel\n","\n","OOF_Preds = []\n","Test_Preds = []\n","drop_cols = [\"Source\", \"id\", \"Id\", \"SMILES\"] + CFG.targets\n","\n","for target in tqdm(['FFV']):\n","    print(f\"\\n=== Training Graph Neural Network for {target} ===\\n\")\n","    \n","    # Prepare data\n","    Xtrain_ = Xtrain.copy()\n","    Xtrain_[target] = Ytrain[target]\n","    Xtrain_ = Xtrain_.dropna(subset=[target])\n","    idx = Xtrain_.index\n","    Xtrain_.index = range(len(Xtrain_))\n","    ytrain_ = Xtrain_[target]\n","    \n","    # Convert SMILES to molecular graphs\n","    featurizer = dc.feat.ConvMolFeaturizer()\n","    train_mols = [Chem.MolFromSmiles(smi) for smi in Xtrain_['SMILES']]\n","    test_mols = [Chem.MolFromSmiles(smi) for smi in Xtest['SMILES']]\n","    \n","    train_features = featurizer.featurize(train_mols)\n","    test_features = featurizer.featurize(test_mols)\n","    \n","    # Filter invalid molecules\n","    valid_train_idx = [i for i, feat in enumerate(train_features) if isinstance(feat, dc.feat.mol_graphs.ConvMol)]\n","    valid_test_idx = [i for i, feat in enumerate(test_features) if isinstance(feat, dc.feat.mol_graphs.ConvMol)]\n","    \n","    train_features = [train_features[i] for i in valid_train_idx]\n","    ytrain_ = ytrain_.iloc[valid_train_idx]\n","    idx = idx[valid_train_idx]\n","    test_features = [test_features[i] for i in valid_test_idx]\n","    \n","    # Cross-validation setup\n","    cv = KFold(n_splits=3, shuffle=True, random_state=CFG.random_state)\n","    oof_preds = np.zeros(len(ytrain_))\n","    test_preds = np.zeros(len(test_mols))\n","    \n","    for fold, (train_idx, dev_idx) in enumerate(cv.split(train_features, ytrain_)):\n","        X_tr = [train_features[i] for i in train_idx]\n","        X_dev = [train_features[i] for i in dev_idx]\n","        y_tr = ytrain_.iloc[train_idx]\n","        y_dev = ytrain_.iloc[dev_idx]\n","        \n","        # Create datasets\n","        train_dataset = dc.data.NumpyDataset(X_tr, y_tr.values)\n","        dev_dataset = dc.data.NumpyDataset(X_dev, y_dev.values)\n","        test_dataset = dc.data.NumpyDataset(test_features)\n","        \n","        # Initialize model\n","        model = GraphConvModel(n_tasks=1, mode='regression', batch_size=16, learning_rate=0.001, graph_conv_layers=[32, 32])\n","        \n","        # Train model\n","        model.fit(train_dataset, nb_epoch=30)\n","        \n","        # Predict\n","        oof_preds[dev_idx] = model.predict(dev_dataset).flatten()\n","        test_preds[valid_test_idx] += model.predict(test_dataset).flatten() / 3\n","    \n","    # Store predictions\n","    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n","    Test_Preds.append(pd.DataFrame({target: test_preds}))\n","    \n","    # Compute score\n","    score = mean_absolute_error(ytrain_, oof_preds)\n","    print(f\"GNN CV score for {target}: {score:.8f}\")\n","    \n","    clean_memory()\n","\n","# Combine predictions (fill others with NaN)\n","gnn_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\n","gnn_test_preds = pd.concat(Test_Preds, axis=1)\n","for t in CFG.targets:\n","    if t not in gnn_oof_preds.columns:\n","        gnn_oof_preds[t] = np.nan\n","        gnn_test_preds[t] = np.nan\n","\n","# Submission\n","sub_fl = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\n","gnn_test_preds[\"id\"] = sub_fl[\"id\"]\n","gnn_test_preds[[\"id\"] + CFG.targets].to_csv(\"submission_gnn.csv\", index=False)\n","print(\"\\nGNN submission saved as submission_gnn.csv\")"]},{"cell_type":"markdown","id":"c1f97a0c","metadata":{"papermill":{"duration":0.021956,"end_time":"2025-06-20T15:04:33.826178","exception":false,"start_time":"2025-06-20T15:04:33.804222","status":"completed"},"tags":[]},"source":["## Approach 4: Stacking Ensemble\n","The stacking ensemble combines predictions from the gradient boosting models and neural network using a meta-learner (Linear Regression). This approach leverages the strengths of individual models to improve overall accuracy. First-level predictions are generated via cross-validation, and the meta-learner is trained on these predictions to produce the final output."]},{"cell_type":"code","execution_count":2,"id":"5af75d7d","metadata":{"execution":{"iopub.execute_input":"2025-06-20T15:04:33.874847Z","iopub.status.busy":"2025-06-20T15:04:33.874589Z","iopub.status.idle":"2025-06-20T15:47:20.475633Z","shell.execute_reply":"2025-06-20T15:47:20.474966Z"},"papermill":{"duration":2566.627798,"end_time":"2025-06-20T15:47:20.476733","exception":false,"start_time":"2025-06-20T15:04:33.848935","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["--- RUNNING SCRIPT 1: GBDT STACKING MODEL ---\n","  Training GBDT for Tg...\n","  Training GBDT for FFV...\n","  Training GBDT for Tc...\n","  Training GBDT for Density...\n","  Training GBDT for Rg...\n","--- SCRIPT 1 COMPLETE: GBDT predictions and OOF file saved. ---\n","\n","--- RUNNING SCRIPT 2: NEURAL NETWORK MODEL ---\n"]},{"name":"stderr","output_type":"stream","text":["NN Targets:   0%|          | 0/5 [00:00<?, ?it/s][I 2025-06-20 15:10:52,354] A new study created in memory with name: no-name-fe8faf38-e09a-4c2a-b3bb-a1e790f2092d\n"]},{"name":"stdout","output_type":"stream","text":["  Training Neural Network for Tg...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-06-20 15:11:05,811] Trial 0 finished with value: 0.41289010269057813 and parameters: {'hidden_dim': 384, 'dropout': 0.27858358530991245, 'learning_rate': 7.417471150640797e-05, 'batch_size': 64, 'weight_decay': 2.123682976185424e-06}. Best is trial 0 with value: 0.41289010269057813.\n","[I 2025-06-20 15:11:10,119] Trial 1 finished with value: 0.3700394649806735 and parameters: {'hidden_dim': 256, 'dropout': 0.1620430819861643, 'learning_rate': 0.0031422275830705294, 'batch_size': 32, 'weight_decay': 9.921569931477194e-06}. Best is trial 1 with value: 0.3700394649806735.\n","[I 2025-06-20 15:11:26,989] Trial 2 finished with value: 0.3767794530248606 and parameters: {'hidden_dim': 512, 'dropout': 0.29436615022110557, 'learning_rate': 6.264581117047302e-05, 'batch_size': 16, 'weight_decay': 0.00015275997130249097}. Best is trial 1 with value: 0.3700394649806735.\n","[I 2025-06-20 15:11:54,506] Trial 3 finished with value: 0.43949772751905114 and parameters: {'hidden_dim': 384, 'dropout': 0.18382691627801565, 'learning_rate': 1.9231805700262847e-05, 'batch_size': 16, 'weight_decay': 3.7490314650300092e-06}. Best is trial 1 with value: 0.3700394649806735.\n","[I 2025-06-20 15:12:01,071] Trial 4 finished with value: 0.4365827451635103 and parameters: {'hidden_dim': 384, 'dropout': 0.3558256473936504, 'learning_rate': 0.009729968500380005, 'batch_size': 16, 'weight_decay': 3.4786628422843997e-06}. Best is trial 1 with value: 0.3700394649806735.\n","[I 2025-06-20 15:12:03,157] Trial 5 finished with value: 0.3663621616720465 and parameters: {'hidden_dim': 384, 'dropout': 0.2437354919611185, 'learning_rate': 0.0041273985038324055, 'batch_size': 64, 'weight_decay': 6.248038278967219e-06}. Best is trial 5 with value: 0.3663621616720465.\n","[I 2025-06-20 15:12:07,463] Trial 6 finished with value: 0.3627910774081967 and parameters: {'hidden_dim': 192, 'dropout': 0.382312787810702, 'learning_rate': 0.006695247851257887, 'batch_size': 32, 'weight_decay': 4.555555029473743e-06}. Best is trial 6 with value: 0.3627910774081967.\n","[I 2025-06-20 15:12:24,298] Trial 7 finished with value: 0.3701315161711063 and parameters: {'hidden_dim': 256, 'dropout': 0.12892992292661912, 'learning_rate': 7.632320524167424e-05, 'batch_size': 16, 'weight_decay': 1.5644599952654121e-06}. Best is trial 6 with value: 0.3627910774081967.\n","[I 2025-06-20 15:12:29,778] Trial 8 finished with value: 0.36030957956113974 and parameters: {'hidden_dim': 320, 'dropout': 0.2957733572165643, 'learning_rate': 0.0003115146340946117, 'batch_size': 32, 'weight_decay': 6.517050202101402e-05}. Best is trial 8 with value: 0.36030957956113974.\n","[I 2025-06-20 15:12:34,390] Trial 9 finished with value: 0.3629452908165104 and parameters: {'hidden_dim': 256, 'dropout': 0.25645625546696915, 'learning_rate': 0.0006514928974740263, 'batch_size': 32, 'weight_decay': 0.0004928357842859822}. Best is trial 8 with value: 0.36030957956113974.\n","NN Targets:  20%|██        | 1/5 [01:58<07:54, 118.51s/it]"]},{"name":"stdout","output_type":"stream","text":["  Neural Network CV score for Tg: 92.82990586\n","  Training Neural Network for FFV...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-06-20 15:12:51,018] A new study created in memory with name: no-name-7f8f9630-e29c-4038-a249-c873116a4a95\n","[I 2025-06-20 15:15:10,886] Trial 0 finished with value: 0.18010484101647656 and parameters: {'hidden_dim': 256, 'dropout': 0.1340277077592507, 'learning_rate': 0.0005093098426135426, 'batch_size': 32, 'weight_decay': 0.00012180210052767605}. Best is trial 0 with value: 0.18010484101647656.\n","[I 2025-06-20 15:16:28,724] Trial 1 finished with value: 0.21316080493491504 and parameters: {'hidden_dim': 512, 'dropout': 0.2063138277775779, 'learning_rate': 7.016509418580735e-05, 'batch_size': 64, 'weight_decay': 0.00010873415635303614}. Best is trial 0 with value: 0.18010484101647656.\n","[I 2025-06-20 15:18:52,046] Trial 2 finished with value: 0.20356494055806854 and parameters: {'hidden_dim': 320, 'dropout': 0.22984447640740963, 'learning_rate': 9.296045380818418e-05, 'batch_size': 32, 'weight_decay': 2.295813195586595e-05}. Best is trial 0 with value: 0.18010484101647656.\n","[I 2025-06-20 15:20:25,418] Trial 3 finished with value: 0.2626408416928603 and parameters: {'hidden_dim': 384, 'dropout': 0.21130487708026635, 'learning_rate': 2.7781483847626733e-05, 'batch_size': 64, 'weight_decay': 5.655770330919378e-05}. Best is trial 0 with value: 0.18010484101647656.\n","[I 2025-06-20 15:22:49,542] Trial 4 finished with value: 0.18179519456172744 and parameters: {'hidden_dim': 256, 'dropout': 0.21009066460421624, 'learning_rate': 0.0005249266894842084, 'batch_size': 32, 'weight_decay': 4.852667965677262e-05}. Best is trial 0 with value: 0.18010484101647656.\n","[I 2025-06-20 15:24:06,848] Trial 5 finished with value: 0.19111285240703252 and parameters: {'hidden_dim': 512, 'dropout': 0.2643320728595507, 'learning_rate': 0.0001975278268547933, 'batch_size': 64, 'weight_decay': 2.53083365615306e-06}. Best is trial 0 with value: 0.18010484101647656.\n","[I 2025-06-20 15:28:25,332] Trial 6 finished with value: 0.18547034714074123 and parameters: {'hidden_dim': 128, 'dropout': 0.17372511350748632, 'learning_rate': 0.001282095516429482, 'batch_size': 16, 'weight_decay': 2.418742313369861e-05}. Best is trial 0 with value: 0.18010484101647656.\n","[I 2025-06-20 15:30:43,839] Trial 7 finished with value: 0.18750260130872315 and parameters: {'hidden_dim': 256, 'dropout': 0.33059430751794827, 'learning_rate': 0.001478513884106837, 'batch_size': 32, 'weight_decay': 3.5810798312831886e-06}. Best is trial 0 with value: 0.18010484101647656.\n","[I 2025-06-20 15:31:58,720] Trial 8 finished with value: 0.18621277549877774 and parameters: {'hidden_dim': 128, 'dropout': 0.16753905810442982, 'learning_rate': 0.0021540263058247577, 'batch_size': 64, 'weight_decay': 1.6829608454745576e-05}. Best is trial 0 with value: 0.18010484101647656.\n","[I 2025-06-20 15:33:11,599] Trial 9 finished with value: 0.18460885432959911 and parameters: {'hidden_dim': 448, 'dropout': 0.22446241477933437, 'learning_rate': 0.00044325324251615414, 'batch_size': 64, 'weight_decay': 1.1926411646952657e-05}. Best is trial 0 with value: 0.18010484101647656.\n","NN Targets:  40%|████      | 2/5 [29:26<50:54, 1018.19s/it][I 2025-06-20 15:40:18,811] A new study created in memory with name: no-name-6b873423-866d-46fd-8b3b-27b6155f51f6\n"]},{"name":"stdout","output_type":"stream","text":["  Neural Network CV score for FFV: 0.29391058\n","  Training Neural Network for Tc...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-06-20 15:40:23,551] Trial 0 finished with value: 0.19635745423981701 and parameters: {'hidden_dim': 448, 'dropout': 0.22744032432573147, 'learning_rate': 0.0015485416983196988, 'batch_size': 64, 'weight_decay': 6.89605345947444e-06}. Best is trial 0 with value: 0.19635745423981701.\n","[I 2025-06-20 15:40:31,013] Trial 1 finished with value: 0.19804777575880048 and parameters: {'hidden_dim': 192, 'dropout': 0.3676205299339157, 'learning_rate': 0.00033187731671628596, 'batch_size': 64, 'weight_decay': 0.0006153792135273262}. Best is trial 0 with value: 0.19635745423981701.\n","[I 2025-06-20 15:40:48,514] Trial 2 finished with value: 0.25048018254912596 and parameters: {'hidden_dim': 256, 'dropout': 0.14807163020394465, 'learning_rate': 2.5063016093134245e-05, 'batch_size': 32, 'weight_decay': 2.9096932601970973e-06}. Best is trial 0 with value: 0.19635745423981701.\n","[I 2025-06-20 15:40:56,068] Trial 3 finished with value: 0.18629656531026556 and parameters: {'hidden_dim': 128, 'dropout': 0.17075215913480085, 'learning_rate': 0.0017297014684537, 'batch_size': 32, 'weight_decay': 1.6787818905283193e-06}. Best is trial 3 with value: 0.18629656531026556.\n","[I 2025-06-20 15:41:05,507] Trial 4 finished with value: 0.4186147720864667 and parameters: {'hidden_dim': 256, 'dropout': 0.3399781400390858, 'learning_rate': 2.471082356753873e-05, 'batch_size': 64, 'weight_decay': 3.573761550726579e-06}. Best is trial 3 with value: 0.18629656531026556.\n","[I 2025-06-20 15:41:09,569] Trial 5 finished with value: 0.21061010827277493 and parameters: {'hidden_dim': 384, 'dropout': 0.1971466447145103, 'learning_rate': 0.0096806290725885, 'batch_size': 64, 'weight_decay': 0.000480794814033431}. Best is trial 3 with value: 0.18629656531026556.\n","[I 2025-06-20 15:41:21,152] Trial 6 finished with value: 0.36937133992687865 and parameters: {'hidden_dim': 320, 'dropout': 0.3492395168992792, 'learning_rate': 2.1456188986905456e-05, 'batch_size': 64, 'weight_decay': 1.9214429859307104e-06}. Best is trial 3 with value: 0.18629656531026556.\n","[I 2025-06-20 15:41:44,664] Trial 7 finished with value: 0.2174066552297412 and parameters: {'hidden_dim': 512, 'dropout': 0.3530203955974124, 'learning_rate': 4.092173995432974e-05, 'batch_size': 16, 'weight_decay': 1.8926614774736684e-05}. Best is trial 3 with value: 0.18629656531026556.\n","[I 2025-06-20 15:41:50,087] Trial 8 finished with value: 0.19223733682419658 and parameters: {'hidden_dim': 256, 'dropout': 0.3674455130247357, 'learning_rate': 0.006961383734299415, 'batch_size': 64, 'weight_decay': 0.00023482623610549176}. Best is trial 3 with value: 0.18629656531026556.\n","[I 2025-06-20 15:42:23,647] Trial 9 finished with value: 0.2429913768530315 and parameters: {'hidden_dim': 192, 'dropout': 0.3380311173158671, 'learning_rate': 3.269798166306742e-05, 'batch_size': 16, 'weight_decay': 0.0004003334357134599}. Best is trial 3 with value: 0.18629656531026556.\n","NN Targets:  60%|██████    | 3/5 [31:53<20:41, 620.59s/it] [I 2025-06-20 15:42:46,243] A new study created in memory with name: no-name-9d60128d-255f-49b4-a011-a451695fbd8f\n"]},{"name":"stdout","output_type":"stream","text":["  Neural Network CV score for Tc: 0.20533856\n","  Training Neural Network for Density...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-06-20 15:42:52,982] Trial 0 finished with value: 0.20024101755278823 and parameters: {'hidden_dim': 320, 'dropout': 0.10626125759993284, 'learning_rate': 0.0019307446814006081, 'batch_size': 32, 'weight_decay': 3.356118589735505e-06}. Best is trial 0 with value: 0.20024101755278823.\n","[I 2025-06-20 15:42:58,452] Trial 1 finished with value: 0.28680748869935957 and parameters: {'hidden_dim': 448, 'dropout': 0.2942458838293634, 'learning_rate': 9.48682582943146e-05, 'batch_size': 64, 'weight_decay': 7.640908302555496e-05}. Best is trial 0 with value: 0.20024101755278823.\n","[I 2025-06-20 15:43:14,385] Trial 2 finished with value: 0.19869447747916322 and parameters: {'hidden_dim': 384, 'dropout': 0.359241148131234, 'learning_rate': 0.00028922749959017096, 'batch_size': 16, 'weight_decay': 9.389952311428072e-05}. Best is trial 2 with value: 0.19869447747916322.\n","[I 2025-06-20 15:43:25,329] Trial 3 finished with value: 0.2832072541344561 and parameters: {'hidden_dim': 192, 'dropout': 0.3651293441283253, 'learning_rate': 0.00011220979025612166, 'batch_size': 32, 'weight_decay': 4.581127024961768e-06}. Best is trial 2 with value: 0.19869447747916322.\n","[I 2025-06-20 15:43:36,824] Trial 4 finished with value: 0.23470051076590223 and parameters: {'hidden_dim': 384, 'dropout': 0.24500161885332294, 'learning_rate': 0.00011347154338201607, 'batch_size': 32, 'weight_decay': 7.4195936092534925e-06}. Best is trial 2 with value: 0.19869447747916322.\n","[I 2025-06-20 15:43:46,014] Trial 5 finished with value: 0.5593288364906671 and parameters: {'hidden_dim': 320, 'dropout': 0.1409555946368261, 'learning_rate': 1.2911491879475699e-05, 'batch_size': 64, 'weight_decay': 0.0009232369793252058}. Best is trial 2 with value: 0.19869447747916322.\n","[I 2025-06-20 15:43:54,839] Trial 6 finished with value: 0.30572154669420726 and parameters: {'hidden_dim': 192, 'dropout': 0.13265696007689817, 'learning_rate': 6.397390235024074e-05, 'batch_size': 64, 'weight_decay': 9.615302724000284e-05}. Best is trial 2 with value: 0.19869447747916322.\n","[I 2025-06-20 15:43:59,536] Trial 7 finished with value: 0.2571274618746425 and parameters: {'hidden_dim': 448, 'dropout': 0.2685412903758174, 'learning_rate': 0.006956057110217094, 'batch_size': 64, 'weight_decay': 0.00019850006039471495}. Best is trial 2 with value: 0.19869447747916322.\n","[I 2025-06-20 15:44:16,764] Trial 8 finished with value: 0.4850385803144497 and parameters: {'hidden_dim': 512, 'dropout': 0.20656260403370413, 'learning_rate': 1.1197112119222809e-05, 'batch_size': 32, 'weight_decay': 8.347846696782598e-05}. Best is trial 2 with value: 0.19869447747916322.\n","[I 2025-06-20 15:44:25,141] Trial 9 finished with value: 0.19488546632158538 and parameters: {'hidden_dim': 128, 'dropout': 0.24517996101046127, 'learning_rate': 0.0008295371723784684, 'batch_size': 32, 'weight_decay': 2.606766620388557e-05}. Best is trial 9 with value: 0.19488546632158538.\n","NN Targets:  80%|████████  | 4/5 [34:01<07:05, 425.93s/it][I 2025-06-20 15:44:53,775] A new study created in memory with name: no-name-0776358d-27fd-40e7-a685-e1912324a8c4\n"]},{"name":"stdout","output_type":"stream","text":["  Neural Network CV score for Density: 0.78940139\n","  Training Neural Network for Rg...\n"]},{"name":"stderr","output_type":"stream","text":["[I 2025-06-20 15:45:04,064] Trial 0 finished with value: 0.23792295469804808 and parameters: {'hidden_dim': 384, 'dropout': 0.25651001365070336, 'learning_rate': 0.00017132255457120928, 'batch_size': 32, 'weight_decay': 5.3985253248564655e-06}. Best is trial 0 with value: 0.23792295469804808.\n","[I 2025-06-20 15:45:29,094] Trial 1 finished with value: 0.260556204914702 and parameters: {'hidden_dim': 448, 'dropout': 0.3451019762120703, 'learning_rate': 5.9540132344261834e-05, 'batch_size': 16, 'weight_decay': 9.093184048712408e-06}. Best is trial 0 with value: 0.23792295469804808.\n","[I 2025-06-20 15:46:01,465] Trial 2 finished with value: 0.38737653319307663 and parameters: {'hidden_dim': 384, 'dropout': 0.22452320403534504, 'learning_rate': 1.022201921482735e-05, 'batch_size': 16, 'weight_decay': 1.407640481451453e-05}. Best is trial 0 with value: 0.23792295469804808.\n","[I 2025-06-20 15:46:04,341] Trial 3 finished with value: 0.23897033698314454 and parameters: {'hidden_dim': 256, 'dropout': 0.21478224782477273, 'learning_rate': 0.005592684379978348, 'batch_size': 64, 'weight_decay': 0.00014153223782565884}. Best is trial 0 with value: 0.23792295469804808.\n","[I 2025-06-20 15:46:19,319] Trial 4 finished with value: 0.2332814284774706 and parameters: {'hidden_dim': 128, 'dropout': 0.32176434764550826, 'learning_rate': 0.0002740126797185732, 'batch_size': 16, 'weight_decay': 2.7691364877780593e-06}. Best is trial 4 with value: 0.2332814284774706.\n","[I 2025-06-20 15:46:29,618] Trial 5 finished with value: 0.23628695896000823 and parameters: {'hidden_dim': 512, 'dropout': 0.14561216473052946, 'learning_rate': 0.0008692123643472353, 'batch_size': 16, 'weight_decay': 1.0987478013756898e-06}. Best is trial 4 with value: 0.2332814284774706.\n","[I 2025-06-20 15:46:40,524] Trial 6 finished with value: 0.23369067203404487 and parameters: {'hidden_dim': 320, 'dropout': 0.3568372395851037, 'learning_rate': 0.0005348441390507978, 'batch_size': 16, 'weight_decay': 1.1699679424430103e-06}. Best is trial 4 with value: 0.2332814284774706.\n","[I 2025-06-20 15:46:46,994] Trial 7 finished with value: 0.22885460553486253 and parameters: {'hidden_dim': 256, 'dropout': 0.34988145793510894, 'learning_rate': 0.0015701616200756814, 'batch_size': 32, 'weight_decay': 6.046619174581407e-06}. Best is trial 7 with value: 0.22885460553486253.\n","[I 2025-06-20 15:46:53,322] Trial 8 finished with value: 0.2570688295960454 and parameters: {'hidden_dim': 128, 'dropout': 0.3218977017631485, 'learning_rate': 0.00028354562665458544, 'batch_size': 64, 'weight_decay': 3.4108040190543663e-06}. Best is trial 7 with value: 0.22885460553486253.\n","[I 2025-06-20 15:47:01,121] Trial 9 finished with value: 0.23713759947916788 and parameters: {'hidden_dim': 320, 'dropout': 0.19685299940868933, 'learning_rate': 0.00016062521244769308, 'batch_size': 32, 'weight_decay': 9.60448409121527e-06}. Best is trial 7 with value: 0.22885460553486253.\n","NN Targets: 100%|██████████| 5/5 [36:27<00:00, 437.54s/it]"]},{"name":"stdout","output_type":"stream","text":["  Neural Network CV score for Rg: 13.16957143\n","--- SCRIPT 2 COMPLETE: Neural Network predictions and OOF file saved. ---\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","--- RUNNING SCRIPT 3: OPTIMAL BLENDING ---\n","Finding optimal blend weights...\n","  Best weight for Tg: 0.00 (Local RMSE: 64.75848)\n","  Best weight for FFV: 0.00 (Local RMSE: 0.01279)\n","  Best weight for Tc: 0.00 (Local RMSE: 0.04069)\n","  Best weight for Density: 0.00 (Local RMSE: 0.07008)\n","  Best weight for Rg: 0.00 (Local RMSE: 2.46020)\n","\n","Blending test predictions...\n","\n","Final submission saved as submission.csv\n","Preview:\n","           id          Tg       FFV        Tc   Density         Rg\n","0  1109053969  156.259104  0.375796  0.186964  1.178455  21.283007\n","1  1422188626  171.046137  0.378355  0.242433  1.097039  21.010584\n","2  2032016830   95.031012  0.353134  0.262752  1.140527  21.157024\n","CPU times: user 47min 44s, sys: 25.4 s, total: 48min 9s\n","Wall time: 42min 46s\n"]}],"source":["\n","%%time\n","\n","# Suppress warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Install dependencies\n","#!pip install pandas numpy torch scikit-learn optuna tqdm rdkit xgboost lightgbm\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.linear_model import Ridge\n","import xgboost as xgb\n","import lightgbm as lgb\n","from rdkit import Chem, rdBase\n","from rdkit.Chem import Descriptors, AllChem\n","from rdkit.ML.Descriptors import MoleculeDescriptors\n","from tqdm import tqdm\n","import optuna\n","import gc\n","from torch.cuda.amp import GradScaler, autocast\n","rdBase.DisableLog('rdApp.warning')\n","\n","# Configuration\n","class CFG:\n","    data_path = \"/kaggle/input/neurips-open-polymer-prediction-2025/\"\n","    targets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n","    n_splits = 5\n","    random_state = 42\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Memory cleanup\n","def clean_memory():\n","    gc.collect()\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","\n","# === SCRIPT 1: GBDT STACKING MODEL ===\n","print(\"--- RUNNING SCRIPT 1: GBDT STACKING MODEL ---\")\n","\n","# Feature Engineering\n","def generate_rdkit_features(smiles_str: str):\n","    mol = Chem.MolFromSmiles(smiles_str)\n","    desc_list = [d[0] for d in Descriptors._descList]\n","    morgan_fp_size = 256  # Reduced for efficiency\n","    if mol is None:\n","        return np.full(len(desc_list) + morgan_fp_size, np.nan)\n","    calculator = MoleculeDescriptors.MolecularDescriptorCalculator(desc_list)\n","    descriptors = np.array(calculator.CalcDescriptors(mol))\n","    descriptors[desc_list.index(\"Ipc\")] = np.log1p(descriptors[desc_list.index(\"Ipc\")])  # Log-transform Ipc\n","    mfp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=morgan_fp_size)\n","    mfp_array = np.array(list(mfp.ToBitString())).astype(int)\n","    return np.concatenate([descriptors, mfp_array])\n","\n","# Load data\n","train_df = pd.read_csv(f\"{CFG.data_path}/train.csv\")\n","test_df = pd.read_csv(f\"{CFG.data_path}/test.csv\")\n","sample_df = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\n","\n","# Generate features\n","desc_list_names = [d[0] for d in Descriptors._descList]\n","fp_morgan_cols = [f'mfp_{i}' for i in range(256)]\n","feature_columns = desc_list_names + fp_morgan_cols\n","X = pd.DataFrame(np.vstack([generate_rdkit_features(s) for s in train_df['SMILES']]), columns=feature_columns)\n","X_test = pd.DataFrame(np.vstack([generate_rdkit_features(s) for s in test_df['SMILES']]), columns=feature_columns)\n","\n","# Preprocess features\n","f32_max = np.finfo(np.float32).max\n","for df in [X, X_test]:\n","    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","    df[df > f32_max] = np.nan\n","    df[df < -f32_max] = np.nan\n","impute_values = X.mean()\n","X.fillna(impute_values, inplace=True)\n","X_test = X_test.reindex(columns=X.columns).fillna(impute_values)\n","\n","# GBDT parameters\n","XGB_PARAMS = {'n_estimators': 2000, 'learning_rate': 0.02, 'max_depth': 6, 'subsample': 0.7, 'colsample_bytree': 0.6, 'random_state': CFG.random_state, 'n_jobs': -1, 'tree_method': 'hist'}\n","LGBM_PARAMS = {'n_estimators': 2000, 'learning_rate': 0.02, 'max_depth': 7, 'subsample': 0.7, 'colsample_bytree': 0.6, 'random_state': CFG.random_state, 'n_jobs': -1, 'verbosity': -1}\n","META_MODEL = Ridge(alpha=1.0, random_state=CFG.random_state)\n","\n","gbdt_oof_df = pd.DataFrame(index=train_df.index)\n","gbdt_predictions_df = pd.DataFrame({'id': test_df['id']})\n","\n","for target in CFG.targets:\n","    print(f\"  Training GBDT for {target}...\")\n","    y = train_df[target].dropna()\n","    X_subset = X.loc[y.index]\n","    \n","    oof_preds_xgb = pd.Series(np.zeros(len(X_subset)), index=X_subset.index)\n","    oof_preds_lgb = pd.Series(np.zeros(len(X_subset)), index=X_subset.index)\n","    test_preds_xgb_folds = np.zeros((len(X_test), CFG.n_splits))\n","    test_preds_lgb_folds = np.zeros((len(X_test), CFG.n_splits))\n","    \n","    kf = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n","    for fold, (train_idx, val_idx) in enumerate(kf.split(X_subset, y)):\n","        X_train_fold, y_train_fold = X_subset.iloc[train_idx], y.iloc[train_idx]\n","        X_val_fold, y_val_fold = X_subset.iloc[val_idx], y.iloc[val_idx]\n","        \n","        # XGBoost\n","        xgb_model = xgb.XGBRegressor(**XGB_PARAMS)\n","        xgb_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], callbacks=[xgb.callback.EarlyStopping(50, save_best=True)], verbose=0)\n","        oof_preds_xgb.iloc[val_idx] = xgb_model.predict(X_val_fold)\n","        test_preds_xgb_folds[:, fold] = xgb_model.predict(X_test)\n","        \n","        # LightGBM\n","        lgb_model = lgb.LGBMRegressor(**LGBM_PARAMS)\n","        lgb_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], callbacks=[lgb.early_stopping(50, verbose=False)])\n","        oof_preds_lgb.iloc[val_idx] = lgb_model.predict(X_val_fold)\n","        test_preds_lgb_folds[:, fold] = lgb_model.predict(X_test)\n","    \n","    # Meta-model\n","    X_meta_train = pd.concat([oof_preds_xgb, oof_preds_lgb], axis=1)\n","    X_meta_test = pd.DataFrame({'xgb': np.mean(test_preds_xgb_folds, axis=1), 'lgb': np.mean(test_preds_lgb_folds, axis=1)})\n","    meta_model = META_MODEL\n","    meta_model.fit(X_meta_train, y)\n","    gbdt_predictions_df[target] = meta_model.predict(X_meta_test)\n","    gbdt_oof_df.loc[X_meta_train.index, target] = meta_model.predict(X_meta_train)\n","\n","gbdt_predictions_df.to_csv('submission_gbdt.csv', index=False)\n","gbdt_oof_df.to_csv('oof_gbdt.csv')\n","print(\"--- SCRIPT 1 COMPLETE: GBDT predictions and OOF file saved. ---\")\n","clean_memory()\n","\n","# === SCRIPT 2: NEURAL NETWORK MODEL ===\n","print(\"\\n--- RUNNING SCRIPT 2: NEURAL NETWORK MODEL ---\")\n","\n","class PolymerDataset(Dataset):\n","    def __init__(self, X, y=None):\n","        self.X = torch.tensor(X.values, dtype=torch.float32).nan_to_num(0)\n","        self.y = torch.tensor(y.values, dtype=torch.float32) if y is not None else None\n","    \n","    def __len__(self):\n","        return len(self.X)\n","    \n","    def __getitem__(self, idx):\n","        if self.y is not None:\n","            return self.X[idx], self.y[idx]\n","        return self.X[idx]\n","\n","class AttentionResidualMLP(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, dropout):\n","        super(AttentionResidualMLP, self).__init__()\n","        self.input_layer = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.LayerNorm(hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout)\n","        )\n","        self.attention = nn.Sequential(\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.Softmax(dim=-1)\n","        )\n","        self.residual_block1 = nn.Sequential(\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.LayerNorm(hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout)\n","        )\n","        self.residual_block2 = nn.Sequential(\n","            nn.Linear(hidden_dim, hidden_dim // 2),\n","            nn.LayerNorm(hidden_dim // 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout)\n","        )\n","        self.output_layer = nn.Linear(hidden_dim // 2, 1)\n","        \n","    def forward(self, x):\n","        x1 = self.input_layer(x)\n","        attn_weights = self.attention(x1)\n","        x1 = x1 * attn_weights\n","        x2 = self.residual_block1(x1) + x1\n","        x3 = self.residual_block2(x2)\n","        return self.output_layer(x3)\n","\n","def weighted_mae_loss(outputs, targets, weights):\n","    return torch.mean(weights * torch.abs(outputs.squeeze() - targets))\n","\n","def objective(trial, X, y, cv, target):\n","    params = {\n","        'hidden_dim': trial.suggest_int('hidden_dim', 128, 512, step=64),\n","        'dropout': trial.suggest_float('dropout', 0.1, 0.4),\n","        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n","        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n","        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n","    }\n","    \n","    scores = []\n","    for train_idx, dev_idx in cv.split(X, y):\n","        X_tr, X_dev = X[train_idx], X[dev_idx]\n","        y_tr, y_dev = y[train_idx], y[dev_idx]\n","        \n","        train_dataset = PolymerDataset(pd.DataFrame(X_tr), pd.Series(y_tr))\n","        dev_dataset = PolymerDataset(pd.DataFrame(X_dev), pd.Series(y_dev))\n","        \n","        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n","        dev_loader = DataLoader(dev_dataset, batch_size=params['batch_size'])\n","        \n","        model = AttentionResidualMLP(input_dim=X_tr.shape[1], hidden_dim=params['hidden_dim'], dropout=params['dropout']).to(CFG.device)\n","        criterion = nn.L1Loss()\n","        optimizer = optim.AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n","        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n","        scaler = GradScaler()\n","        \n","        best_loss = float('inf')\n","        patience = 5\n","        counter = 0\n","        for epoch in range(50):\n","            model.train()\n","            for X_batch, y_batch in train_loader:\n","                X_batch, y_batch = X_batch.to(CFG.device), y_batch.to(CFG.device)\n","                optimizer.zero_grad()\n","                with autocast():\n","                    outputs = model(X_batch)\n","                    loss = criterion(outputs.squeeze(), y_batch)\n","                scaler.scale(loss).backward()\n","                scaler.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                scaler.step(optimizer)\n","                scaler.update()\n","            \n","            model.eval()\n","            val_loss = 0\n","            with torch.no_grad():\n","                for X_batch, y_batch in dev_loader:\n","                    X_batch, y_batch = X_batch.to(CFG.device), y_batch.to(CFG.device)\n","                    outputs = model(X_batch)\n","                    val_loss += criterion(outputs.squeeze(), y_batch).item() * len(y_batch)\n","            val_loss /= len(dev_dataset)\n","            scheduler.step()\n","            \n","            if val_loss < best_loss:\n","                best_loss = val_loss\n","                counter = 0\n","            else:\n","                counter += 1\n","                if counter >= patience:\n","                    break\n","        \n","        model.eval()\n","        preds = []\n","        with torch.no_grad():\n","            for X_batch, _ in dev_loader:\n","                X_batch = X_batch.to(CFG.device)\n","                preds.extend(model(X_batch).squeeze().cpu().numpy())\n","        scores.append(mean_absolute_error(y_dev, preds))\n","    \n","    return np.mean(scores)\n","\n","nn_oof_df = pd.DataFrame(index=train_df.index)\n","nn_predictions_df = pd.DataFrame({'id': test_df['id']})\n","target_weights = {'Tg': 0.5, 'FFV': 2.0, 'Tc': 1.0, 'Density': 1.0, 'Rg': 0.8}\n","target_scalers = {t: RobustScaler() for t in CFG.targets}\n","\n","for target in tqdm(CFG.targets, desc=\"NN Targets\"):\n","    print(f\"  Training Neural Network for {target}...\")\n","    \n","    # Prepare data\n","    Xtrain_ = X.loc[train_df[target].dropna().index]\n","    ytrain_ = train_df.loc[Xtrain_.index, target]\n","    \n","    # Scale target\n","    ytrain_scaled = target_scalers[target].fit_transform(ytrain_.values.reshape(-1, 1)).flatten()\n","    \n","    # Feature selection\n","    selector = VarianceThreshold(threshold=0.01)\n","    Xtrain_selected = selector.fit_transform(Xtrain_.fillna(0))\n","    Xtest_selected = selector.transform(X_test.fillna(0))\n","    \n","    # Scale features\n","    scaler = RobustScaler()\n","    Xtrain_scaled = scaler.fit_transform(Xtrain_selected)\n","    Xtest_scaled = scaler.transform(Xtest_selected)\n","    \n","    # Hyperparameter tuning\n","    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n","    study = optuna.create_study(direction='minimize')\n","    study.optimize(lambda trial: objective(trial, Xtrain_scaled, ytrain_scaled, cv, target), n_trials=10)\n","    best_params = study.best_params\n","    \n","    # Train final model\n","    oof_preds = np.zeros(len(Xtrain_))\n","    test_preds = np.zeros(len(X_test))\n","    seeds = [42, 123, 456]  # Reduced for speed\n","    \n","    for seed in seeds:\n","        torch.manual_seed(seed)\n","        np.random.seed(seed)\n","        \n","        for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_scaled, ytrain_scaled)):\n","            X_tr, X_dev = Xtrain_scaled[train_idx], Xtrain_scaled[dev_idx]\n","            y_tr, y_dev = ytrain_scaled[train_idx], ytrain_scaled[dev_idx]\n","            \n","            train_dataset = PolymerDataset(pd.DataFrame(X_tr), pd.Series(y_tr))\n","            dev_dataset = PolymerDataset(pd.DataFrame(X_dev), pd.Series(y_dev))\n","            test_dataset = PolymerDataset(pd.DataFrame(Xtest_scaled))\n","            \n","            train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n","            dev_loader = DataLoader(dev_dataset, batch_size=best_params['batch_size'])\n","            test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n","            \n","            model = AttentionResidualMLP(input_dim=X_tr.shape[1], hidden_dim=best_params['hidden_dim'], dropout=best_params['dropout']).to(CFG.device)\n","            criterion = lambda x, y: weighted_mae_loss(x, y, torch.tensor(target_weights[target], device=CFG.device))\n","            optimizer = optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n","            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n","            scaler = GradScaler()\n","            \n","            best_loss = float('inf')\n","            patience = 5\n","            counter = 0\n","            for epoch in range(50):\n","                model.train()\n","                for X_batch, y_batch in train_loader:\n","                    X_batch, y_batch = X_batch.to(CFG.device), y_batch.to(CFG.device)\n","                    optimizer.zero_grad()\n","                    with autocast():\n","                        outputs = model(X_batch)\n","                        loss = criterion(outputs, y_batch)\n","                    scaler.scale(loss).backward()\n","                    scaler.unscale_(optimizer)\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                    scaler.step(optimizer)\n","                    scaler.update()\n","                \n","                model.eval()\n","                val_loss = 0\n","                with torch.no_grad():\n","                    for X_batch, y_batch in dev_loader:\n","                        X_batch, y_batch = X_batch.to(CFG.device), y_batch.to(CFG.device)\n","                        outputs = model(X_batch)\n","                        val_loss += criterion(outputs, y_batch).item() * len(y_batch)\n","                val_loss /= len(dev_dataset)\n","                scheduler.step()\n","                \n","                if val_loss < best_loss:\n","                    best_loss = val_loss\n","                    counter = 0\n","                    best_model = model.state_dict()\n","                else:\n","                    counter += 1\n","                    if counter >= patience:\n","                        break\n","            \n","            model.load_state_dict(best_model)\n","            model.eval()\n","            with torch.no_grad():\n","                dev_preds = []\n","                for X_batch, _ in dev_loader:\n","                    X_batch = X_batch.to(CFG.device)\n","                    dev_preds.extend(model(X_batch).squeeze().cpu().numpy())\n","                oof_preds[dev_idx] += target_scalers[target].inverse_transform(np.array(dev_preds).reshape(-1, 1)).flatten() / (CFG.n_splits * len(seeds))\n","                \n","                test_preds_fold = []\n","                for X_batch in test_loader:\n","                    X_batch = X_batch.to(CFG.device)\n","                    test_preds_fold.extend(model(X_batch).squeeze().cpu().numpy())\n","                test_preds += target_scalers[target].inverse_transform(np.array(test_preds_fold).reshape(-1, 1)).flatten() / (CFG.n_splits * len(seeds))\n","    \n","    nn_oof_df.loc[Xtrain_.index, target] = oof_preds\n","    nn_predictions_df[target] = test_preds\n","    \n","    score = mean_absolute_error(ytrain_, oof_preds)\n","    print(f\"  Neural Network CV score for {target}: {score:.8f}\")\n","\n","nn_predictions_df.to_csv('submission_nn.csv', index=False)\n","nn_oof_df.to_csv('oof_nn.csv')\n","print(\"--- SCRIPT 2 COMPLETE: Neural Network predictions and OOF file saved. ---\")\n","clean_memory()\n","\n","# === SCRIPT 3: OPTIMAL BLENDING ===\n","print(\"\\n--- RUNNING SCRIPT 3: OPTIMAL BLENDING ---\")\n","\n","nn_test_preds = pd.read_csv('submission_nn.csv')\n","gbdt_test_preds = pd.read_csv('submission_gbdt.csv')\n","nn_oof_df = pd.read_csv('oof_nn.csv', index_col=0)\n","gbdt_oof_df = pd.read_csv('oof_gbdt.csv', index_col=0)\n","\n","final_submission = pd.DataFrame({'id': nn_test_preds['id']})\n","best_weights = {}\n","\n","print(\"Finding optimal blend weights...\")\n","for target in CFG.targets:\n","    oof_df = pd.concat([train_df[target], nn_oof_df[target], gbdt_oof_df[target]], axis=1)\n","    oof_df.columns = ['true', 'nn', 'gbdt']\n","    oof_df.dropna(inplace=True)\n","    \n","    best_rmse = float('inf')\n","    best_w = 0.5\n","    for w in np.arange(0.0, 1.01, 0.01):\n","        blend_preds = w * oof_df['nn'] + (1 - w) * oof_df['gbdt']\n","        rmse = np.sqrt(mean_squared_error(oof_df['true'], blend_preds))\n","        if rmse < best_rmse:\n","            best_rmse = rmse\n","            best_w = w\n","    \n","    best_weights[target] = best_w\n","    print(f\"  Best weight for {target}: {best_w:.2f} (Local RMSE: {best_rmse:.5f})\")\n","\n","print(\"\\nBlending test predictions...\")\n","for col in nn_test_preds.columns:\n","    if col != 'id':\n","        w = best_weights.get(col, 0.5)\n","        final_submission[col] = (w * nn_test_preds[col]) + ((1 - w) * gbdt_test_preds[col])\n","\n","final_submission = final_submission[sample_df.columns]\n","final_submission.to_csv('submission.csv', index=False)\n","print(\"\\nFinal submission saved as submission.csv\")\n","print(\"Preview:\")\n","print(final_submission.head())\n"]},{"cell_type":"markdown","id":"d0f923be","metadata":{"papermill":{"duration":0.025262,"end_time":"2025-06-20T15:47:20.528172","exception":false,"start_time":"2025-06-20T15:47:20.50291","status":"completed"},"tags":[]},"source":["## Approach 5: Feature Selection with Recursive Feature Elimination\n","To reduce noise and overfitting, we apply Recursive Feature Elimination (RFE) with XGBoost to select the top 50 features for each target. This approach trains an XGBoost model on the selected features, improving efficiency and potentially accuracy by focusing on the most relevant descriptors."]},{"cell_type":"markdown","id":"994d53d5","metadata":{"papermill":{"duration":0.025184,"end_time":"2025-06-20T15:47:20.578616","exception":false,"start_time":"2025-06-20T15:47:20.553432","status":"completed"},"tags":[]},"source":["%%time\n","\n","from sklearn.feature_selection import RFE\n","\n","OOF_Preds = []\n","Test_Preds = []\n","\n","for target in tqdm(CFG.targets):\n","    print(f\"\\n=== Training with Feature Selection for {target} ===\\n\")\n","    \n","    # Prepare data\n","    Xtrain_ = Xtrain.copy()\n","    Xtrain_[target] = Ytrain[target]\n","    Xtrain_ = Xtrain_.dropna(subset=[target])\n","    idx = Xtrain_.index\n","    Xtrain_.index = range(len(Xtrain_))\n","    ytrain_ = Xtrain_[target]\n","    \n","    # Feature selection\n","    X_tr = Xtrain_.drop(drop_cols, axis=1)\n","    model = XGBRegressor(objective='reg:absoluteerror', random_state=CFG.random_state, n_estimators=100)\n","    rfe = RFE(estimator=model, n_features_to_select=50)\n","    rfe.fit(X_tr, ytrain_)\n","    selected_cols = X_tr.columns[rfe.support_].tolist()\n","    \n","    # Cross-validation setup\n","    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n","    oof_preds = np.zeros(len(Xtrain_))\n","    test_preds = np.zeros(len(Xtest))\n","    \n","    # Train model on selected features\n","    params = {\n","        'objective': 'reg:absoluteerror',\n","        'n_estimators': 1000 if target == 'FFV' else 300,\n","        'learning_rate': 0.02,\n","        'max_depth': 6 if target == 'FFV' else 4,\n","        'colsample_bytree': 0.3,\n","        'random_state': CFG.random_state,\n","        'verbosity': 0\n","    }\n","    model = XGBRegressor(**params)\n","    \n","    for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_, ytrain_)):\n","        X_tr_fold = Xtrain_.iloc[train_idx][selected_cols]\n","        X_dev_fold = Xtrain_.iloc[dev_idx][selected_cols]\n","        y_tr, y_dev = ytrain_.iloc[train_idx], ytrain_.iloc[dev_idx]\n","        \n","        model.fit(X_tr_fold, y_tr)\n","        oof_preds[dev_idx] = model.predict(X_dev_fold)\n","        test_preds += model.predict(Xtest[selected_cols]) / CFG.n_splits\n","    \n","    # Store predictions\n","    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n","    Test_Preds.append(pd.DataFrame({target: test_preds}))\n","    \n","    # Compute score\n","    score = mean_absolute_error(ytrain_, oof_preds)\n","    print(f\"RFE CV score for {target}: {score:.8f}\")\n","    \n","    clean_memory()\n","\n","# Combine predictions\n","rfe_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\n","rfe_test_preds = pd.concat(Test_Preds, axis=1)"]},{"cell_type":"markdown","id":"5ee64285","metadata":{"papermill":{"duration":0.025097,"end_time":"2025-06-20T15:47:20.629221","exception":false,"start_time":"2025-06-20T15:47:20.604124","status":"completed"},"tags":[]},"source":["## Final Ensemble and Submission\n","The final predictions are obtained by averaging the OOF and test predictions from all approaches (Gradient Boosting, Neural Network, GNN, Stacking, and RFE). This ensemble leverages the strengths of each method to maximize accuracy. The wMAE score is computed on the OOF predictions, and the test predictions are formatted for submission."]},{"cell_type":"markdown","id":"e428a846","metadata":{"papermill":{"duration":0.025038,"end_time":"2025-06-20T15:47:20.679442","exception":false,"start_time":"2025-06-20T15:47:20.654404","status":"completed"},"tags":[]},"source":["%%time\n","\n","# Combine all predictions\n","all_oof_preds = [\n","    gb_oof_preds.fillna(0),\n","    nn_oof_preds.fillna(0),\n","    gnn_oof_preds.fillna(0),\n","    stack_oof_preds.fillna(0),\n","    rfe_oof_preds.fillna(0)\n","]\n","all_test_preds = [\n","    gb_test_preds.fillna(0),\n","    nn_test_preds.fillna(0),\n","    gnn_test_preds.fillna(0),\n","    stack_test_preds.fillna(0),\n","    rfe_test_preds.fillna(0)\n","]\n","\n","final_oof_preds = pd.DataFrame(0, index=gb_oof_preds.index, columns=CFG.targets)\n","final_test_preds = pd.DataFrame(0, index=gb_test_preds.index, columns=CFG.targets)\n","\n","for oof, test in zip(all_oof_preds, all_test_preds):\n","    for target in CFG.targets:\n","        final_oof_preds[target] += oof[target] / len(all_oof_preds)\n","        final_test_preds[target] += test[target] / len(all_test_preds)\n","\n","# Compute final CV score\n","score = score_metric(Ytrain, final_oof_preds)\n","print(f\"\\nFinal Ensemble CV score: {score:.8f}\\n\")\n","\n","# Prepare submission\n","sub_fl = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\n","final_test_preds[\"id\"] = sub_fl[\"id\"]\n","final_test_preds[[\"id\"] + CFG.targets].to_csv(\"submission.csv\", index=False)\n","\n","# Display first few rows\n","!head submission.csv"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":12609125,"sourceId":74608,"sourceType":"competition"},{"sourceId":246274448,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"papermill":{"default_parameters":{},"duration":2656.200249,"end_time":"2025-06-20T15:47:23.157803","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-20T15:03:06.957554","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}