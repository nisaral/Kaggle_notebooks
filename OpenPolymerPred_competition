{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12609125,"sourceType":"competition"},{"sourceId":12206965,"sourceType":"datasetVersion","datasetId":7689713},{"sourceId":12189904,"sourceType":"datasetVersion","datasetId":7678100},{"sourceId":246274448,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/keyushnisar/openpolymerpred?scriptVersionId=247298310\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Open Polymer Prediction 2025: Enhanced Baseline Notebook\n\n<a href=\"https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025\" target=\"_blank\">\n  <img src=\"https://img.shields.io/badge/Kaggle-Competition-blue?style=for-the-badge&logo=kaggle\" alt=\"Kaggle Competition\">\n</a>\n\nThis notebook addresses the *NeurIPS - Open Polymer Prediction 2025* competition, which involves predicting five key polymer properties—glass transition temperature (Tg), fractional free volume (FFV), thermal conductivity (Tc), density, and radius of gyration (Rg)—from SMILES representations of polymer structures. The evaluation metric is a weighted Mean Absolute Error (wMAE), designed to balance the contribution of each property by accounting for their scale and data availability.\n\nThis enhanced baseline improves the original notebook by incorporating advanced feature engineering, a broader ensemble of machine learning models, and hyperparameter optimization. The notebook is structured for clarity, reproducibility, and scalability to handle the competition's hidden test set of approximately 1,500 polymers.\nIn this script, we'll:\n1. Find similar images to pair them up efficiently\n2. Detect key points (like corners or distinctive features) in each image\n3. Match these points between image pairs\n4. Verify matches to ensure accuracy\n5. Build a 3D model using COLMAP\n6. Create a submission file with camera poses\n\n\n\n**Approach Overview**\nThe approach consists of the following components:\n\n* Data Preprocessing: Load and clean the train and test datasets, handling missing values and outliers to ensure robust data quality.\n* Feature Engineering: Use RDKit to compute molecular descriptors and introduce custom polymer-specific features to capture structural properties.\n* Model Selection: Train an ensemble of gradient boosting models (XGBoost, LightGBM, CatBoost) and a simple neural network for each target property.\n* Hyperparameter Tuning: Use Optuna to optimize model hyperparameters, improving predictive performance.\n* Cross-Validation: Implement K-Fold cross-validation to ensure reliable model evaluation and generalization.\n* Ensemble Predictions: Combine predictions from multiple models to enhance accuracy and robustness.\n* Submission Preparation: Generate predictions for the test set and format them according to the competition requirements.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Configuration\nThe configuration class defines key parameters for data preprocessing, model training, and cross-validation. Key settings include:\n\n* Target Variables: The five properties to predict: Tg, FFV, Tc, Density, and Rg.\n* Cross-Validation: 5-fold K-Fold cross-validation for robust model evaluation.\n* Model Parameters: Settings for GPU usage, early stopping, and random state for reproducibility.\n* Feature Engineering: Enable feature importance analysis and preprocessing steps.\n* Evaluation Metric: Minimize the weighted Mean Absolute Error (wMAE).","metadata":{}},{"cell_type":"markdown","source":"%%time\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors\nfrom tqdm import tqdm\nimport gc\n\nclass CFG:\n    \"\"\"\n    Simple configuration class for paths and settings.\n    \"\"\"\n    data_path = \"/kaggle/input/neurips-open-polymer-prediction-2025\"\n    output_path = \"/kaggle/working\"\n    targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n    n_splits = 5\n    random_state = 42\n\n# Utility function for memory cleanup\ndef clean_memory():\n    gc.collect()\n\n# Utility function for wMAE metric\ndef score_metric(y_true, y_pred):\n    \"\"\"\n    Compute weighted Mean Absolute Error (wMAE) across targets.\n    \"\"\"\n    weights = {'Tg': 0.2, 'FFV': 0.2, 'Tc': 0.2, 'Density': 0.2, 'Rg': 0.2}  # Simplified equal weights\n    mae = 0\n    for col in y_true.columns:\n        valid_idx = y_true[col].notna()\n        if valid_idx.sum() > 0:\n            mae += weights[col] * mean_absolute_error(y_true[col][valid_idx], y_pred[col][valid_idx])\n    return mae\n\nprint(\"Setup complete\")","metadata":{"execution":{"iopub.status.busy":"2025-06-20T07:49:59.617411Z","iopub.execute_input":"2025-06-20T07:49:59.617573Z","iopub.status.idle":"2025-06-20T07:50:02.808543Z","shell.execute_reply.started":"2025-06-20T07:49:59.617558Z","shell.execute_reply":"2025-06-20T07:50:02.807851Z"}}},{"cell_type":"markdown","source":"## Feature Engineering\nFeature engineering is critical for capturing the chemical and structural properties of polymers. The FeatureMaker class uses RDKit to compute molecular descriptors from SMILES strings, such as molecular weight, topological polar surface area, and other chemical properties. Additional custom features are introduced to capture polymer-specific characteristics, including:\n\n* Chain Length: Number of repeating units inferred from SMILES.\n* Functional Group Counts: Counts of specific chemical groups (e.g., aromatic rings, hydroxyl groups).\n* Molecular Complexity: Metrics like the BertzCT index for structural complexity.\n  \nThe Ipc descriptor is log-transformed to handle its large range, and infinity values are replaced with NaN. Columns with near-zero variance or excessive missing values (>99.75%) are dropped to reduce noise.","metadata":{}},{"cell_type":"markdown","source":"%%writefile myfe.py\n\nimport pandas as pd\nimport numpy as np\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors, AllChem\n\nclass FeatureMaker:\n    \"\"\"\n    Generate RDKit descriptors and Morgan fingerprints from SMILES strings.\n    \"\"\"\n    def __init__(self):\n        self.feature_names = None\n\n    def _compute_descriptors(self, smiles):\n        \"\"\"\n        Compute RDKit descriptors and Morgan fingerprints for a SMILES string.\n        \"\"\"\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            return [None] * (len(Descriptors.descList) + 256)  # 256 for Morgan\n        desc_values = [desc[1](mol) for desc in Descriptors.descList]\n        # Compute Morgan fingerprint (ECFP4, radius=2, 256 bits)\n        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=256)\n        fp_values = list(fp)\n        return desc_values + fp_values\n\n    def fit_transform(self, X):\n        \"\"\"\n        Fit and transform the dataset to create features.\n        \"\"\"\n        desc_names = [desc[0] for desc in Descriptors.descList]\n        fp_names = [f'ECFP_{i}' for i in range(256)]\n        descriptors = [self._compute_descriptors(smi) for smi in X['SMILES']]\n        df = pd.DataFrame(descriptors, columns=desc_names + fp_names)\n        df = pd.concat([X, df], axis=1)\n        df[\"Ipc\"] = np.log1p(df[\"Ipc\"])  # Log-transform Ipc\n        self.feature_names = list(df.columns)\n        return df.replace([np.inf, -np.inf], np.nan)\n\n    def transform(self, X):\n        \"\"\"\n        Transform the dataset using the same features.\n        \"\"\"\n        desc_names = [desc[0] for desc in Descriptors.descList]\n        fp_names = [f'ECFP_{i}' for i in range(256)]\n        descriptors = [self._compute_descriptors(smi) for smi in X['SMILES']]\n        df = pd.DataFrame(descriptors, columns=desc_names + fp_names)\n        df = pd.concat([X, df], axis=1)\n        df[\"Ipc\"] = np.log1p(df[\"Ipc\"])\n        return df.replace([np.inf, -np.inf], np.nan)\n\n    def get_feature_names(self):\n        \"\"\"\n        Return feature names.\n        \"\"\"\n        return self.feature_names","metadata":{"execution":{"iopub.status.busy":"2025-06-20T07:50:21.63078Z","iopub.execute_input":"2025-06-20T07:50:21.631486Z","iopub.status.idle":"2025-06-20T07:50:21.637203Z","shell.execute_reply.started":"2025-06-20T07:50:21.631461Z","shell.execute_reply":"2025-06-20T07:50:21.63647Z"}}},{"cell_type":"markdown","source":"## Model Training Strategy\n\nThe model training strategy employs an ensemble of gradient boosting models (XGBoost, LightGBM, CatBoost) and a neural network implemented with PyTorch for each target property. Key aspects include:\n\n* FFV (Fractional Free Volume): As the most populated target (~90% fill rate), it uses a larger ensemble with tuned hyperparameters.\n* Other Targets (Tg, Tc, Density, Rg): These have higher missing rates, so simpler models with fewer iterations are used to avoid overfitting.\n* Hyperparameter Tuning: Optuna is used to optimize key parameters (e.g., learning rate, max depth) for each model.\n* Cross-Validation: 5-fold K-Fold cross-validation ensures robust evaluation.\nEnsemble: Predictions are averaged across models to improve robustness.\n\nThe weighted MAE metric is computed for each target, and the final score aggregates these errors with weights based on data availability and property ranges.","metadata":{}},{"cell_type":"markdown","source":"## Approach 1: Gradient Boosting Ensemble\n\nThis approach trains an ensemble of three gradient boosting models—XGBoost, LightGBM, and CatBoost—for each target property. These models are robust to missing data and handle non-linear relationships well. We use Optuna for hyperparameter tuning to optimize parameters like learning rate, max depth, and subsampling rates. FFV, with the highest data availability, uses larger models, while other targets use smaller models to prevent overfitting.","metadata":{}},{"cell_type":"markdown","source":"%%time\n\n# Install required packages\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom tqdm import tqdm\nimport optuna\nimport gc\nfrom myfe import FeatureMaker\n\n# Define configuration\nclass CFG:\n    data_path = \"/kaggle/input/neurips-open-polymer-prediction-2025/\"  # Kaggle input path\n    random_state = 42\n    n_splits = 3\n    targets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n\n# Memory cleanup function\ndef clean_memory():\n    gc.collect()\n\n# Data loading and preprocessing\nprint(\"Loading data...\")\ntrain_df = pd.read_csv(f\"{CFG.data_path}/train.csv\")\ntest_df = pd.read_csv(f\"{CFG.data_path}/test.csv\")\n\n# Initialize feature maker\nfeaturizer = FeatureMaker()\n\n# Generate features\nprint(\"Generating features...\")\nXtrain = featurizer.fit_transform(train_df)\nXtest = featurizer.transform(test_df)\n\n# Separate targets\nYtrain = Xtrain[CFG.targets].copy()\nXtrain = Xtrain.drop(CFG.targets, axis=1, errors='ignore')\n\n# Ensure test set has 'id' column\nif 'id' not in Xtest.columns:\n    Xtest['id'] = test_df['id']\n\n# Optuna objective function\ndef objective(trial, X, y, model_type, cv):\n    \"\"\"\n    Objective function for Optuna hyperparameter tuning.\n    \"\"\"\n    if model_type == 'XGB':\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n            'max_depth': trial.suggest_int('max_depth', 3, 8),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.8),\n            'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0),\n            'objective': 'reg:absoluteerror',\n            'random_state': CFG.random_state,\n            'early_stopping_rounds': 50,\n            'tree_method': 'hist',\n            'verbosity': 0\n        }\n        model = XGBRegressor(**params)\n    \n    elif model_type == 'LGBM':\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n            'max_depth': trial.suggest_int('max_depth', 3, 8),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.8),\n            'num_leaves': trial.suggest_int('num_leaves', 20, 50),\n            'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n            'objective': 'regression_l1',\n            'random_state': CFG.random_state,\n            'early_stopping_rounds': 50,\n            'verbosity': -1\n        }\n        model = LGBMRegressor(**params)\n    \n    elif model_type == 'CB':\n        params = {\n            'iterations': trial.suggest_int('iterations', 100, 1000),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n            'depth': trial.suggest_int('depth', 3, 8),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 10.0),\n            'loss_function': 'MAE',\n            'eval_metric': 'MAE',\n            'random_state': CFG.random_state,\n            'early_stopping_rounds': 50,\n            'thread_count': -1,\n            'verbose': 0\n        }\n        model = CatBoostRegressor(**params)\n    \n    scores = []\n    for train_idx, dev_idx in cv.split(X, y):\n        X_tr, X_dev = X.iloc[train_idx], X.iloc[dev_idx]\n        y_tr, y_dev = y.iloc[train_idx], y.iloc[dev_idx]\n        if model_type in ['XGB', 'LGBM']:\n            model.fit(X_tr, y_tr, eval_set=[(X_dev, y_dev)], verbose_eval=False)\n        else:\n            model.fit(X_tr, y_tr, eval_set=(X_dev, y_dev), verbose=False)\n        preds = model.predict(X_dev)\n        scores.append(mean_absolute_error(y_dev, preds))\n    \n    return np.mean(scores)\n\ndef tune_model(X, y, model_type, cv, n_trials=5):\n    \"\"\"\n    Tune hyperparameters using Optuna.\n    \"\"\"\n    study = optuna.create_study(direction='minimize')\n    study.optimize(lambda trial: objective(trial, X, y, model_type, cv), n_trials=n_trials)\n    return study.best_params\n\n# Gradient Boosting Ensemble\nOOF_Preds = []\nTest_Preds = []\ndrop_cols = [\"Source\", \"id\", \"Id\", \"SMILES\"] + CFG.targets\n\nfor target in tqdm(CFG.targets):\n    print(f\"\\n=== Training Gradient Boosting Ensemble for {target} ===\\n\")\n    \n    # Prepare data\n    Xtrain_ = Xtrain.copy()\n    Xtrain_[target] = Ytrain[target]\n    Xtrain_ = Xtrain_.dropna(subset=[target])\n    idx = Xtrain_.index\n    Xtrain_.index = range(len(Xtrain_))\n    ytrain_ = Xtrain_[target]\n    \n    # Filter drop_cols\n    existing_drop_cols = [col for col in drop_cols if col in Xtrain_.columns and col != target]\n    X_features = Xtrain_.drop(existing_drop_cols + [target], axis=1, errors='ignore')\n    \n    # Cross-validation setup\n    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n    \n    # Tune hyperparameters\n    tuned_params = {}\n    for model_type in ['XGB', 'LGBM', 'CB']:\n        tuned_params[model_type] = tune_model(X_features, ytrain_, model_type, cv, n_trials=5)\n    \n    # Model definitions\n    models = {\n        'XGB': XGBRegressor(**tuned_params['XGB'], objective='reg:absoluteerror', random_state=CFG.random_state, early_stopping_rounds=50, tree_method='hist', verbosity=0),\n        'LGBM': LGBMRegressor(**tuned_params['LGBM'], objective='regression_l1', random_state=CFG.random_state, early_stopping_rounds=50, verbosity=-1),\n        'CB': CatBoostRegressor(**tuned_params['CB'], loss_function='MAE', eval_metric='MAE', random_state=CFG.random_state, early_stopping_rounds=50, thread_count=-1, verbose=0)\n    }\n    \n    oof_preds = np.zeros(len(Xtrain_))\n    test_preds = np.zeros(len(Xtest))\n    \n    # Train across folds\n    for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_, ytrain_)):\n        X_tr = Xtrain_.iloc[train_idx].drop(existing_drop_cols + [target], axis=1, errors='ignore')\n        X_dev = Xtrain_.iloc[dev_idx].drop(existing_drop_cols + [target], axis=1, errors='ignore')\n        y_tr, y_dev = ytrain_.iloc[train_idx], ytrain_.iloc[dev_idx]\n        \n        fold_preds = np.zeros(len(X_dev))\n        fold_test_preds = np.zeros(len(Xtest))\n        \n        for name, model in models.items():\n            if name in ['XGB', 'LGBM']:\n                model.fit(X_tr, y_tr, eval_set=[(X_dev, y_dev)], verbose_eval=False)\n            else:\n                model.fit(X_tr, y_tr, eval_set=(X_dev, y_dev), verbose=False)\n            fold_preds += model.predict(X_dev) / len(models)\n            fold_test_preds += model.predict(Xtest.drop([col for col in existing_drop_cols if col in Xtest.columns], axis=1, errors='ignore')) / len(models)\n        \n        oof_preds[dev_idx] = fold_preds\n        test_preds += fold_test_preds / CFG.n_splits\n    \n    # Store predictions\n    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n    Test_Preds.append(pd.DataFrame({target: test_preds}))\n    \n    # Compute score\n    score = mean_absolute_error(ytrain_, oof_preds)\n    print(f\"Gradient Boosting CV score for {target}: {score:.8f}\")\n    \n    clean_memory()\n\n# Combine predictions\ngb_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\ngb_test_preds = pd.concat(Test_Preds, axis=1)\n\n# Submission\nsub_fl = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\ngb_test_preds[\"id\"] = sub_fl[\"id\"]\ngb_test_preds[[\"id\"] + CFG.targets].to_csv(\"submission.csv\", index=False)\nprint(\"\\nGradient Boosting submission saved as submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-06-20T07:36:21.346501Z","iopub.execute_input":"2025-06-20T07:36:21.347376Z","iopub.status.idle":"2025-06-20T07:39:17.031167Z","shell.execute_reply.started":"2025-06-20T07:36:21.347344Z","shell.execute_reply":"2025-06-20T07:39:17.030204Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}}},{"cell_type":"markdown","source":"%%time\n\nOOF_Preds = []\nTest_Preds = []\nArtefacts = {}\ndrop_cols = [\"Source\", \"id\", \"Id\", \"SMILES\"] + CFG.targets\n\nfor target in tqdm(CFG.targets):\n    print(f\"\\n=== Training Gradient Boosting Ensemble for {target} ===\\n\")\n    \n    # Prepare data\n    Xtrain_ = Xtrain.copy()\n    Xtrain_[target] = Ytrain[target]\n    Xtrain_ = Xtrain_.dropna(subset=[target])\n    idx = Xtrain_.index\n    Xtrain_.index = range(len(Xtrain_))\n    ytrain_ = Xtrain_[target]\n    \n    # Filter drop_cols to include only existing columns, excluding the current target\n    existing_drop_cols = [col for col in drop_cols if col in Xtrain_.columns and col != target]\n    \n    # Features for hyperparameter tuning (exclude target)\n    X_features = Xtrain_.drop(existing_drop_cols + [target], axis=1, errors='ignore')\n    \n    # Cross-validation setup\n    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n    \n    # Tune hyperparameters\n    tuned_params = {}\n    for model_type in ['XGB', 'LGBM', 'CB']:\n        tuned_params[model_type] = tune_model(X_features, ytrain_, model_type, cv, n_trials=10)\n    \n    # Model definitions\n    models = {\n        'XGB': XGBRegressor(**tuned_params['XGB'], objective='reg:absoluteerror', random_state=CFG.random_state, verbosity=0),\n        'LGBM': LGBMRegressor(**tuned_params['LGBM'], objective='regression_l1', random_state=CFG.random_state, verbosity=-1),\n        'CB': CatBoostRegressor(**tuned_params['CB'], loss_function='MAE', eval_metric='MAE', random_state=CFG.random_state, verbose=0)\n    }\n    \n    oof_preds = np.zeros(len(Xtrain_))\n    test_preds = np.zeros(len(Xtest))\n    model_oof = {name: np.zeros(len(Xtrain_)) for name in models}\n    \n    # Train across folds\n    for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_, ytrain_)):\n        X_tr = Xtrain_.iloc[train_idx].drop(existing_drop_cols + [target], axis=1, errors='ignore')\n        X_dev = Xtrain_.iloc[dev_idx].drop(existing_drop_cols + [target], axis=1, errors='ignore')\n        y_tr, y_dev = ytrain_.iloc[train_idx], ytrain_.iloc[dev_idx]\n        \n        fold_preds = np.zeros(len(X_dev))\n        fold_test_preds = np.zeros(len(Xtest))\n        \n        for name, model in models.items():\n            model.fit(X_tr, y_tr)\n            model_oof[name][dev_idx] = model.predict(X_dev)\n            fold_preds += model.predict(X_dev) / len(models)\n            fold_test_preds += model.predict(Xtest.drop([col for col in existing_drop_cols if col in Xtest.columns], axis=1, errors='ignore')) / len(models)\n            Artefacts[f\"{target}_{name}_fold{fold}\"] = model\n        \n        oof_preds[dev_idx] = fold_preds\n        test_preds += fold_test_preds / CFG.n_splits\n    \n    # Store predictions\n    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n    Test_Preds.append(pd.DataFrame({target: test_preds}))\n    \n    # Compute score\n    score = mean_absolute_error(ytrain_, oof_preds)\n    print(f\"Ensemble CV score for {target}: {score:.8f}\")\n    \n    clean_memory()\n\n# Combine predictions\ngb_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\ngb_test_preds = pd.concat(Test_Preds, axis=1)","metadata":{"execution":{"iopub.status.busy":"2025-06-20T07:39:58.08172Z","iopub.execute_input":"2025-06-20T07:39:58.082637Z","iopub.status.idle":"2025-06-20T07:39:58.155101Z","shell.execute_reply.started":"2025-06-20T07:39:58.082598Z","shell.execute_reply":"2025-06-20T07:39:58.154203Z"}}},{"cell_type":"markdown","source":"## Approach 2: Neural Network\n\nA multilayer perceptron (MLP) implemented in PyTorch is used to capture complex non-linear relationships in the molecular descriptors. The MLP is trained with a mean absolute error (MAE) loss function, matching the competition’s metric. Early stopping and learning rate scheduling prevent overfitting. This approach is applied to all targets, with larger architectures for FFV due to its higher data availability.","metadata":{}},{"cell_type":"markdown","source":"%%time\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.feature_selection import VarianceThreshold\nfrom tqdm import tqdm\nimport optuna\nimport gc\nfrom torch.cuda.amp import GradScaler, autocast\nfrom myfe import FeatureMaker\n\n# Define configuration\nclass CFG:\n    data_path = \"/kaggle/input/neurips-open-polymer-prediction-2025/\" \n    random_state = 42\n    n_splits = 5  \n    targets = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Memory cleanup function\ndef clean_memory():\n    gc.collect()\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\n# Data loading and preprocessing\nprint(\"Loading data...\")\ntrain_df = pd.read_csv(f\"{CFG.data_path}/train.csv\")\ntest_df = pd.read_csv(f\"{CFG.data_path}/test.csv\")\n\n# Initialize feature maker\nfeaturizer = FeatureMaker()\n\n# Generate features\nprint(\"Generating features...\")\nXtrain = featurizer.fit_transform(train_df)\nXtest = featurizer.transform(test_df)\n\n# Separate targets\nYtrain = Xtrain[CFG.targets].copy()\nXtrain = Xtrain.drop(CFG.targets, axis=1, errors='ignore')\n\n# Ensure test set has 'id' column\nif 'id' not in Xtest.columns:\n    Xtest['id'] = test_df['id']\n\nclass PolymerDataset(Dataset):\n    \"\"\"\n    Custom dataset for polymer data with NaN handling.\n    \"\"\"\n    def __init__(self, X, y=None):\n        self.X = torch.tensor(X.values, dtype=torch.float32).nan_to_num(0)\n        self.y = torch.tensor(y.values, dtype=torch.float32) if y is not None else None\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.X[idx], self.y[idx]\n        return self.X[idx]\n\nclass AttentionResidualMLP(nn.Module):\n    \"\"\"\n    Enhanced MLP with attention mechanism, residual connections, and layer normalization.\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, dropout):\n        super(AttentionResidualMLP, self).__init__()\n        self.input_layer = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        # Attention mechanism\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Softmax(dim=-1)\n        )\n        self.residual_block1 = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.residual_block2 = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.LayerNorm(hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.output_layer = nn.Linear(hidden_dim // 2, 1)\n        \n    def forward(self, x):\n        x1 = self.input_layer(x)\n        # Apply attention\n        attn_weights = self.attention(x1)\n        x1 = x1 * attn_weights\n        x2 = self.residual_block1(x1) + x1  # Residual connection\n        x3 = self.residual_block2(x2)\n        return self.output_layer(x3)\n\ndef weighted_mae_loss(outputs, targets, weights):\n    \"\"\"\n    Custom MAE loss with target-specific weights.\n    \"\"\"\n    return torch.mean(weights * torch.abs(outputs.squeeze() - targets))\n\ndef objective(trial, X, y, cv, target):\n    \"\"\"\n    Objective function for Optuna hyperparameter tuning.\n    \"\"\"\n    params = {\n        'hidden_dim': trial.suggest_int('hidden_dim', 128, 512, step=64),  # Wider range\n        'dropout': trial.suggest_float('dropout', 0.1, 0.4),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n    }\n    \n    scores = []\n    for train_idx, dev_idx in cv.split(X, y):\n        X_tr, X_dev = X[train_idx], X[dev_idx]\n        y_tr, y_dev = y[train_idx], y[dev_idx]\n        \n        train_dataset = PolymerDataset(pd.DataFrame(X_tr), pd.Series(y_tr))\n        dev_dataset = PolymerDataset(pd.DataFrame(X_dev), pd.Series(y_dev))\n        \n        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n        dev_loader = DataLoader(dev_dataset, batch_size=params['batch_size'])\n        \n        model = AttentionResidualMLP(input_dim=X_tr.shape[1], hidden_dim=params['hidden_dim'], dropout=params['dropout']).to(device)\n        criterion = nn.L1Loss()\n        optimizer = optim.AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)  # Longer cycle\n        scaler = GradScaler()\n        \n        best_loss = float('inf')\n        patience = 5  # Increased patience\n        counter = 0\n        for epoch in range(50):  # More epochs\n            model.train()\n            for X_batch, y_batch in train_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                optimizer.zero_grad()\n                with autocast():\n                    outputs = model(X_batch)\n                    loss = criterion(outputs.squeeze(), y_batch)\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n                scaler.step(optimizer)\n                scaler.update()\n            \n            model.eval()\n            val_loss = 0\n            with torch.no_grad():\n                for X_batch, y_batch in dev_loader:\n                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                    outputs = model(X_batch)\n                    val_loss += criterion(outputs.squeeze(), y_batch).item() * len(y_batch)\n            val_loss /= len(dev_dataset)\n            scheduler.step()\n            \n            if val_loss < best_loss:\n                best_loss = val_loss\n                counter = 0\n            else:\n                counter += 1\n                if counter >= patience:\n                    break\n        \n        model.eval()\n        preds = []\n        with torch.no_grad():\n            for X_batch, _ in dev_loader:\n                X_batch = X_batch.to(device)\n                preds.extend(model(X_batch).squeeze().cpu().numpy())\n        scores.append(mean_absolute_error(y_dev, preds))\n    \n    return np.mean(scores)\n\nOOF_Preds = []\nTest_Preds = []\ndrop_cols = [\"Source\", \"id\", \"Id\", \"SMILES\"] + CFG.targets\nseeds = [42, 123, 456, 789, 101]  # More seeds for bagging\ntarget_weights = {'Tg': 0.5, 'FFV': 2.0, 'Tc': 1.0, 'Density': 1.0, 'Rg': 0.8}  # wMAE weights\ntarget_scalers = {\n    'Tg': RobustScaler(),\n    'FFV': RobustScaler(),\n    'Tc': RobustScaler(),\n    'Density': RobustScaler(),\n    'Rg': RobustScaler()\n}  # Target-specific scaling\n\nfor target in tqdm(CFG.targets):\n    print(f\"\\n=== Training Neural Network for {target} ===\\n\")\n    \n    # Prepare data\n    Xtrain_ = Xtrain.copy()\n    Xtrain_[target] = Ytrain[target]\n    Xtrain_ = Xtrain_.dropna(subset=[target])\n    idx = Xtrain_.index\n    Xtrain_.index = range(len(Xtrain_))\n    ytrain_ = Xtrain_[target]\n    \n    # Scale target\n    ytrain_scaled = target_scalers[target].fit_transform(ytrain_.values.reshape(-1, 1)).flatten()\n    \n    # Filter drop_cols\n    existing_drop_cols = [col for col in drop_cols if col in Xtrain_.columns and col != target]\n    \n    # Feature selection\n    selector = VarianceThreshold(threshold=0.01)\n    Xtrain_features = Xtrain_.drop(existing_drop_cols + [target], axis=1, errors='ignore')\n    Xtrain_selected = selector.fit_transform(Xtrain_features.fillna(0))\n    Xtest_features = Xtest.drop([col for col in existing_drop_cols if col in Xtest.columns], axis=1, errors='ignore')\n    Xtest_selected = selector.transform(Xtest_features.fillna(0))\n    \n    # Scale features\n    scaler = RobustScaler()\n    Xtrain_scaled = scaler.fit_transform(Xtrain_selected)\n    Xtest_scaled = scaler.transform(Xtest_selected)\n    \n    # Cross-validation setup\n    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n    oof_preds = np.zeros(len(Xtrain_))\n    test_preds = np.zeros(len(Xtest))\n    \n    # Hyperparameter tuning\n    study = optuna.create_study(direction='minimize')\n    study.optimize(lambda trial: objective(trial, Xtrain_scaled, ytrain_scaled, cv, target), n_trials=10)  # More trials\n    best_params = study.best_params\n    \n    # Ensemble across seeds\n    for seed in seeds:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n        \n        for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_scaled, ytrain_scaled)):\n            X_tr, X_dev = Xtrain_scaled[train_idx], Xtrain_scaled[dev_idx]\n            y_tr, y_dev = ytrain_scaled[train_idx], ytrain_scaled[dev_idx]\n            \n            # Create datasets\n            train_dataset = PolymerDataset(pd.DataFrame(X_tr), pd.Series(y_tr))\n            dev_dataset = PolymerDataset(pd.DataFrame(X_dev), pd.Series(y_dev))\n            test_dataset = PolymerDataset(pd.DataFrame(Xtest_scaled))\n            \n            train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n            dev_loader = DataLoader(dev_dataset, batch_size=best_params['batch_size'])\n            test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'])\n            \n            # Initialize model\n            model = AttentionResidualMLP(input_dim=X_tr.shape[1], hidden_dim=best_params['hidden_dim'], dropout=best_params['dropout']).to(device)\n            criterion = lambda x, y: weighted_mae_loss(x, y, torch.tensor(target_weights[target], device=device))\n            optimizer = optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n            scaler = GradScaler()\n            \n            # Training loop\n            best_loss = float('inf')\n            patience = 5\n            counter = 0\n            for epoch in range(50):\n                model.train()\n                for X_batch, y_batch in train_loader:\n                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                    optimizer.zero_grad()\n                    with autocast():\n                        outputs = model(X_batch)\n                        loss = criterion(outputs, y_batch)\n                    scaler.scale(loss).backward()\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                \n                # Validation\n                model.eval()\n                val_loss = 0\n                with torch.no_grad():\n                    for X_batch, y_batch in dev_loader:\n                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                        outputs = model(X_batch)\n                        val_loss += criterion(outputs, y_batch).item() * len(y_batch)\n                val_loss /= len(dev_dataset)\n                scheduler.step()\n                \n                # Early stopping\n                if val_loss < best_loss:\n                    best_loss = val_loss\n                    counter = 0\n                    best_model = model.state_dict()\n                else:\n                    counter += 1\n                    if counter >= patience:\n                        break\n            \n            # Load best model\n            model.load_state_dict(best_model)\n            model.eval()\n            with torch.no_grad():\n                dev_preds = []\n                for X_batch, _ in dev_loader:\n                    X_batch = X_batch.to(device)\n                    dev_preds.extend(model(X_batch).squeeze().cpu().numpy())\n                oof_preds[dev_idx] += target_scalers[target].inverse_transform(np.array(dev_preds).reshape(-1, 1)).flatten() / (CFG.n_splits * len(seeds))\n                \n                test_preds_fold = []\n                for X_batch in test_loader:\n                    X_batch = X_batch.to(device)\n                    test_preds_fold.extend(model(X_batch).squeeze().cpu().numpy())\n                test_preds += target_scalers[target].inverse_transform(np.array(test_preds_fold).reshape(-1, 1)).flatten() / (CFG.n_splits * len(seeds))\n    \n    # Store predictions\n    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n    Test_Preds.append(pd.DataFrame({target: test_preds}))\n    \n    # Compute score\n    score = mean_absolute_error(ytrain_, oof_preds)\n    print(f\"Neural Network CV score for {target}: {score:.8f}\")\n    \n    clean_memory()\n\n# Combine predictions\nnn_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\nnn_test_preds = pd.concat(Test_Preds, axis=1)\n\n# Submission\nsub_fl = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\nnn_test_preds[\"id\"] = sub_fl[\"id\"]\nnn_test_preds[[\"id\"] + CFG.targets].to_csv(\"submission.csv\", index=False)\nprint(\"\\nNeural Network submission saved as submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-06-20T07:53:10.036265Z","iopub.execute_input":"2025-06-20T07:53:10.036897Z","iopub.status.idle":"2025-06-20T08:40:06.652168Z","shell.execute_reply.started":"2025-06-20T07:53:10.036866Z","shell.execute_reply":"2025-06-20T08:40:06.651312Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}}},{"cell_type":"markdown","source":"## Approach 3: Graph Neural Network\n\nGraph Neural Networks (GNNs) model polymers as molecular graphs, where atoms are nodes and bonds are edges. This captures spatial and structural relationships better than descriptor-based methods. We use DeepChem’s GraphConvModel, which is well-suited for molecular data. Due to computational constraints, we apply this to FFV and optionally to other targets with reduced epochs for sparse data.","metadata":{}},{"cell_type":"markdown","source":"%%time\n\nimport deepchem as dc\nfrom deepchem.models import GraphConvModel\n\nOOF_Preds = []\nTest_Preds = []\ndrop_cols = [\"Source\", \"id\", \"Id\", \"SMILES\"] + CFG.targets\n\nfor target in tqdm(['FFV']):\n    print(f\"\\n=== Training Graph Neural Network for {target} ===\\n\")\n    \n    # Prepare data\n    Xtrain_ = Xtrain.copy()\n    Xtrain_[target] = Ytrain[target]\n    Xtrain_ = Xtrain_.dropna(subset=[target])\n    idx = Xtrain_.index\n    Xtrain_.index = range(len(Xtrain_))\n    ytrain_ = Xtrain_[target]\n    \n    # Convert SMILES to molecular graphs\n    featurizer = dc.feat.ConvMolFeaturizer()\n    train_mols = [Chem.MolFromSmiles(smi) for smi in Xtrain_['SMILES']]\n    test_mols = [Chem.MolFromSmiles(smi) for smi in Xtest['SMILES']]\n    \n    train_features = featurizer.featurize(train_mols)\n    test_features = featurizer.featurize(test_mols)\n    \n    # Filter invalid molecules\n    valid_train_idx = [i for i, feat in enumerate(train_features) if isinstance(feat, dc.feat.mol_graphs.ConvMol)]\n    valid_test_idx = [i for i, feat in enumerate(test_features) if isinstance(feat, dc.feat.mol_graphs.ConvMol)]\n    \n    train_features = [train_features[i] for i in valid_train_idx]\n    ytrain_ = ytrain_.iloc[valid_train_idx]\n    idx = idx[valid_train_idx]\n    test_features = [test_features[i] for i in valid_test_idx]\n    \n    # Cross-validation setup\n    cv = KFold(n_splits=3, shuffle=True, random_state=CFG.random_state)\n    oof_preds = np.zeros(len(ytrain_))\n    test_preds = np.zeros(len(test_mols))\n    \n    for fold, (train_idx, dev_idx) in enumerate(cv.split(train_features, ytrain_)):\n        X_tr = [train_features[i] for i in train_idx]\n        X_dev = [train_features[i] for i in dev_idx]\n        y_tr = ytrain_.iloc[train_idx]\n        y_dev = ytrain_.iloc[dev_idx]\n        \n        # Create datasets\n        train_dataset = dc.data.NumpyDataset(X_tr, y_tr.values)\n        dev_dataset = dc.data.NumpyDataset(X_dev, y_dev.values)\n        test_dataset = dc.data.NumpyDataset(test_features)\n        \n        # Initialize model\n        model = GraphConvModel(n_tasks=1, mode='regression', batch_size=16, learning_rate=0.001, graph_conv_layers=[32, 32])\n        \n        # Train model\n        model.fit(train_dataset, nb_epoch=30)\n        \n        # Predict\n        oof_preds[dev_idx] = model.predict(dev_dataset).flatten()\n        test_preds[valid_test_idx] += model.predict(test_dataset).flatten() / 3\n    \n    # Store predictions\n    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n    Test_Preds.append(pd.DataFrame({target: test_preds}))\n    \n    # Compute score\n    score = mean_absolute_error(ytrain_, oof_preds)\n    print(f\"GNN CV score for {target}: {score:.8f}\")\n    \n    clean_memory()\n\n# Combine predictions (fill others with NaN)\ngnn_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\ngnn_test_preds = pd.concat(Test_Preds, axis=1)\nfor t in CFG.targets:\n    if t not in gnn_oof_preds.columns:\n        gnn_oof_preds[t] = np.nan\n        gnn_test_preds[t] = np.nan\n\n# Submission\nsub_fl = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\ngnn_test_preds[\"id\"] = sub_fl[\"id\"]\ngnn_test_preds[[\"id\"] + CFG.targets].to_csv(\"submission_gnn.csv\", index=False)\nprint(\"\\nGNN submission saved as submission_gnn.csv\")","metadata":{}},{"cell_type":"markdown","source":"## Approach 4: Stacking Ensemble\nThe stacking ensemble combines predictions from the gradient boosting models and neural network using a meta-learner (Linear Regression). This approach leverages the strengths of individual models to improve overall accuracy. First-level predictions are generated via cross-validation, and the meta-learner is trained on these predictions to produce the final output.","metadata":{}},{"cell_type":"code","source":"/kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1: Imports\n# [Unchanged]\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport optuna\nimport shap\nimport catboost\nimport torch\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors, AllChem\nfrom tqdm import tqdm\n\n# Cell 2: Load Data\n# [Unchanged]\ntrain = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')\ntest = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\nsample = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv')\n\n# Cell 3: Load and Clean Extra Data\n# ==============================================================================\nextra_tg_file_path = \"/kaggle/input/tg-smiles-pid-polymer-class/TgSS_enriched_cleaned.csv\"\nextra_tc_file_path = \"/kaggle/input/tc-smiles/Tc_SMILES.csv\"\nextra_tg_df = pd.read_csv(extra_tg_file_path)\nextra_tc_df = pd.read_csv(extra_tc_file_path)\n\n# Clean and prepare extra_tg_df\nextra_tg_clean = extra_tg_df[['SMILES', 'PID', 'Tg', 'Polymer Class']].rename(columns={'PID': 'id'})\nextra_tg_clean[['FFV', 'Tc', 'Density', 'Rg']] = float('nan')\n\n# Clean and prepare extra_tc_clean\nextra_tc_clean = extra_tc_df[['SMILES', 'TC_mean']].rename(columns={'TC_mean': 'Tc'})\nextra_tc_clean['id'] = range(len(train) + len(extra_tg_df), len(train) + len(extra_tg_df) + len(extra_tc_df))\nextra_tc_clean[['Tg', 'FFV', 'Density', 'Rg', 'Polymer Class']] = 'Unknown'  # Set Polymer Class to 'Unknown'\n\n# Reorder columns\nextra_tg_clean = extra_tg_clean[['id', 'SMILES', 'Tg', 'FFV', 'Tc', 'Density', 'Rg', 'Polymer Class']]\nextra_tc_clean = extra_tc_clean[['id', 'SMILES', 'Tg', 'FFV', 'Tc', 'Density', 'Rg', 'Polymer Class']]\n\n# Add Polymer Class to train (default to 'Unknown')\ntrain['Polymer Class'] = 'Unknown'\n\n# Tanimoto similarity for deduplication\ndef tanimoto_similarity(smiles1, smiles2):\n    mol1 = Chem.MolFromSmiles(smiles1)\n    mol2 = Chem.MolFromSmiles(smiles2)\n    if mol1 is None or mol2 is None:\n        return 0.0\n    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 2, nBits=256)\n    fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, 2, nBits=256)\n    return AllChem.DataStructs.TanimotoSimilarity(fp1, fp2)\n\ntrain_all = pd.concat([train, extra_tg_clean, extra_tc_clean], ignore_index=True)\ntrain_all = train_all.drop_duplicates(subset=['SMILES'], keep='first')\ntrain_all['Polymer Class'] = train_all['Polymer Class'].fillna('Unknown')  # Ensure no NaN in Polymer Class\ntrain_all = train_all.reset_index(drop=True)\ntrain = train_all\ntargets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\nprint(\"Target counts:\")\nprint(train[targets].count())\nprint(\"\\nPolymer Class distribution:\")\nprint(train['Polymer Class'].value_counts())\n\n# Cell 4: Install RDKit\n# [Unchanged]\n!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n\n# Cell 5: Script 1 - CatBoost Stacking Model\n# [Unchanged]\n# Outputs: submission_catboost.csv, oof_catboost.csv\n\n# Cell 6: Empty Cell\n# [Unchanged]\n# Placeholder for future use\n\n# Cell 7: Script 2 - Single-Task SMILES-BERT TTA Model\n# ==============================================================================\n# SCRIPT 2: SINGLE-TASK SMILES-BERT TTA MODEL (ADVANCED)\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport random\nimport joblib\nimport torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedModel, AutoConfig, AutoModel, AutoTokenizer\nfrom transformers.activations import ACT2FN\nfrom rdkit import Chem\nimport gc\nwarnings.filterwarnings('ignore')\nfrom rdkit import rdBase\nrdBase.DisableLog('rdApp.warning')\n\n# Configuration\ntrain_df = train\ntest_df = test\nsample_df = sample\nTARGET_VARIABLES = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\nN_AUGMENTATIONS = 300\nRANDOM_STATE = 42\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = {'Tg': 16, 'FFV': 32, 'Tc': 16, 'Density': 32, 'Rg': 32}\nSEEDS = [42, 123, 456]\nrandom.seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)\ntorch.manual_seed(RANDOM_STATE)\n\n# Model Definition\nclass ContextPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(0.2)  # For MCD\n        self.activation = ACT2FN[config.hidden_act]\n    def forward(self, hidden_states):\n        context_token = hidden_states[:, 0]\n        context_token = self.dropout(context_token)\n        pooled_output = self.dense(context_token)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\nclass CustomModel(PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.backbone = AutoModel.from_config(config)\n        self.pooler = ContextPooler(config)\n        self.output = nn.Linear(config.hidden_size, 1)\n    def forward(self, input_ids, attention_mask):\n        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = self.pooler(outputs.last_hidden_state)\n        return self.output(pooled_output)\n\n# Helper Functions\ndef load_model_and_scaler(model_path, scaler_path, target):\n    config = AutoConfig.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')\n    model = CustomModel(config).to(DEVICE)\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n    model.eval()\n    scalers = joblib.load(scaler_path)\n    scaler_index = TARGET_VARIABLES.index(target)\n    scaler = scalers[scaler_index]\n    return model, scaler\n\ndef augment_smiles(smiles: str, n_augs: int):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return [smiles]\n    augmented = {smiles}\n    for _ in range(n_augs * 2):\n        if len(augmented) >= n_augs:\n            break\n        aug_smiles = Chem.MolToSmiles(mol, canonical=False, doRandom=True, isomericSmiles=True)\n        augmented.add(aug_smiles)\n    return list(augmented)\n\ndef prepare_input(smiles, polymer_class):\n    return f\"SMILES: {smiles} | Class: {polymer_class}\"\n\n# Inference Pipeline\nprint(\"\\n--- RUNNING SCRIPT 2: SMILES-BERT MODEL (OOF + TTA) ---\")\ntokenizer = AutoTokenizer.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')\nscaler_path = '/kaggle/input/smiles-bert-models/target_scalers.pkl'\nbert_oof_df = pd.DataFrame(index=train_df.index)\nbert_predictions_df = pd.DataFrame({'id': test_df['id']})\nuncertainty_df = pd.DataFrame(index=train_df.index)  # For OOF\ntest_uncertainty_df = pd.DataFrame(index=test_df.index)  # For test\n\nfor target in TARGET_VARIABLES:\n    print(f\"\\n  Processing for {target}...\")\n    y = train_df[target].dropna()\n    X_subset_smiles = train_df.loc[y.index]\n    model_path = f'/kaggle/input/smiles-bert-models/trained_smiles_model_{target}_target.pth'\n    model, scaler = load_model_and_scaler(model_path, scaler_path, target)\n    batch_size = BATCH_SIZE[target]\n    \n    # Debug Polymer Class\n    print(f\"    Polymer Class distribution for {target}:\")\n    print(X_subset_smiles['Polymer Class'].value_counts(dropna=False))\n    \n    # OOF Predictions\n    print(f\"    Generating OOF predictions for {target}...\")\n    oof_preds = pd.Series(0.0, index=X_subset_smiles.index, dtype=np.float32)\n    oof_uncertainty = pd.Series(0.0, index=X_subset_smiles.index, dtype=np.float32)\n    batches = [X_subset_smiles.index[i:i + batch_size] for i in range(0, len(X_subset_smiles), batch_size)]\n    \n    for seed in SEEDS:\n        torch.manual_seed(seed)\n        with torch.no_grad():\n            for batch_idx in tqdm(batches, desc=\"    OOF Batches\"):\n                batch_smiles = X_subset_smiles.loc[batch_idx]['SMILES'].tolist()\n                batch_classes = X_subset_smiles.loc[batch_idx]['Polymer Class'].fillna('Unknown').tolist()\n                batch_inputs = [prepare_input(s, c) for s, c in zip(batch_smiles, batch_classes)]\n                inputs = tokenizer(batch_inputs, return_tensors='pt', truncation=True, padding=True, max_length=512)\n                inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n                preds = model(**inputs).squeeze(-1)\n                scaled_preds = preds.cpu().numpy()\n                unscaled_preds = scaler.inverse_transform(scaled_preds.reshape(-1, 1)).flatten()\n                oof_preds.loc[batch_idx] += unscaled_preds / len(SEEDS)\n    \n    bert_oof_df[target] = oof_preds\n    \n    # Compute OOF uncertainty\n    print(f\"    Computing OOF uncertainty for {target}...\")\n    for idx in tqdm(X_subset_smiles.index, desc=\"    OOF Uncertainty\"):\n        smiles = X_subset_smiles.loc[idx, 'SMILES']\n        polymer_class = X_subset_smiles.loc[idx, 'Polymer Class']\n        if pd.isna(polymer_class):\n            polymer_class = 'Unknown'\n        augmented_smiles_list = augment_smiles(smiles, N_AUGMENTATIONS)\n        aug_inputs = [prepare_input(s, polymer_class) for s in augmented_smiles_list]\n        pred_samples = []\n        for chunk in [aug_inputs[i:i + batch_size] for i in range(0, len(aug_inputs), batch_size)]:\n            inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding=True, max_length=512)\n            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n            with torch.no_grad():\n                for _ in range(5):  # MCD with 5 samples\n                    preds = model(**inputs).squeeze(-1)\n                    scaled_preds = preds.cpu().numpy()\n                    unscaled_preds = scaler.inverse_transform(scaled_preds.reshape(-1, 1)).flatten()\n                    pred_samples.extend(unscaled_preds)\n        oof_uncertainty.loc[idx] = np.std(pred_samples)\n    uncertainty_df[target] = oof_uncertainty\n    \n    # Test Predictions with Uncertainty-Aware TTA\n    print(f\"    Generating Test predictions with TTA for {target}...\")\n    target_preds = np.zeros(len(test_df))\n    target_uncertainty = np.zeros(len(test_df))\n    \n    for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n        augmented_smiles_list = augment_smiles(row['SMILES'], N_AUGMENTATIONS)\n        polymer_class = row.get('Polymer Class', 'Unknown')\n        aug_inputs = [prepare_input(s, polymer_class) for s in augmented_smiles_list]\n        pred_samples = []\n        for chunk in [aug_inputs[i:i + batch_size] for i in range(0, len(aug_inputs), batch_size)]:\n            inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding=True, max_length=512)\n            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n            with torch.no_grad():\n                for _ in range(5):  # MCD with 5 samples\n                    preds = model(**inputs).squeeze(-1)\n                    scaled_preds = preds.cpu().numpy()\n                    unscaled_preds = scaler.inverse_transform(scaled_preds.reshape(-1, 1)).flatten()\n                    pred_samples.extend(unscaled_preds)\n        target_preds[i] = np.mean(pred_samples)\n        target_uncertainty[i] = np.std(pred_samples)\n    \n    bert_predictions_df[target] = target_preds\n    test_uncertainty_df[target] = target_uncertainty\n    \n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n\nbert_predictions_df.to_csv('submission_bert.csv', index=False)\nbert_oof_df.to_csv('oof_bert.csv')\nuncertainty_df.to_csv('uncertainty_bert_oof.csv')\ntest_uncertainty_df.to_csv('uncertainty_bert_test.csv')\nprint(\"\\n--- SCRIPT 2 COMPLETE: BERT predictions and OOF file saved. ---\")\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Cell 8: Define Metrics\n# [Unchanged]\nMINMAX_DICT = {\n    'Tg': [-148.0297376, 472.25],\n    'FFV': [0.2269924, 0.77709707],\n    'Tc': [0.0465, 0.524],\n    'Density': [0.748691234, 1.840998909],\n    'Rg': [9.7283551, 34.672905605],\n}\nNULL_FOR_SUBMISSION = -9999\ntarget_weights = {'Tg': 0.5, 'FFV': 2.0, 'Tc': 1.0, 'Density': 1.0, 'Rg': 0.8}\n\ndef scaling_error(labels, preds, property):\n    error = np.abs(labels - preds)\n    min_val, max_val = MINMAX_DICT[property]\n    label_range = max_val - min_val\n    return np.mean(error / label_range)\n\ndef wmae(labels, preds, weights):\n    return np.mean(np.abs(labels - preds) * weights)\n\n# Cell 9: Script 3 - Optimal Blending\n# ==============================================================================\n# SCRIPT 3: FINAL OPTIMAL BLENDING (ADVANCED)\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nfrom catboost import CatBoostRegressor, Pool\n\n# Configuration\ntrain_df = train\nTARGET_VARIABLES = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n\nprint(\"\\n--- RUNNING SCRIPT 3: OPTIMAL WEIGHT FINDING & BLENDING ---\")\n\n# Load predictions and OOF files\nbert_test_preds = pd.read_csv('submission_bert.csv')\ncatboost_test_preds = pd.read_csv('submission_catboost.csv')\nbert_oof_df = pd.read_csv('oof_bert.csv', index_col=0)\ncatboost_oof_df = pd.read_csv('oof_catboost.csv', index_col=0)\nuncertainty_df = pd.read_csv('uncertainty_bert_oof.csv', index_col=0)\ntest_uncertainty_df = pd.read_csv('uncertainty_bert_test.csv', index_col=0)\n\nfinal_submission = pd.DataFrame({'id': bert_test_preds['id']})\n\nprint(\"Finding optimal blend weights for each target...\")\nfor target in TARGET_VARIABLES:\n    print(f\"  Processing blending for {target}...\")\n    \n    # Filter non-NaN true labels\n    valid_indices = train_df[target].dropna().index\n    if len(valid_indices) == 0:\n        print(f\"  Warning: No non-NaN labels for {target}. Using CatBoost predictions.\")\n        final_submission[target] = catboost_test_preds[target]\n        print(f\"  wMAE for {target}: Not calculated (no valid OOF samples).\")\n        continue\n    \n    # Debug index alignment\n    print(f\"  Number of valid indices for {target}: {len(valid_indices)}\")\n    print(f\"  Available indices in uncertainty_df: {len(uncertainty_df.index)}\")\n    \n    # Align OOF predictions with valid indices\n    try:\n        oof_df = pd.concat([\n            train_df.loc[valid_indices, target],\n            bert_oof_df.loc[valid_indices, target],\n            catboost_oof_df.loc[valid_indices, target],\n            uncertainty_df.loc[valid_indices, target]\n        ], axis=1)\n        oof_df.columns = ['true', 'bert', 'catboost', 'uncertainty']\n        oof_df.dropna(inplace=True)\n    except KeyError as e:\n        print(f\"  Error: Index alignment failed for {target}: {e}\")\n        print(f\"  Using CatBoost predictions due to alignment failure.\")\n        final_submission[target] = catboost_test_preds[target]\n        print(f\"  wMAE for {target}: Not calculated (alignment error).\")\n        continue\n    \n    if len(oof_df) == 0:\n        print(f\"  Warning: No valid OOF samples for {target} after alignment. Using CatBoost predictions.\")\n        final_submission[target] = catboost_test_preds[target]\n        print(f\"  wMAE for {target}: Not calculated (no valid OOF samples).\")\n        continue\n    \n    print(f\"  Number of valid OOF samples for {target}: {len(oof_df)}\")\n    \n    # Stack with uncertainty\n    X_stack = oof_df[['bert', 'catboost', 'uncertainty']]\n    y_stack = oof_df['true']\n    model = CatBoostRegressor(iterations=100, learning_rate=0.05, depth=6, random_seed=42, verbose=False)\n    train_pool = Pool(X_stack, y_stack)\n    model.fit(train_pool)\n    \n    # Predict on test\n    X_test_stack = pd.DataFrame({\n        'bert': bert_test_preds[target],\n        'catboost': catboost_test_preds[target],\n        'uncertainty': test_uncertainty_df[target]\n    })\n    final_submission[target] = model.predict(X_test_stack)\n    \n    # Evaluate wMAE\n    blend_preds = model.predict(X_stack)\n    wmae_score = wmae(oof_df['true'], blend_preds, target_weights[target])\n    print(f\"  wMAE for {target}: {wmae_score:.5f}\")\n\n# Ensure correct column order\nsample_df = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/sample_submission.csv')\nfinal_submission = final_submission[sample_df.columns]\nfinal_submission.to_csv('submission.csv', index=False)\n\nprint(\"\\nFinal submission file created using advanced stacking!\")\nprint(\"Preview:\")\nprint(final_submission.head())\n\n# Cell 10: Empty Cell\n# [Unchanged]\n# Placeholder for future use","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T10:20:09.37379Z","iopub.execute_input":"2025-06-25T10:20:09.374421Z","iopub.status.idle":"2025-06-25T11:19:14.835672Z","shell.execute_reply.started":"2025-06-25T10:20:09.374396Z","shell.execute_reply":"2025-06-25T11:19:14.834169Z"}},"outputs":[{"name":"stdout","text":"Target counts:\nTg         2565\nFFV        7101\nTc          808\nDensity     684\nRg          685\ndtype: int64\n\nPolymer Class distribution:\nPolymer Class\nUnknown                                                                                        8044\nPolyacrylics, Polyvinyls                                                                        143\nPolyimides/thioimides                                                                           123\nPolysiloxanes/silanes                                                                           122\nPolyimides/thioimides, Polyoxides/ethers/acetals                                                113\n                                                                                               ... \nPolyamides/thioamides, Polysiloxanes/silanes, Polysulfones/sulfoxides/sufonates/sulfoamides       1\nPolyamides/thioamides, Polysiloxanes/silanes                                                      1\nPolyurethanes/thiourethanes, Polysulfones/sulfoxides/sufonates/sulfoamides                        1\nPolyimides/thioimides, Polyimines, Polyketones/thioketones                                        1\nPolyimides/thioimides, Polyureas/thioureas                                                        1\nName: count, Length: 136, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\nrdkit is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n\n--- RUNNING SCRIPT 2: SMILES-BERT MODEL (OOF + TTA) ---\n\n  Processing for Tg...\n    Polymer Class distribution for Tg:\nPolymer Class\nUnknown                                                                                        582\nPolyacrylics, Polyvinyls                                                                       143\nPolyimides/thioimides                                                                          123\nPolysiloxanes/silanes                                                                          122\nPolyimides/thioimides, Polyoxides/ethers/acetals                                               113\n                                                                                              ... \nPolyamides/thioamides, Polysiloxanes/silanes, Polysulfones/sulfoxides/sufonates/sulfoamides      1\nPolyamides/thioamides, Polysiloxanes/silanes                                                     1\nPolyurethanes/thiourethanes, Polysulfones/sulfoxides/sufonates/sulfoamides                       1\nPolyimides/thioimides, Polyimines, Polyketones/thioketones                                       1\nPolyimides/thioimides, Polyureas/thioureas                                                       1\nName: count, Length: 136, dtype: int64\n    Generating OOF predictions for Tg...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/161 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6fb20b1f724457b8aed264d8bc5cdb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/161 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6818d26b8c80491e9fad4087ed60a631"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/161 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e55fd05510d4813a5520e1e0bf4d6ab"}},"metadata":{}},{"name":"stdout","text":"    Computing OOF uncertainty for Tg...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    OOF Uncertainty:   0%|          | 0/2565 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cad8748309b4ba68c4d38e4fe4d2a13"}},"metadata":{}},{"name":"stdout","text":"    Generating Test predictions with TTA for Tg...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c75d48b9c84fe295ecaae6f3fb5630"}},"metadata":{}},{"name":"stdout","text":"\n  Processing for FFV...\n    Polymer Class distribution for FFV:\nPolymer Class\nUnknown    7101\nName: count, dtype: int64\n    Generating OOF predictions for FFV...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b3d026f326145d9aedcc0eb1af8d5ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c76392fa496422398632aa065ad38b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b02c38a6b241f68d55e7a854ce4634"}},"metadata":{}},{"name":"stdout","text":"    Computing OOF uncertainty for FFV...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    OOF Uncertainty:   0%|          | 0/7101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4719e11bbc3d43c286d3cc19b77ee622"}},"metadata":{}},{"name":"stdout","text":"    Generating Test predictions with TTA for FFV...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e73840dceea42749e8f6de632c965ce"}},"metadata":{}},{"name":"stdout","text":"\n  Processing for Tc...\n    Polymer Class distribution for Tc:\nPolymer Class\nUnknown    808\nName: count, dtype: int64\n    Generating OOF predictions for Tc...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/51 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0dded95da11410fad1fa6d4ea9696fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/51 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da7cc8428026443b9f437d7cc47fcde7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/51 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e3f5404ce23415badc49024a5e16daf"}},"metadata":{}},{"name":"stdout","text":"    Computing OOF uncertainty for Tc...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    OOF Uncertainty:   0%|          | 0/808 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"160c265b1f82446193d5771d43d37112"}},"metadata":{}},{"name":"stdout","text":"    Generating Test predictions with TTA for Tc...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220b744143af4149bbb577fd9c05c6f0"}},"metadata":{}},{"name":"stdout","text":"\n  Processing for Density...\n    Polymer Class distribution for Density:\nPolymer Class\nUnknown    684\nName: count, dtype: int64\n    Generating OOF predictions for Density...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed67f39b4724e06a2c58564910da9e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c187659f207748f58a226e86d7fc3870"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a2c22532f6048fab7ede8bf66e29027"}},"metadata":{}},{"name":"stdout","text":"    Computing OOF uncertainty for Density...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    OOF Uncertainty:   0%|          | 0/684 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"237961e3cba949dd8049a178a6d49061"}},"metadata":{}},{"name":"stdout","text":"    Generating Test predictions with TTA for Density...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a007116ea2044069721aac4cff1b45f"}},"metadata":{}},{"name":"stdout","text":"\n  Processing for Rg...\n    Polymer Class distribution for Rg:\nPolymer Class\nUnknown    685\nName: count, dtype: int64\n    Generating OOF predictions for Rg...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dd17ae6456b4052807ad9d08da96a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c2bda080fae4d41bd6f540dc19a5b50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    OOF Batches:   0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"022a1a36b4a14c11b776fc5665f87f27"}},"metadata":{}},{"name":"stdout","text":"    Computing OOF uncertainty for Rg...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    OOF Uncertainty:   0%|          | 0/685 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a92826b4beff43b5a1bfe35144c55864"}},"metadata":{}},{"name":"stdout","text":"    Generating Test predictions with TTA for Rg...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88860f3cddbd4c13a52b2a2848dfce53"}},"metadata":{}},{"name":"stdout","text":"\n--- SCRIPT 2 COMPLETE: BERT predictions and OOF file saved. ---\n\n--- RUNNING SCRIPT 3: OPTIMAL WEIGHT FINDING & BLENDING ---\nFinding optimal blend weights for each target...\n  Processing blending for Tg...\n  Number of valid indices for Tg: 2565\n  Available indices in uncertainty_df: 10027\n  Number of valid OOF samples for Tg: 2494\n  wMAE for Tg: 16.02795\n  Processing blending for FFV...\n  Number of valid indices for FFV: 7101\n  Available indices in uncertainty_df: 10027\n  Number of valid OOF samples for FFV: 7030\n  wMAE for FFV: 0.01125\n  Processing blending for Tc...\n  Number of valid indices for Tc: 808\n  Available indices in uncertainty_df: 10027\n  Number of valid OOF samples for Tc: 808\n  wMAE for Tc: 0.02662\n  Processing blending for Density...\n  Number of valid indices for Density: 684\n  Available indices in uncertainty_df: 10027\n  Number of valid OOF samples for Density: 613\n  wMAE for Density: 0.02481\n  Processing blending for Rg...\n  Number of valid indices for Rg: 685\n  Available indices in uncertainty_df: 10027\n  Number of valid OOF samples for Rg: 614\n  wMAE for Rg: 1.11181\n\nFinal submission file created using advanced stacking!\nPreview:\n           id          Tg       FFV        Tc   Density         Rg\n0  1109053969  165.235385  0.377400  0.195896  1.118931  22.097961\n1  1422188626  175.376600  0.376952  0.237827  1.074450  19.468149\n2  2032016830  106.941294  0.349627  0.246004  1.117349  20.144888\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"d=pd.read_csv(\"/kaggle/working/oof_catboost.csv\")\nd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T10:11:57.272646Z","iopub.execute_input":"2025-06-25T10:11:57.273352Z","iopub.status.idle":"2025-06-25T10:11:57.299521Z","shell.execute_reply.started":"2025-06-25T10:11:57.273328Z","shell.execute_reply":"2025-06-25T10:11:57.298945Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"       Unnamed: 0  Tg       FFV        Tc  Density  Rg\n0               0 NaN  0.376041  0.219956      NaN NaN\n1               1 NaN  0.370727       NaN      NaN NaN\n2               2 NaN  0.375176       NaN      NaN NaN\n3               3 NaN  0.385296       NaN      NaN NaN\n4               4 NaN  0.356034       NaN      NaN NaN\n...           ...  ..       ...       ...      ...  ..\n10022       10022 NaN       NaN  0.229409      NaN NaN\n10023       10023 NaN       NaN  0.234503      NaN NaN\n10024       10024 NaN       NaN  0.145057      NaN NaN\n10025       10025 NaN       NaN  0.221726      NaN NaN\n10026       10026 NaN       NaN  0.226170      NaN NaN\n\n[10027 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Tg</th>\n      <th>FFV</th>\n      <th>Tc</th>\n      <th>Density</th>\n      <th>Rg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.376041</td>\n      <td>0.219956</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>0.370727</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>0.375176</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>0.385296</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>0.356034</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10022</th>\n      <td>10022</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.229409</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10023</th>\n      <td>10023</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.234503</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10024</th>\n      <td>10024</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.145057</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10025</th>\n      <td>10025</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.221726</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10026</th>\n      <td>10026</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.226170</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>10027 rows × 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Approach 5: Feature Selection with Recursive Feature Elimination\nTo reduce noise and overfitting, we apply Recursive Feature Elimination (RFE) with XGBoost to select the top 50 features for each target. This approach trains an XGBoost model on the selected features, improving efficiency and potentially accuracy by focusing on the most relevant descriptors.","metadata":{}},{"cell_type":"markdown","source":"%%time\n\nfrom sklearn.feature_selection import RFE\n\nOOF_Preds = []\nTest_Preds = []\n\nfor target in tqdm(CFG.targets):\n    print(f\"\\n=== Training with Feature Selection for {target} ===\\n\")\n    \n    # Prepare data\n    Xtrain_ = Xtrain.copy()\n    Xtrain_[target] = Ytrain[target]\n    Xtrain_ = Xtrain_.dropna(subset=[target])\n    idx = Xtrain_.index\n    Xtrain_.index = range(len(Xtrain_))\n    ytrain_ = Xtrain_[target]\n    \n    # Feature selection\n    X_tr = Xtrain_.drop(drop_cols, axis=1)\n    model = XGBRegressor(objective='reg:absoluteerror', random_state=CFG.random_state, n_estimators=100)\n    rfe = RFE(estimator=model, n_features_to_select=50)\n    rfe.fit(X_tr, ytrain_)\n    selected_cols = X_tr.columns[rfe.support_].tolist()\n    \n    # Cross-validation setup\n    cv = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n    oof_preds = np.zeros(len(Xtrain_))\n    test_preds = np.zeros(len(Xtest))\n    \n    # Train model on selected features\n    params = {\n        'objective': 'reg:absoluteerror',\n        'n_estimators': 1000 if target == 'FFV' else 300,\n        'learning_rate': 0.02,\n        'max_depth': 6 if target == 'FFV' else 4,\n        'colsample_bytree': 0.3,\n        'random_state': CFG.random_state,\n        'verbosity': 0\n    }\n    model = XGBRegressor(**params)\n    \n    for fold, (train_idx, dev_idx) in enumerate(cv.split(Xtrain_, ytrain_)):\n        X_tr_fold = Xtrain_.iloc[train_idx][selected_cols]\n        X_dev_fold = Xtrain_.iloc[dev_idx][selected_cols]\n        y_tr, y_dev = ytrain_.iloc[train_idx], ytrain_.iloc[dev_idx]\n        \n        model.fit(X_tr_fold, y_tr)\n        oof_preds[dev_idx] = model.predict(X_dev_fold)\n        test_preds += model.predict(Xtest[selected_cols]) / CFG.n_splits\n    \n    # Store predictions\n    OOF_Preds.append(pd.DataFrame({target: oof_preds}, index=idx))\n    Test_Preds.append(pd.DataFrame({target: test_preds}))\n    \n    # Compute score\n    score = mean_absolute_error(ytrain_, oof_preds)\n    print(f\"RFE CV score for {target}: {score:.8f}\")\n    \n    clean_memory()\n\n# Combine predictions\nrfe_oof_preds = pd.concat(OOF_Preds, axis=1).sort_index()\nrfe_test_preds = pd.concat(Test_Preds, axis=1)","metadata":{}},{"cell_type":"markdown","source":"## Final Ensemble and Submission\nThe final predictions are obtained by averaging the OOF and test predictions from all approaches (Gradient Boosting, Neural Network, GNN, Stacking, and RFE). This ensemble leverages the strengths of each method to maximize accuracy. The wMAE score is computed on the OOF predictions, and the test predictions are formatted for submission.","metadata":{}},{"cell_type":"markdown","source":"%%time\n\n# Combine all predictions\nall_oof_preds = [\n    gb_oof_preds.fillna(0),\n    nn_oof_preds.fillna(0),\n    gnn_oof_preds.fillna(0),\n    stack_oof_preds.fillna(0),\n    rfe_oof_preds.fillna(0)\n]\nall_test_preds = [\n    gb_test_preds.fillna(0),\n    nn_test_preds.fillna(0),\n    gnn_test_preds.fillna(0),\n    stack_test_preds.fillna(0),\n    rfe_test_preds.fillna(0)\n]\n\nfinal_oof_preds = pd.DataFrame(0, index=gb_oof_preds.index, columns=CFG.targets)\nfinal_test_preds = pd.DataFrame(0, index=gb_test_preds.index, columns=CFG.targets)\n\nfor oof, test in zip(all_oof_preds, all_test_preds):\n    for target in CFG.targets:\n        final_oof_preds[target] += oof[target] / len(all_oof_preds)\n        final_test_preds[target] += test[target] / len(all_test_preds)\n\n# Compute final CV score\nscore = score_metric(Ytrain, final_oof_preds)\nprint(f\"\\nFinal Ensemble CV score: {score:.8f}\\n\")\n\n# Prepare submission\nsub_fl = pd.read_csv(f\"{CFG.data_path}/sample_submission.csv\")\nfinal_test_preds[\"id\"] = sub_fl[\"id\"]\nfinal_test_preds[[\"id\"] + CFG.targets].to_csv(\"submission.csv\", index=False)\n\n# Display first few rows\n!head submission.csv","metadata":{}}]}