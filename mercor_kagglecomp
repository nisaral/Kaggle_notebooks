{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":117171,"databundleVersionId":14089262,"sourceType":"competition"},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":881.724544,"end_time":"2025-10-13T08:26:55.016349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-13T08:12:13.291805","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\"\"\"\nMercor AI Text Detection - 0.99+ AUC CHAMPION VERSION\n=====================================================\n\nv27: Dual Calibrated LLM Features + L2 Classical Stack\n- FIXES the 'token_type_ids' error for Desklib by manually passing inputs.\n- FIXES the 'size mismatch' error for Abhi by using the STANDARD\n  AutoModelForSequenceClassification loader for it.\n- Generates TWO calibrated LLM features:\n  1. 'llm_pred_desklib' (from desklib/ai-text-detector-v1.01)\n  2. 'llm_pred_abhi' (from abhi099k/ai-text-detector-deberta-v3-large-h1)\n- Feeds BOTH features into the proven v25.3 classical pipeline\n  (L2 Stack, SelectKBest(k=850), all handcrafted features, strong reg).\n\"\"\"\n\n# === General Imports ===\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport gc\nimport warnings\nimport time\nimport os\n\n# === Sklearn Imports ===\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.utils import shuffle\n\n# === GBDT Imports ===\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\n# === Hugging Face Imports ===\nprint(\"Installing/Updating transformers...\"); os.system('pip install -q transformers torch'); print(\"Libraries installed.\")\ntry:\n    import torch\n    import torch.nn as nn\n    # --- MODIFIED: Need AutoModel, AutoConfig, PreTrainedModel for custom class ---\n    from transformers import AutoTokenizer, AutoModel, AutoConfig, PreTrainedModel, AutoModelForSequenceClassification\n    from tqdm.auto import tqdm\n    import transformers\n    transformers.logging.set_verbosity_error()\nexcept ImportError as e: print(f\"FATAL ERROR during imports: {e}\"); raise e\nprint(\"All libraries imported successfully!\")\n\n# === Configuration ===\n# --- TWO LLM MODELS ---\nLLM_MODEL_NAME_1 = \"desklib/ai-text-detector-v1.01\"\nLLM_MODEL_NAME_2 = \"abhi099k/ai-text-detector-deberta-v3-large-h1\"\nN_SPLITS_CLASSICAL = 5\nN_FEATURES_CLASSICAL = 850\nMAX_LEN_LLM = 512\nRANDOM_SEED = 42\n\n# -------------------------------------------------------------------\n# 0a. CUSTOM LLM CLASS (for Desklib)\n# -------------------------------------------------------------------\nclass DesklibAIDetectionModel(PreTrainedModel):\n    \"\"\"Custom model class for models with this specific head.\"\"\"\n    config_class = AutoConfig\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = AutoModel.from_config(config)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.init_weights()\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = self.model(input_ids, attention_mask=attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n        pooled_output = sum_embeddings / sum_mask\n        logits = self.classifier(pooled_output)\n        loss = None\n        if labels is not None:\n            loss_fct = nn.BCEWithLogitsLoss()\n            loss = loss_fct(logits.view(-1), labels.float())\n        output = {\"logits\": logits}\n        if loss is not None: output[\"loss\"] = loss\n        return output\n\n# -------------------------------------------------------------------\n# 0b. LLM INFERENCE FUNCTION (for Desklib - WITH BUG FIX)\n# -------------------------------------------------------------------\ndef get_desklib_predictions(texts, model_name):\n    \"\"\"Generates predictions using the specified CUSTOM Desklib model class.\"\"\"\n    print(f\"\\n--- Starting LLM Inference ({model_name}) ---\"); model = None; tokenizer = None\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n    try:\n        print(f\"Loading Tokenizer: {model_name}...\"); tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        print(f\"Loading Model (Custom Class): {model_name}...\"); \n        model = DesklibAIDetectionModel.from_pretrained(model_name, trust_remote_code=True)\n        model.to(device); model.eval()\n        print(\"Model and Tokenizer loaded successfully.\")\n    except Exception as e:\n        print(f\"Error loading model/tokenizer {model_name}: {e}\\nFalling back to 0.5.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); return np.full(len(texts), 0.5)\n    \n    print(f\"Running LLM inference on {len(texts)} texts...\")\n    batch_size = 16 # Keep batch size small for 'large' models\n    print(f\"Using batch size: {batch_size}\")\n    \n    llm_probs = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), batch_size), desc=f\"LLM 1 ({model_name.split('/')[1]})\"):\n            batch_texts = texts[i : i + batch_size].tolist();\n            if not batch_texts: continue\n            try:\n                inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN_LLM)\n                # --- THE BUG FIX ---\n                input_ids = inputs['input_ids'].to(device)\n                attention_mask = inputs['attention_mask'].to(device)\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                # --- END BUG FIX ---\n                logits = outputs[\"logits\"]\n                probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n                if batch_size == 1 and isinstance(probs, (float, np.floating)): probs = [probs]\n                llm_probs.extend(probs)\n            except Exception as e: \n                print(f\"Error during LLM batch {i // batch_size}: {e}\")\n                if \"out of memory\" in str(e).lower(): print(\"!!! CUDA Out of Memory. Try reducing batch_size. !!!\")\n                llm_probs.extend([0.5] * len(batch_texts))\n            if i % (batch_size * 20) == 0: gc.collect(); torch.cuda.empty_cache()\n            \n    print(f\"LLM 1 ({model_name.split('/')[1]}) prediction feature generated.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); print(\"--- Finished LLM 1 Inference ---\")\n    if len(llm_probs) != len(texts): print(f\"CRITICAL WARNING: Length mismatch {len(llm_probs)} vs {len(texts)}. Padding/Truncating.\"); llm_probs = (llm_probs + [0.5] * len(texts))[:len(texts)]\n    return np.array(llm_probs)\n\n# -------------------------------------------------------------------\n# 0c. LLM INFERENCE FUNCTION (for Generic Classifiers like Abhi)\n# -------------------------------------------------------------------\ndef get_generic_llm_predictions(texts, model_name):\n    \"\"\"Generates predictions using a STANDARD AutoModelForSequenceClassification.\"\"\"\n    print(f\"\\n--- Starting LLM Inference ({model_name}) ---\"); model = None; tokenizer = None\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n    try:\n        print(f\"Loading Tokenizer: {model_name}...\"); tokenizer = AutoTokenizer.from_pretrained(model_name)\n        # --- USE STANDARD LOADER ---\n        print(f\"Loading Model (Standard Class): {model_name}...\"); \n        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n        model.to(device); model.eval()\n        print(\"Model and Tokenizer loaded successfully.\")\n        \n        positive_label_id = -1\n        if hasattr(model.config, \"label2id\"):\n            print(f\"Model labels found: {model.config.label2id}\")\n            for name, id_val in model.config.label2id.items():\n                if \"ai\" in name.lower() or \"fake\" in name.lower() or \"generated\" in name.lower() or name == \"LABEL_1\" or name.upper() == \"AI\":\n                    positive_label_id = id_val\n                    print(f\"Determined positive (AI) label ID: {positive_label_id} ('{name}')\")\n                    break\n        if positive_label_id == -1: positive_label_id = 1; print(f\"Could not determine AI label. Assuming ID {positive_label_id} is positive.\")\n            \n    except Exception as e:\n        print(f\"Error loading model/tokenizer {model_name}: {e}\\nFalling back to 0.5.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); return np.full(len(texts), 0.5)\n    \n    print(f\"Running LLM inference on {len(texts)} texts...\")\n    batch_size = 16 # Keep batch size small for 'large' models\n    print(f\"Using batch size: {batch_size}\")\n    \n    llm_probs = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), batch_size), desc=f\"LLM 2 ({model_name.split('/')[1]})\"):\n            batch_texts = texts[i : i + batch_size].tolist();\n            if not batch_texts: continue\n            try:\n                inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN_LLM).to(device)\n                # --- STANDARD CALL ---\n                outputs = model(**inputs); \n                logits = outputs.logits\n                # --- Use Softmax (model has 2 outputs) ---\n                probs_all = torch.softmax(logits, dim=-1)\n                probs = probs_all[:, positive_label_id].cpu().numpy()\n                if batch_size == 1 and isinstance(probs, (float, np.floating)): probs = [probs]\n                llm_probs.extend(probs.tolist())\n            except Exception as e: \n                print(f\"Error during LLM batch {i // batch_size}: {e}\")\n                if \"out of memory\" in str(e).lower(): print(\"!!! CUDA Out of Memory. Try reducing batch_size. !!!\")\n                llm_probs.extend([0.5] * len(batch_texts))\n            if i % (batch_size * 20) == 0: gc.collect(); torch.cuda.empty_cache()\n            \n    print(f\"LLM 2 ({model_name.split('/')[1]}) prediction feature generated.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); print(\"--- Finished LLM 2 Inference ---\")\n    if len(llm_probs) != len(texts): print(f\"CRITICAL WARNING: Length mismatch {len(llm_probs)} vs {len(texts)}. Padding/Truncating.\"); llm_probs = (llm_probs + [0.5] * len(texts))[:len(texts)]\n    return np.array(llm_probs)\n\n\n# -------------------------------------------------------------------\n# 1. CHAMPION FEATURE EXTRACTOR (Handles BOTH LLM preds)\n# -------------------------------------------------------------------\nclass ChampionFeatureExtractor:\n    \"\"\"Extracts base features, includes llm_preds, + new features.\"\"\"\n    def __init__(self):\n        self.ai_connectors = ['in conclusion', 'in summary', 'furthermore', 'moreover', 'additionally', 'however', 'therefore', 'thus', 'consequently', 'as a result', 'on the other hand', 'for instance', 'for example', 'it is important to note', 'it is worth noting', 'that being said']; self.formal_words = ['utilize', 'facilitate', 'implement', 'methodology', 'paradigm', 'leverage', 'robust', 'optimal', 'enhance', 'demonstrate', 'comprehensive', 'articulate']; self.hedging_words = ['may', 'might', 'could', 'possibly', 'perhaps', 'suggests', 'seems', 'appears', 'likely']; self.passive_indicators = ['is made', 'was made', 'is given', 'was given', 'is shown', 'was shown', 'is considered', 'was considered']\n        self.stopwords = set(ENGLISH_STOP_WORDS); self.sentence_splitter = re.compile(r'[.!?]+')\n    def _get_sent_len_var(self, text):\n        sentences = self.sentence_splitter.split(text); sentence_lengths = [len(s.split()) for s in sentences if len(s.split()) > 0]\n        if len(sentence_lengths) > 1: return np.var(sentence_lengths)\n        else: return 0\n    def extract_base_features(self, df):\n        features = pd.DataFrame(index=df.index);\n        # --- ADD BOTH LLM PREDS ---\n        if 'llm_pred_desklib' in df.columns: features['llm_pred_desklib'] = df['llm_pred_desklib']\n        if 'llm_pred_abhi' in df.columns: features['llm_pred_abhi'] = df['llm_pred_abhi']\n        \n        answers = df['answer'].fillna(''); words = answers.str.split(); word_counts = words.str.len()\n        features['text_length'] = answers.str.len(); features['word_count'] = word_counts; features['avg_word_length'] = features['text_length'] / (features['word_count'] + 1e-6); features['sentence_count'] = answers.str.count(r'[.!?]+'); features['avg_sentence_length'] = features['word_count'] / (features['sentence_count'] + 1e-6); features['comma_count'] = answers.str.count(','); features['period_count'] = answers.str.count(r'\\.'); features['exclamation_count'] = answers.str.count(r'!'); features['question_count'] = answers.str.count(r'\\?'); features['punctuation_ratio'] = (features['comma_count'] + features['period_count']) / (features['word_count'] + 1e-6); features['unique_words'] = answers.apply(lambda x: len(set(str(x).lower().split()))); features['ttr'] = features['unique_words'] / (features['word_count'] + 1e-6)\n        def count_phrases(text, phrases): text_lower = str(text).lower(); return sum(1 for phrase in phrases if phrase in text_lower)\n        features['ai_connector_count'] = answers.apply(lambda x: count_phrases(x, self.ai_connectors)); features['formal_word_count'] = answers.apply(lambda x: count_phrases(x, self.formal_words)); features['passive_voice_count'] = answers.apply(lambda x: count_phrases(x, self.passive_indicators)); features['hedging_word_count'] = answers.apply(lambda x: count_phrases(x, self.hedging_words)); features['ai_connector_ratio'] = features['ai_connector_count'] / (features['word_count'] + 1e-6); features['formal_word_ratio'] = features['formal_word_count'] / (features['word_count'] + 1e-6); features['passive_voice_ratio'] = features['passive_voice_count'] / (features['word_count'] + 1e-6); features['hedging_ratio'] = features['hedging_word_count'] / (features['word_count'] + 1e-6)\n        features['subordinate_ratio'] = answers.str.count(r'\\b(that|which|who|when|where|while|although|because|if)\\b') / (features['word_count'] + 1e-6); features['paragraph_count'] = answers.str.count(r'\\n\\n') + 1; features['avg_paragraph_len'] = features['word_count'] / (features['paragraph_count'] + 1e-6); features['word_length_std'] = answers.apply(lambda x: np.std([len(w) for w in str(x).split()]) if len(str(x).split()) > 1 else 0)\n        features['em_dash_count'] = answers.str.count('â€”'); features['em_dash_ratio'] = features['em_dash_count'] / (features['word_count'] + 1e-6)\n        features['question_mark_count'] = answers.str.count(r'\\?'); features['exclamation_mark_count'] = answers.str.count(r'!')\n        features['stopword_count'] = words.apply(lambda x: sum(1 for word in x if word.lower() in self.stopwords)); features['stopword_ratio'] = features['stopword_count'] / (features['word_count'] + 1e-6)\n        features['uppercase_word_count'] = words.apply(lambda x: sum(1 for word in x if word.isascii() and word.istitle())); features['uppercase_ratio'] = features['uppercase_word_count'] / (features['word_count'] + 1e-6)\n        features['sentence_length_variance'] = answers.apply(self._get_sent_len_var)\n        features.replace([np.inf, -np.inf], 0, inplace=True); features.fillna(0, inplace=True); return features\n\n# -------------------------------------------------------------------\n# 2. CHAMPION AI DETECTOR (L2 Stack model - Handles 2 LLM feats)\n# -------------------------------------------------------------------\nclass ChampionAIDetector: \n    \"\"\"Trains L2 Stack (LGBM+CatBoost+Ridge) using SelectKBest feats including 2 llm_preds.\"\"\"\n    def __init__(self, n_folds=N_SPLITS_CLASSICAL, n_features=N_FEATURES_CLASSICAL):\n        self.feature_extractor = ChampionFeatureExtractor(); self.n_folds = n_folds; self.n_features = n_features; self.models = {}; self.meta_model = None; self.calibrator = IsotonicRegression(out_of_bounds='clip'); self.scaler = StandardScaler(); self.tfidf_word = None; self.tfidf_char = None; self.selector = None; self.topic_columns = None; self.is_trained = False; print(f\"\\nClassical Detector (L2 Stack): {n_folds} folds, SelectKBest(k={n_features}).\")\n    def _get_consistent_topic_dummies(self, series, fit_columns=False):\n        # (Unchanged)\n        series = series.fillna(\"Unknown_Topic\"); dummies = pd.get_dummies(series, prefix='topic', dtype=int, dummy_na=False)\n        if fit_columns:\n            self.topic_columns = dummies.columns\n            if \"topic_Unknown_Topic\" not in self.topic_columns and series.astype(str).str.contains(\"Unknown_Topic\").any(): self.topic_columns = self.topic_columns.append(pd.Index([\"topic_Unknown_Topic\"]))\n            print(f\"Identified {len(self.topic_columns)} topic columns during fit.\"); return dummies.reindex(columns=self.topic_columns, fill_value=0)\n        elif self.topic_columns is not None: return dummies.reindex(columns=self.topic_columns, fill_value=0)\n        else: raise ValueError(\"Topic columns not fitted.\")\n    def prepare_champion_features(self, train_df, test_df, is_training_fold=False, is_full_training=False):\n        \"\"\"Prepares features. Fits objects if training. Separates llm_preds.\"\"\"\n        print(\"Extracting base handcrafted + LLM features...\"); train_base_features_df = self.feature_extractor.extract_base_features(train_df); test_base_features_df = self.feature_extractor.extract_base_features(test_df)\n        print(\"Processing topic features...\")\n        fit_topic_cols = (self.topic_columns is None) or is_full_training\n        train_topic_dummies = self._get_consistent_topic_dummies(train_df['topic'], fit_columns=fit_topic_cols); test_topic_dummies = self._get_consistent_topic_dummies(test_df['topic'], fit_columns=False)\n        \n        # --- SEPARATE BOTH LLM FEATURES ---\n        llm_train_feat_1 = train_base_features_df.pop('llm_pred_desklib').values.reshape(-1, 1)\n        llm_test_feat_1 = test_base_features_df.pop('llm_pred_desklib').values.reshape(-1, 1)\n        llm_train_feat_2 = train_base_features_df.pop('llm_pred_abhi').values.reshape(-1, 1)\n        llm_test_feat_2 = test_base_features_df.pop('llm_pred_abhi').values.reshape(-1, 1)\n\n        train_dense_features_df = pd.concat([train_base_features_df, train_topic_dummies], axis=1); test_dense_features_df = pd.concat([test_base_features_df, test_topic_dummies], axis=1)\n        test_dense_features_df = test_dense_features_df.reindex(columns=train_dense_features_df.columns, fill_value=0)\n        if is_training_fold or is_full_training: print(\"Fitting/Transforming dense features (excl. LLM)...\"); train_features_scaled = self.scaler.fit_transform(train_dense_features_df); test_features_scaled = self.scaler.transform(test_dense_features_df)\n        else: print(\"Transforming dense features (excl. LLM)...\"); train_features_scaled = self.scaler.transform(train_dense_features_df); test_features_scaled = self.scaler.transform(test_dense_features_df)\n        if is_training_fold or is_full_training:\n            print(\"Fitting Word TF-IDF...\"); self.tfidf_word = TfidfVectorizer(ngram_range=(1, 3), min_df=3, max_df=0.9, sublinear_tf=True, stop_words='english', max_features=2500)\n            train_tfidf_word = self.tfidf_word.fit_transform(train_df['answer'].fillna('')); test_tfidf_word = self.tfidf_word.transform(test_df['answer'].fillna(''))\n            print(\"Fitting Char TF-IDF...\"); self.tfidf_char = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 5), min_df=3, sublinear_tf=True, max_features=1250)\n            train_tfidf_char = self.tfidf_char.fit_transform(train_df['answer'].fillna('')); test_tfidf_char = self.tfidf_char.transform(test_df['answer'].fillna(''))\n        else:\n            if not self.tfidf_word or not self.tfidf_char: raise ValueError(\"TF-IDF vectorizers not fitted.\")\n            print(\"Transforming text using fitted TF-IDF...\"); train_tfidf_word = self.tfidf_word.transform(train_df['answer'].fillna('')); test_tfidf_word = self.tfidf_word.transform(test_df['answer'].fillna(''))\n            train_tfidf_char = self.tfidf_char.transform(train_df['answer'].fillna('')); test_tfidf_char = self.tfidf_char.transform(test_df['answer'].fillna(''))\n        \n        # --- COMBINE ALL FEATURES (incl. BOTH LLM preds) ---\n        print(\"Combining all feature sets...\"); \n        X_train_full = hstack([csr_matrix(train_features_scaled), train_tfidf_word, train_tfidf_char, csr_matrix(llm_train_feat_1), csr_matrix(llm_train_feat_2)]).tocsr()\n        X_test_full = hstack([csr_matrix(test_features_scaled), test_tfidf_word, test_tfidf_char, csr_matrix(llm_test_feat_1), csr_matrix(llm_test_feat_2)]).tocsr()\n        \n        print(f\"Shape before feature selection: Train={X_train_full.shape}, Test={X_test_full.shape}\")\n        if is_training_fold or is_full_training:\n            y_train = train_df['is_cheating'].values; print(f\"Fitting SelectKBest (k={self.n_features})...\"); self.selector = SelectKBest(f_classif, k=min(self.n_features, X_train_full.shape[1]))\n            with warnings.catch_warnings(): warnings.filterwarnings('ignore'); self.selector.fit(X_train_full, y_train)\n            X_train_selected = self.selector.transform(X_train_full); X_test_selected = self.selector.transform(X_test_full)\n        else:\n            if not self.selector: raise ValueError(\"SelectKBest not fitted.\"); print(f\"Transforming features using fitted SelectKBest (k={self.n_features})...\")\n            X_train_selected = self.selector.transform(X_train_full); X_test_selected = self.selector.transform(X_test_full)\n        X_train_final = X_train_selected.toarray(); X_test_final = X_test_selected.toarray()\n        print(f\"Final training feature shape: {X_train_final.shape}\"); print(f\"Final testing feature shape: {X_test_final.shape}\")\n        return X_train_final, X_test_final\n    \n    # --- L2 STACKING (Using v25.3 params) ---\n    def train_champion_ensemble(self, full_train_df):\n        \"\"\"Trains the L2 Stacking ensemble. Fits Scaler/TFIDF/Selector on first fold.\"\"\"\n        y_train = full_train_df['is_cheating'].values\n        print(f\"\\n--- Starting Classical Ensemble Training ---\"); print(f\"Training set size: {len(full_train_df)}\"); print(f\"Positive class ratio: {y_train.mean():.6f}\")\n        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=RANDOM_SEED)\n        oof_preds = {'lgb': np.zeros(len(full_train_df)), 'cat': np.zeros(len(full_train_df)), 'ridge': np.zeros(len(full_train_df))}\n        print(\"Fitting topic columns on full training data...\"); _ = self._get_consistent_topic_dummies(full_train_df['topic'], fit_columns=True)\n        print(\"\\nFitting Scalers, TFIDF, and Selector on FULL training data...\")\n        _ = self.prepare_champion_features(full_train_df, full_train_df, is_training_fold=True, is_full_training=True) # Fit preprocessing objects\n        print(\"\\nStarting K-Fold Training...\")\n        for fold, (train_idx, val_idx) in enumerate(skf.split(full_train_df, y_train)):\n            print(f\"\\n{'='*20} Classical Fold {fold+1}/{self.n_folds} {'='*20}\")\n            train_fold_df, val_fold_df = full_train_df.iloc[train_idx], full_train_df.iloc[val_idx]\n            y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n            X_train_fold, X_val_fold = self.prepare_champion_features(train_fold_df, val_fold_df, is_training_fold=False, is_full_training=False)\n            print(f\"Fold {fold+1}: Train shape {X_train_fold.shape}, Val shape {X_val_fold.shape}\")\n            fold_seed = RANDOM_SEED + fold\n            print(\"Training LightGBM...\"); lgb_model = lgb.LGBMClassifier(n_estimators=2500, learning_rate=0.01, max_depth=7, num_leaves=31, min_child_samples=20, subsample=0.7, colsample_bytree=0.6, reg_alpha=0.4, reg_lambda=0.4, random_state=fold_seed, verbose=-1, n_jobs=-1)\n            lgb_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], callbacks=[lgb.early_stopping(200, verbose=False)]); oof_preds['lgb'][val_idx] = lgb_model.predict_proba(X_val_fold)[:, 1]\n            print(\"Training CatBoost...\"); cat_model = CatBoostClassifier(iterations=2500, learning_rate=0.02, depth=6, l2_leaf_reg=5, random_seed=fold_seed, verbose=0, early_stopping_rounds=200, task_type=\"CPU\")\n            cat_model.fit(X_train_fold, y_train_fold, eval_set=(X_val_fold, y_val_fold), verbose=False); oof_preds['cat'][val_idx] = cat_model.predict_proba(X_val_fold)[:, 1]\n            print(\"Training Ridge Classifier...\"); ridge_scaler = StandardScaler(); X_train_fold_scaled = ridge_scaler.fit_transform(X_train_fold); X_val_fold_scaled = ridge_scaler.transform(X_val_fold)\n            ridge_model = RidgeClassifier(alpha=1.0, random_state=fold_seed); ridge_model.fit(X_train_fold_scaled, y_train_fold); ridge_scores = ridge_model.decision_function(X_val_fold_scaled); ridge_probs = 1 / (1 + np.exp(-ridge_scores))\n            oof_preds['ridge'][val_idx] = ridge_probs\n            fold_auc_lgb = roc_auc_score(y_val_fold, oof_preds['lgb'][val_idx]); fold_auc_cat = roc_auc_score(y_val_fold, oof_preds['cat'][val_idx]); fold_auc_ridge = roc_auc_score(y_val_fold, oof_preds['ridge'][val_idx])\n            print(f\"Fold {fold+1} LGB AUC: {fold_auc_lgb:.6f}\"); print(f\"Fold {fold+1} CAT AUC: {fold_auc_cat:.6f}\"); print(f\"Fold {fold+1} RIDGE AUC: {fold_auc_ridge:.6f}\")\n        print(\"\\n\" + \"=\"*50); print(\"Classical L1 Model OOF Scores:\"); [print(f\"  {name.upper()} OOF AUC: {roc_auc_score(y_train, preds):.8f}\") for name, preds in oof_preds.items()]\n        print(\"\\nTraining Classical L2 Meta-Model...\"); X_meta_train = np.stack(list(oof_preds.values()), axis=1)\n        self.meta_model = LogisticRegression(C=0.3, solver='liblinear', random_state=RANDOM_SEED); self.meta_model.fit(X_meta_train, y_train)\n        oof_ensemble_preds = self.meta_model.predict_proba(X_meta_train)[:, 1]; oof_ensemble_auc = roc_auc_score(y_train, oof_ensemble_preds)\n        print(f\"\\nCLASSICAL L2 STACKING OOF AUC: {oof_ensemble_auc:.8f}\"); print(\"Training Classical Isotonic Calibrator...\"); self.calibrator.fit(oof_ensemble_preds, y_train)\n        self.is_trained = True; return oof_ensemble_auc, oof_ensemble_preds\n    def predict_champion(self, full_train_df, test_df):\n        \"\"\"Generates final predictions using fitted classical components.\"\"\"\n        if not self.is_trained: raise ValueError(\"Classical model not trained!\")\n        print(\"\\n\" + \"=\"*50); print(\"Generating Final Classical Ensemble Predictions...\"); print(\"=\"*50)\n        print(\"Preparing features for final classical model training and prediction...\")\n        X_train_full, X_test = self.prepare_champion_features(full_train_df, test_df, is_training_fold=False, is_full_training=True) # Use is_full_training=True\n        y_train_full = full_train_df['is_cheating'].values; test_preds = {}\n        print(\"Training final LightGBM...\"); lgb_final = lgb.LGBMClassifier(n_estimators=2000, learning_rate=0.01, max_depth=7, num_leaves=31, min_child_samples=20, subsample=0.7, colsample_bytree=0.6, reg_alpha=0.4, reg_lambda=0.4, random_state=RANDOM_SEED, verbose=-1, n_jobs=-1)\n        lgb_final.fit(X_train_full, y_train_full); test_preds['lgb'] = lgb_final.predict_proba(X_test)[:, 1]\n        print(\"Training final CatBoost...\"); cat_final = CatBoostClassifier(iterations=2200, learning_rate=0.02, depth=6, l2_leaf_reg=5, random_seed=RANDOM_SEED, verbose=0, task_type=\"CPU\")\n        cat_final.fit(X_train_full, y_train_full); test_preds['cat'] = cat_final.predict_proba(X_test)[:, 1]\n        print(\"Training final Ridge...\"); ridge_scaler = StandardScaler(); X_train_full_scaled = ridge_scaler.fit_transform(X_train_full); X_test_scaled = ridge_scaler.transform(X_test)\n        ridge_final = RidgeClassifier(alpha=1.0, random_state=RANDOM_SEED); ridge_final.fit(X_train_full_scaled, y_train_full)\n        ridge_scores = ridge_final.decision_function(X_test_scaled); test_preds['ridge'] = 1 / (1 + np.exp(-ridge_scores))\n        print(\"Applying Classical L2 Meta-Model...\"); X_meta_test = np.stack(list(test_preds.values()), axis=1)\n        final_proba = self.meta_model.predict_proba(X_meta_test)[:, 1]\n        print(\"Applying Classical Isotonic Calibration...\"); calibrated_proba = self.calibrator.transform(final_proba)\n        calibrated_proba = np.clip(calibrated_proba, 0.001, 0.999)\n        return calibrated_proba, final_proba\n\n# --- (Fallback model - unchanged) ---\ndef champion_fallback():\n    \"\"\"Fallback using basic LGBM, basic features.\"\"\"\n    print(\"CRITICAL ERROR: Main pipeline failed. Using CHAMPION FALLBACK model...\")\n    try:\n        from sklearn.preprocessing import StandardScaler; from scipy.sparse import hstack, csr_matrix;\n        train_df = pd.read_csv('/kaggle/input/mercor-ai-detection/train.csv'); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv')\n        feature_extractor = ChampionFeatureExtractor(); train_base_features = feature_extractor.extract_base_features(train_df.drop(columns=['llm_pred_desklib', 'llm_pred_abhi'], errors='ignore'))\n        test_base_features = feature_extractor.extract_base_features(test_df.drop(columns=['llm_pred_desklib', 'llm_pred_abhi'], errors='ignore'))\n        train_topic_dummies = pd.get_dummies(train_df['topic'], prefix='topic', dtype=int); fallback_topic_columns = train_topic_dummies.columns; test_topic_dummies = pd.get_dummies(test_df['topic'], prefix='topic', dtype=int).reindex(columns=fallback_topic_columns, fill_value=0)\n        train_dense_features = pd.concat([train_base_features, train_topic_dummies], axis=1); test_dense_features = pd.concat([test_base_features, test_topic_dummies], axis=1).reindex(columns=train_dense_features.columns, fill_value=0)\n        scaler = StandardScaler(); train_dense_scaled = scaler.fit_transform(train_dense_features); test_dense_scaled = scaler.transform(test_dense_features)\n        tfidf = TfidfVectorizer(max_features=2500, ngram_range=(1, 3), stop_words='english'); train_tfidf = tfidf.fit_transform(train_df['answer'].fillna('')); test_tfidf = tfidf.transform(test_df['answer'].fillna(''))\n        X_train = hstack([csr_matrix(train_dense_scaled), train_tfidf]).tocsr(); X_test = hstack([csr_matrix(test_dense_scaled), test_tfidf]).tocsr(); y_train = train_df['is_cheating']\n        model = lgb.LGBMClassifier(n_estimators=1000, learning_rate=0.01, max_depth=7, num_leaves=63, random_state=RANDOM_SEED, verbose=-1, n_jobs=-1, reg_alpha=0.1, reg_lambda=0.1)\n        model.fit(X_train, y_train); test_proba = model.predict_proba(X_test.toarray())[:, 1]\n        submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': test_proba}); submission.to_csv('submission.csv', index=False, float_format='%.10f')\n        print(\"Fallback submission.csv created successfully.\"); return submission\n    except Exception as e:\n        print(f\"Fallback model ALSO failed: {e}\"); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv'); submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': 0.5}); submission.to_csv('submission.csv', index=False, float_format='%.10f'); print(\"CRITICAL: Created dummy submission.csv with 0.5 probability.\"); return submission\n\n# -------------------------------------------------------------------\n# 4. MAIN EXECUTION (Calibrates BOTH LLM Preds, uses L2 Stack)\n# -------------------------------------------------------------------\ndef champion_main():\n    \"\"\"Main function: Runs 2 LLM inferences, calibrates features, runs classical L2 stack.\"\"\"\n    print(\"=\" * 60); print(f\"  Mercor AI Text Detection - CHAMPION v27 (Dual LLM + L2 Stack)\"); print(\"=\" * 60)\n    train_df = pd.read_csv('/kaggle/input/mercor-ai-detection/train.csv'); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv')\n    print(f\"Training data shape: {train_df.shape}\"); print(f\"Test data shape: {test_df.shape}\")\n    train_df['answer'] = train_df['answer'].fillna(''); test_df['answer'] = test_df['answer'].fillna('')\n    all_texts = pd.concat([train_df['answer'], test_df['answer']])\n    \n    # --- Step 1A: Generate Desklib predictions ---\n    llm_preds_all_1 = get_desklib_predictions(all_texts, LLM_MODEL_NAME_1)\n    llm_oof_preds_1 = llm_preds_all_1[:len(train_df)]; llm_test_preds_1 = llm_preds_all_1[len(train_df):]\n    print(\"\\nCalibrating Desklib predictions...\"); llm_calibrator_1 = IsotonicRegression(out_of_bounds='clip')\n    llm_calibrator_1.fit(llm_oof_preds_1, train_df['is_cheating'].values)\n    llm_oof_calibrated_1 = llm_calibrator_1.transform(llm_oof_preds_1)\n    llm_test_calibrated_1 = lll_calibrator_1.transform(llm_test_preds_1) # --- TYPO WAS HERE ---\n    llm_test_calibrated_1 = llm_calibrator_1.transform(llm_test_preds_1) # --- FIX ---\n    print(f\"Calibrated Desklib OOF AUC: {roc_auc_score(train_df['is_cheating'], llm_oof_calibrated_1):.8f}\")\n    train_df['llm_pred_desklib'] = llm_oof_calibrated_1\n    test_df['llm_pred_desklib'] = llm_test_calibrated_1\n    print(\"CALIBRATED Desklib feature added.\")\n\n    # --- Step 1B: Generate Abhi predictions ---\n    llm_preds_all_2 = get_generic_llm_predictions(all_texts, LLM_MODEL_NAME_2) # Use the generic loader\n    llm_oof_preds_2 = llm_preds_all_2[:len(train_df)]; llm_test_preds_2 = llm_preds_all_2[len(train_df):]\n    print(\"\\nCalibrating Abhi predictions...\"); llm_calibrator_2 = IsotonicRegression(out_of_bounds='clip')\n    llm_calibrator_2.fit(llm_oof_preds_2, train_df['is_cheating'].values)\n    llm_oof_calibrated_2 = llm_calibrator_2.transform(llm_oof_preds_2)\n    llm_test_calibrated_2 = llm_calibrator_2.transform(llm_test_preds_2)\n    print(f\"Calibrated Abhi OOF AUC: {roc_auc_score(train_df['is_cheating'], llm_oof_calibrated_2):.8f}\")\n    train_df['llm_pred_abhi'] = llm_oof_calibrated_2\n    test_df['llm_pred_abhi'] = llm_test_calibrated_2\n    print(\"CALIBRATED Abhi feature added.\")\n    \n    # --- Step 2: Run Classical L2 Stack Ensemble ---\n    classical_detector = ChampionAIDetector(n_folds=N_SPLITS_CLASSICAL, n_features=N_FEATURES_CLASSICAL) # L2 Stack class\n    oof_auc_classical, oof_preds_classical = classical_detector.train_champion_ensemble(train_df)\n    print(\"\\nStarting Final Test Set Prediction using L2 Stack...\");\n    calibrated_proba, base_proba = classical_detector.predict_champion(train_df, test_df)\n    \n    # Create submission files\n    submission_calibrated = pd.DataFrame({'id': test_df['id'], 'is_cheating': calibrated_proba}); submission_base = pd.DataFrame({'id': test_df['id'], 'is_cheating': base_proba})\n    submission_calibrated.to_csv('submission.csv', index=False, float_format='%.10f'); submission_base.to_csv('submission_base_uncalibrated.csv', index=False, float_format='%.10f')\n    # Final Output\n    print(\"\\n\" + \"=\"*60); print(\"CHAMPION PIPELINE COMPLETED SUCCESSFULLY!\");\n    print(f\"Final L2 Stack OOF AUC (with DUAL Calibrated LLM Feats): {oof_auc_classical:.8f}\")\n    print(\"\\nSubmission files created:\"); print(f\"  1. submission.csv (RECOMMENDED)\"); print(f\"  2. submission_base_uncalibrated.csv (Backup)\")\n    print(f\"\\nFinal 'submission.csv' prediction statistics:\"); print(f\"  Min: {calibrated_proba.min():.6f}\"); print(f\"  Max: {calibrated_proba.max():.6f}\"); print(f\"  Mean: {calibrated_proba.mean():.6f}\"); print(f\"  Median: {np.median(calibrated_proba):.6f}\")\n    print(\"\\nTop 10 Predictions:\"); print(submission_calibrated.head(10).to_string(index=False)); print(\"=\" * 60)\n\n# --- Run the Champion Pipeline ---\nif __name__ == \"__main__\":\n    start_time = time.time()\n    try:\n        gc.collect(); torch.cuda.empty_cache()\n        champion_main()\n    except Exception as e:\n        print(f\"\\n!!! AN ERROR OCCURRED IN THE MAIN PIPELINE !!!: {e}\")\n        import traceback; traceback.print_exc()\n        champion_fallback()\n    finally:\n        gc.collect(); torch.cuda.empty_cache(); end_time = time.time()\n        print(f\"\\nTotal execution time: {(end_time - start_time) / 60:.2f} minutes\")","metadata":{"execution":{"iopub.status.busy":"2025-11-01T15:51:42.985273Z","iopub.execute_input":"2025-11-01T15:51:42.985618Z","iopub.status.idle":"2025-11-01T15:52:46.812082Z","shell.execute_reply.started":"2025-11-01T15:51:42.985586Z","shell.execute_reply":"2025-11-01T15:52:46.811395Z"}}},{"cell_type":"code","source":"id\tis_cheating\n0\tscr_81822029c661\t0.996701\n1\tscr_52efb19e0ea9\t0.999000\n2\tscr_8fc0f33c559e\t0.494624\n3\tscr_bac3f5d3aa12\t0.137097\n4\tscr_adfbe009984d\t0.015537\n...\t...\t...\n259\tscr_9497704e40c5\t0.494624\n260\tscr_df97a5fa0b8f\t0.997403\n261\tscr_6175540a0c81\t0.927039\n262\tscr_fa79a8d5de1c\t0.935484\n263\tscr_8f2dde6f6734\t0.999000","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"d=pd.read_csv(\"/kaggle/working/submission.csv\")\nd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\"\"\"\nMercor AI Text Detection - 0.99+ AUC CHAMPION VERSION\n=====================================================\n\nv27.1: The \"Fixed Typo\" Dual LLM Version\n- FIXES the 'lll_calibrator_1' NameError in champion_main.\n- Uses the 'desklib' model with the custom class (FIXED).\n- Uses the 'abhi099k/large' model with the standard class (FIXED).\n- Feeds BOTH calibrated features into the proven v25.3 classical stack.\n\"\"\"\n\n# === General Imports ===\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport gc\nimport warnings\nimport time\nimport os\n\n# === Sklearn Imports ===\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.utils import shuffle\n\n# === GBDT Imports ===\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\n# === Hugging Face Imports ===\nprint(\"Installing/Updating transformers...\"); os.system('pip install -q transformers torch'); print(\"Libraries installed.\")\ntry:\n    import torch\n    import torch.nn as nn\n    from transformers import AutoTokenizer, AutoModel, AutoConfig, PreTrainedModel, AutoModelForSequenceClassification\n    from tqdm.auto import tqdm\n    import transformers\n    transformers.logging.set_verbosity_error()\nexcept ImportError as e: print(f\"FATAL ERROR during imports: {e}\"); raise e\nprint(\"All libraries imported successfully!\")\n\n# === Configuration ===\nLLM_MODEL_NAME_1 = \"desklib/ai-text-detector-v1.01\"\nLLM_MODEL_NAME_2 = \"abhi099k/ai-text-detector-deberta-v3-large-h1\"\nN_SPLITS_CLASSICAL = 5\nN_FEATURES_CLASSICAL = 850\nMAX_LEN_LLM = 512\nRANDOM_SEED = 42\n\n# -------------------------------------------------------------------\n# 0a. CUSTOM LLM CLASS (for Desklib)\n# -------------------------------------------------------------------\nclass DesklibAIDetectionModel(PreTrainedModel):\n    \"\"\"Custom model class for models with this specific head.\"\"\"\n    config_class = AutoConfig\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = AutoModel.from_config(config)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.init_weights()\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = self.model(input_ids, attention_mask=attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n        pooled_output = sum_embeddings / sum_mask\n        logits = self.classifier(pooled_output)\n        loss = None\n        if labels is not None:\n            loss_fct = nn.BCEWithLogitsLoss()\n            loss = loss_fct(logits.view(-1), labels.float())\n        output = {\"logits\": logits}\n        if loss is not None: output[\"loss\"] = loss\n        return output\n\n# -------------------------------------------------------------------\n# 0b. LLM INFERENCE FUNCTION (for Desklib - WITH BUG FIX)\n# -------------------------------------------------------------------\ndef get_desklib_predictions(texts, model_name):\n    \"\"\"Generates predictions using the specified CUSTOM Desklib model class.\"\"\"\n    print(f\"\\n--- Starting LLM Inference ({model_name}) ---\"); model = None; tokenizer = None\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n    try:\n        print(f\"Loading Tokenizer: {model_name}...\"); tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        print(f\"Loading Model (Custom Class): {model_name}...\"); \n        model = DesklibAIDetectionModel.from_pretrained(model_name, trust_remote_code=True)\n        model.to(device); model.eval()\n        print(\"Model and Tokenizer loaded successfully.\")\n    except Exception as e:\n        print(f\"Error loading model/tokenizer {model_name}: {e}\\nFalling back to 0.5.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); return np.full(len(texts), 0.5)\n    \n    print(f\"Running LLM inference on {len(texts)} texts...\")\n    batch_size = 16 # Keep batch size small for 'large' models\n    print(f\"Using batch size: {batch_size}\")\n    \n    llm_probs = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), batch_size), desc=f\"LLM 1 ({model_name.split('/')[1]})\"):\n            batch_texts = texts[i : i + batch_size].tolist();\n            if not batch_texts: continue\n            try:\n                inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN_LLM)\n                input_ids = inputs['input_ids'].to(device)\n                attention_mask = inputs['attention_mask'].to(device)\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                logits = outputs[\"logits\"]\n                probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n                if not isinstance(probs, (np.ndarray, list)): probs = [probs.item()] # Handle single item\n                elif isinstance(probs, np.ndarray): probs = probs.tolist()\n                llm_probs.extend(probs)\n            except Exception as e: \n                print(f\"Error during LLM batch {i // batch_size}: {e}\")\n                if \"out of memory\" in str(e).lower(): print(\"!!! CUDA Out of Memory. Try reducing batch_size. !!!\")\n                llm_probs.extend([0.5] * len(batch_texts))\n            if i % (batch_size * 20) == 0: gc.collect(); torch.cuda.empty_cache()\n            \n    print(f\"LLM 1 ({model_name.split('/')[1]}) prediction feature generated.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); print(\"--- Finished LLM 1 Inference ---\")\n    if len(llm_probs) != len(texts): print(f\"CRITICAL WARNING: Length mismatch {len(llm_probs)} vs {len(texts)}. Padding/Truncating.\"); llm_probs = (llm_probs + [0.5] * len(texts))[:len(texts)]\n    return np.array(llm_probs)\n\n# -------------------------------------------------------------------\n# 0c. LLM INFERENCE FUNCTION (for Generic Classifiers like Abhi)\n# -------------------------------------------------------------------\ndef get_generic_llm_predictions(texts, model_name):\n    \"\"\"Generates predictions using a STANDARD AutoModelForSequenceClassification.\"\"\"\n    print(f\"\\n--- Starting LLM Inference ({model_name}) ---\"); model = None; tokenizer = None\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n    try:\n        print(f\"Loading Tokenizer: {model_name}...\"); tokenizer = AutoTokenizer.from_pretrained(model_name)\n        print(f\"Loading Model (Standard Class): {model_name}...\"); \n        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n        model.to(device); model.eval()\n        print(\"Model and Tokenizer loaded successfully.\")\n        \n        positive_label_id = -1\n        if hasattr(model.config, \"label2id\"):\n            print(f\"Model labels found: {model.config.label2id}\")\n            for name, id_val in model.config.label2id.items():\n                if \"ai\" in name.lower() or \"fake\" in name.lower() or \"generated\" in name.lower() or name == \"LABEL_1\" or name.upper() == \"AI\":\n                    positive_label_id = id_val\n                    print(f\"Determined positive (AI) label ID: {positive_label_id} ('{name}')\")\n                    break\n        if positive_label_id == -1: positive_label_id = 1; print(f\"Could not determine AI label by name. Assuming ID {positive_label_id} is positive.\")\n            \n    except Exception as e:\n        print(f\"Error loading model/tokenizer {model_name}: {e}\\nFalling back to 0.5.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); return np.full(len(texts), 0.5)\n    \n    print(f\"Running LLM inference on {len(texts)} texts...\")\n    batch_size = 16\n    print(f\"Using batch size: {batch_size}\")\n    \n    llm_probs = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), batch_size), desc=f\"LLM 2 ({model_name.split('/')[1]})\"):\n            batch_texts = texts[i : i + batch_size].tolist();\n            if not batch_texts: continue\n            try:\n                inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN_LLM).to(device)\n                outputs = model(**inputs); \n                logits = outputs.logits\n                probs_all = torch.softmax(logits, dim=-1)\n                probs = probs_all[:, positive_label_id].cpu().numpy()\n                if batch_size == 1 and isinstance(probs, (float, np.floating)): probs = [probs]\n                elif isinstance(probs, np.ndarray): probs = probs.tolist()\n                llm_probs.extend(probs)\n            except Exception as e: \n                print(f\"Error during LLM batch {i // batch_size}: {e}\")\n                if \"out of memory\" in str(e).lower(): print(\"!!! CUDA Out of Memory. Try reducing batch_size. !!!\")\n                llm_probs.extend([0.5] * len(batch_texts))\n            if i % (batch_size * 20) == 0: gc.collect(); torch.cuda.empty_cache()\n            \n    print(f\"LLM 2 ({model_name.split('/')[1]}) prediction feature generated.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); print(\"--- Finished LLM 2 Inference ---\")\n    if len(llm_probs) != len(texts): print(f\"CRITICAL WARNING: Length mismatch {len(llm_probs)} vs {len(texts)}. Padding/Truncating.\"); llm_probs = (llm_probs + [0.5] * len(texts))[:len(texts)]\n    return np.array(llm_probs)\n\n\n# -------------------------------------------------------------------\n# 1. CHAMPION FEATURE EXTRACTOR (Handles BOTH LLM preds)\n# -------------------------------------------------------------------\nclass ChampionFeatureExtractor:\n    \"\"\"Extracts base features, includes llm_preds, + new features.\"\"\"\n    def __init__(self):\n        self.ai_connectors = ['in conclusion', 'in summary', 'furthermore', 'moreover', 'additionally', 'however', 'therefore', 'thus', 'consequently', 'as a result', 'on the other hand', 'for instance', 'for example', 'it is important to note', 'it is worth noting', 'that being said']; self.formal_words = ['utilize', 'facilitate', 'implement', 'methodology', 'paradigm', 'leverage', 'robust', 'optimal', 'enhance', 'demonstrate', 'comprehensive', 'articulate']; self.hedging_words = ['may', 'might', 'could', 'possibly', 'perhaps', 'suggests', 'seems', 'appears', 'likely']; self.passive_indicators = ['is made', 'was made', 'is given', 'was given', 'is shown', 'was shown', 'is considered', 'was considered']\n        self.stopwords = set(ENGLISH_STOP_WORDS); self.sentence_splitter = re.compile(r'[.!?]+')\n    def _get_sent_len_var(self, text):\n        sentences = self.sentence_splitter.split(text); sentence_lengths = [len(s.split()) for s in sentences if len(s.split()) > 0]\n        if len(sentence_lengths) > 1: return np.var(sentence_lengths)\n        else: return 0\n    def extract_base_features(self, df):\n        features = pd.DataFrame(index=df.index);\n        if 'llm_pred_desklib' in df.columns: features['llm_pred_desklib'] = df['llm_pred_desklib']\n        if 'llm_pred_abhi' in df.columns: features['llm_pred_abhi'] = df['llm_pred_abhi']\n        answers = df['answer'].fillna(''); words = answers.str.split(); word_counts = words.str.len()\n        features['text_length'] = answers.str.len(); features['word_count'] = word_counts; features['avg_word_length'] = features['text_length'] / (features['word_count'] + 1e-6); features['sentence_count'] = answers.str.count(r'[.!?]+'); features['avg_sentence_length'] = features['word_count'] / (features['sentence_count'] + 1e-6); features['comma_count'] = answers.str.count(','); features['period_count'] = answers.str.count(r'\\.'); features['exclamation_count'] = answers.str.count(r'!'); features['question_count'] = answers.str.count(r'\\?'); features['punctuation_ratio'] = (features['comma_count'] + features['period_count']) / (features['word_count'] + 1e-6); features['unique_words'] = answers.apply(lambda x: len(set(str(x).lower().split()))); features['ttr'] = features['unique_words'] / (features['word_count'] + 1e-6)\n        def count_phrases(text, phrases): text_lower = str(text).lower(); return sum(1 for phrase in phrases if phrase in text_lower)\n        features['ai_connector_count'] = answers.apply(lambda x: count_phrases(x, self.ai_connectors)); features['formal_word_count'] = answers.apply(lambda x: count_phrases(x, self.formal_words)); features['passive_voice_count'] = answers.apply(lambda x: count_phrases(x, self.passive_indicators)); features['hedging_word_count'] = answers.apply(lambda x: count_phrases(x, self.hedging_words)); features['ai_connector_ratio'] = features['ai_connector_count'] / (features['word_count'] + 1e-6); features['formal_word_ratio'] = features['formal_word_count'] / (features['word_count'] + 1e-6); features['passive_voice_ratio'] = features['passive_voice_count'] / (features['word_count'] + 1e-6); features['hedging_ratio'] = features['hedging_word_count'] / (features['word_count'] + 1e-6)\n        features['subordinate_ratio'] = answers.str.count(r'\\b(that|which|who|when|where|while|although|because|if)\\b') / (features['word_count'] + 1e-6); features['paragraph_count'] = answers.str.count(r'\\n\\n') + 1; features['avg_paragraph_len'] = features['word_count'] / (features['paragraph_count'] + 1e-6); features['word_length_std'] = answers.apply(lambda x: np.std([len(w) for w in str(x).split()]) if len(str(x).split()) > 1 else 0)\n        features['em_dash_count'] = answers.str.count('â€”'); features['em_dash_ratio'] = features['em_dash_count'] / (features['word_count'] + 1e-6)\n        features['question_mark_count'] = answers.str.count(r'\\?'); features['exclamation_mark_count'] = answers.str.count(r'!')\n        features['stopword_count'] = words.apply(lambda x: sum(1 for word in x if word.lower() in self.stopwords)); features['stopword_ratio'] = features['stopword_count'] / (features['word_count'] + 1e-6)\n        features['uppercase_word_count'] = words.apply(lambda x: sum(1 for word in x if word.isascii() and word.istitle())); features['uppercase_ratio'] = features['uppercase_word_count'] / (features['word_count'] + 1e-6)\n        features['sentence_length_variance'] = answers.apply(self._get_sent_len_var)\n        features.replace([np.inf, -np.inf], 0, inplace=True); features.fillna(0, inplace=True); return features\n\n# -------------------------------------------------------------------\n# 2. CHAMPION AI DETECTOR (L2 Stack model - Handles 2 LLM feats)\n# -------------------------------------------------------------------\nclass ChampionAIDetector: \n    \"\"\"Trains L2 Stack (LGBM+CatBoost+Ridge) using SelectKBest feats including 2 llm_preds.\"\"\"\n    def __init__(self, n_folds=N_SPLITS_CLASSICAL, n_features=N_FEATURES_CLASSICAL):\n        self.feature_extractor = ChampionFeatureExtractor(); self.n_folds = n_folds; self.n_features = n_features; self.models = {}; self.meta_model = None; self.calibrator = IsotonicRegression(out_of_bounds='clip'); self.scaler = StandardScaler(); self.tfidf_word = None; self.tfidf_char = None; self.selector = None; self.topic_columns = None; self.is_trained = False; print(f\"\\nClassical Detector (L2 Stack): {n_folds} folds, SelectKBest(k={n_features}).\")\n    def _get_consistent_topic_dummies(self, series, fit_columns=False):\n        # (Unchanged)\n        series = series.fillna(\"Unknown_Topic\"); dummies = pd.get_dummies(series, prefix='topic', dtype=int, dummy_na=False)\n        if fit_columns:\n            self.topic_columns = dummies.columns\n            if \"topic_Unknown_Topic\" not in self.topic_columns and series.astype(str).str.contains(\"Unknown_Topic\").any(): self.topic_columns = self.topic_columns.append(pd.Index([\"topic_Unknown_Topic\"]))\n            print(f\"Identified {len(self.topic_columns)} topic columns during fit.\"); return dummies.reindex(columns=self.topic_columns, fill_value=0)\n        elif self.topic_columns is not None: return dummies.reindex(columns=self.topic_columns, fill_value=0)\n        else: raise ValueError(\"Topic columns not fitted.\")\n    def prepare_champion_features(self, train_df, test_df, is_training_fold=False, is_full_training=False):\n        \"\"\"Prepares features. Fits objects if training. Separates llm_preds.\"\"\"\n        print(\"Extracting base handcrafted + LLM features...\"); train_base_features_df = self.feature_extractor.extract_base_features(train_df); test_base_features_df = self.feature_extractor.extract_base_features(test_df)\n        print(\"Processing topic features...\")\n        fit_topic_cols = (self.topic_columns is None) or is_full_training\n        train_topic_dummies = self._get_consistent_topic_dummies(train_df['topic'], fit_columns=fit_topic_cols); test_topic_dummies = self._get_consistent_topic_dummies(test_df['topic'], fit_columns=False)\n        \n        # --- SEPARATE BOTH LLM FEATURES ---\n        llm_train_feat_1 = train_base_features_df.pop('llm_pred_desklib').values.reshape(-1, 1)\n        llm_test_feat_1 = test_base_features_df.pop('llm_pred_desklib').values.reshape(-1, 1)\n        llm_train_feat_2 = train_base_features_df.pop('llm_pred_abhi').values.reshape(-1, 1)\n        llm_test_feat_2 = test_base_features_df.pop('llm_pred_abhi').values.reshape(-1, 1)\n\n        train_dense_features_df = pd.concat([train_base_features_df, train_topic_dummies], axis=1); test_dense_features_df = pd.concat([test_base_features_df, test_topic_dummies], axis=1)\n        test_dense_features_df = test_dense_features_df.reindex(columns=train_dense_features_df.columns, fill_value=0)\n        if is_training_fold or is_full_training: print(\"Fitting/Transforming dense features (excl. LLM)...\"); train_features_scaled = self.scaler.fit_transform(train_dense_features_df); test_features_scaled = self.scaler.transform(test_dense_features_df)\n        else: print(\"Transforming dense features (excl. LLM)...\"); train_features_scaled = self.scaler.transform(train_dense_features_df); test_features_scaled = self.scaler.transform(test_dense_features_df)\n        if is_training_fold or is_full_training:\n            print(\"Fitting Word TF-IDF...\"); self.tfidf_word = TfidfVectorizer(ngram_range=(1, 3), min_df=3, max_df=0.9, sublinear_tf=True, stop_words='english', max_features=2500)\n            train_tfidf_word = self.tfidf_word.fit_transform(train_df['answer'].fillna('')); test_tfidf_word = self.tfidf_word.transform(test_df['answer'].fillna(''))\n            print(\"Fitting Char TF-IDF...\"); self.tfidf_char = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 5), min_df=3, sublinear_tf=True, max_features=1250)\n            train_tfidf_char = self.tfidf_char.fit_transform(train_df['answer'].fillna('')); test_tfidf_char = self.tfidf_char.transform(test_df['answer'].fillna(''))\n        else:\n            if not self.tfidf_word or not self.tfidf_char: raise ValueError(\"TF-IDF vectorizers not fitted.\")\n            print(\"Transforming text using fitted TF-IDF...\"); train_tfidf_word = self.tfidf_word.transform(train_df['answer'].fillna('')); test_tfidf_word = self.tfidf_word.transform(test_df['answer'].fillna(''))\n            train_tfidf_char = self.tfidf_char.transform(train_df['answer'].fillna('')); test_tfidf_char = self.tfidf_char.transform(test_df['answer'].fillna(''))\n        \n        # --- COMBINE ALL FEATURES (incl. BOTH LLM preds) ---\n        print(\"Combining all feature sets...\"); \n        X_train_full = hstack([csr_matrix(train_features_scaled), train_tfidf_word, train_tfidf_char, csr_matrix(llm_train_feat_1), csr_matrix(llm_train_feat_2)]).tocsr()\n        X_test_full = hstack([csr_matrix(test_features_scaled), test_tfidf_word, test_tfidf_char, csr_matrix(llm_test_feat_1), csr_matrix(llm_test_feat_2)]).tocsr()\n        \n        print(f\"Shape before feature selection: Train={X_train_full.shape}, Test={X_test_full.shape}\")\n        if is_training_fold or is_full_training:\n            y_train = train_df['is_cheating'].values; print(f\"Fitting SelectKBest (k={self.n_features})...\"); self.selector = SelectKBest(f_classif, k=min(self.n_features, X_train_full.shape[1]))\n            with warnings.catch_warnings(): warnings.filterwarnings('ignore'); self.selector.fit(X_train_full, y_train)\n            X_train_selected = self.selector.transform(X_train_full); X_test_selected = self.selector.transform(X_test_full)\n        else:\n            if not self.selector: raise ValueError(\"SelectKBest not fitted.\"); print(f\"Transforming features using fitted SelectKBest (k={self.n_features})...\")\n            X_train_selected = self.selector.transform(X_train_full); X_test_selected = self.selector.transform(X_test_full)\n        X_train_final = X_train_selected.toarray(); X_test_final = X_test_selected.toarray()\n        print(f\"Final training feature shape: {X_train_final.shape}\"); print(f\"Final testing feature shape: {X_test_final.shape}\")\n        return X_train_final, X_test_final\n    \n    # --- L2 STACKING (Using v25.3 params) ---\n    def train_champion_ensemble(self, full_train_df):\n        \"\"\"Trains the L2 Stacking ensemble. Fits Scaler/TFIDF/Selector on first fold.\"\"\"\n        y_train = full_train_df['is_cheating'].values\n        print(f\"\\n--- Starting Classical Ensemble Training ---\"); print(f\"Training set size: {len(full_train_df)}\"); print(f\"Positive class ratio: {y_train.mean():.6f}\")\n        skf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=RANDOM_SEED)\n        oof_preds = {'lgb': np.zeros(len(full_train_df)), 'cat': np.zeros(len(full_train_df)), 'ridge': np.zeros(len(full_train_df))}\n        print(\"Fitting topic columns on full training data...\"); _ = self._get_consistent_topic_dummies(full_train_df['topic'], fit_columns=True)\n        print(\"\\nFitting Scalers, TFIDF, and Selector on FULL training data...\")\n        _ = self.prepare_champion_features(full_train_df, full_train_df, is_training_fold=True, is_full_training=True) # Fit preprocessing objects\n        print(\"\\nStarting K-Fold Training...\")\n        for fold, (train_idx, val_idx) in enumerate(skf.split(full_train_df, y_train)):\n            print(f\"\\n{'='*20} Classical Fold {fold+1}/{self.n_folds} {'='*20}\")\n            train_fold_df, val_fold_df = full_train_df.iloc[train_idx], full_train_df.iloc[val_idx]\n            y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n            X_train_fold, X_val_fold = self.prepare_champion_features(train_fold_df, val_fold_df, is_training_fold=False, is_full_training=False)\n            print(f\"Fold {fold+1}: Train shape {X_train_fold.shape}, Val shape {X_val_fold.shape}\")\n            fold_seed = RANDOM_SEED + fold\n            print(\"Training LightGBM...\"); lgb_model = lgb.LGBMClassifier(n_estimators=2500, learning_rate=0.01, max_depth=7, num_leaves=31, min_child_samples=20, subsample=0.7, colsample_bytree=0.6, reg_alpha=0.4, reg_lambda=0.4, random_state=fold_seed, verbose=-1, n_jobs=-1)\n            lgb_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], callbacks=[lgb.early_stopping(200, verbose=False)]); oof_preds['lgb'][val_idx] = lgb_model.predict_proba(X_val_fold)[:, 1]\n            print(\"Training CatBoost...\"); cat_model = CatBoostClassifier(iterations=2500, learning_rate=0.02, depth=6, l2_leaf_reg=5, random_seed=fold_seed, verbose=0, early_stopping_rounds=200, task_type=\"CPU\")\n            cat_model.fit(X_train_fold, y_train_fold, eval_set=(X_val_fold, y_val_fold), verbose=False); oof_preds['cat'][val_idx] = cat_model.predict_proba(X_val_fold)[:, 1]\n            print(\"Training Ridge Classifier...\"); ridge_scaler = StandardScaler(); X_train_fold_scaled = ridge_scaler.fit_transform(X_train_fold); X_val_fold_scaled = ridge_scaler.transform(X_val_fold)\n            ridge_model = RidgeClassifier(alpha=1.0, random_state=fold_seed); ridge_model.fit(X_train_fold_scaled, y_train_fold); ridge_scores = ridge_model.decision_function(X_val_fold_scaled); ridge_probs = 1 / (1 + np.exp(-ridge_scores))\n            oof_preds['ridge'][val_idx] = ridge_probs\n            fold_auc_lgb = roc_auc_score(y_val_fold, oof_preds['lgb'][val_idx]); fold_auc_cat = roc_auc_score(y_val_fold, oof_preds['cat'][val_idx]); fold_auc_ridge = roc_auc_score(y_val_fold, oof_preds['ridge'][val_idx])\n            print(f\"Fold {fold+1} LGB AUC: {fold_auc_lgb:.6f}\"); print(f\"Fold {fold+1} CAT AUC: {fold_auc_cat:.6f}\"); print(f\"Fold {fold+1} RIDGE AUC: {fold_auc_ridge:.6f}\")\n        print(\"\\n\" + \"=\"*50); print(\"Classical L1 Model OOF Scores:\"); [print(f\"  {name.upper()} OOF AUC: {roc_auc_score(y_train, preds):.8f}\") for name, preds in oof_preds.items()]\n        print(\"\\nTraining Classical L2 Meta-Model...\"); X_meta_train = np.stack(list(oof_preds.values()), axis=1)\n        self.meta_model = LogisticRegression(C=0.3, solver='liblinear', random_state=RANDOM_SEED); self.meta_model.fit(X_meta_train, y_train)\n        oof_ensemble_preds = self.meta_model.predict_proba(X_meta_train)[:, 1]; oof_ensemble_auc = roc_auc_score(y_train, oof_ensemble_preds)\n        print(f\"\\nCLASSICAL L2 STACKING OOF AUC: {oof_ensemble_auc:.8f}\"); print(\"Training Classical Isotonic Calibrator...\"); self.calibrator.fit(oof_ensemble_preds, y_train)\n        self.is_trained = True; return oof_ensemble_auc, oof_ensemble_preds\n    def predict_champion(self, full_train_df, test_df):\n        \"\"\"Generates final predictions using fitted classical components.\"\"\"\n        if not self.is_trained: raise ValueError(\"Classical model not trained!\")\n        print(\"\\n\" + \"=\"*50); print(\"Generating Final Classical Ensemble Predictions...\"); print(\"=\"*50)\n        print(\"Preparing features for final classical model training and prediction...\")\n        X_train_full, X_test = self.prepare_champion_features(full_train_df, test_df, is_training_fold=False, is_full_training=True) # Use is_full_training=True\n        y_train_full = full_train_df['is_cheating'].values; test_preds = {}\n        print(\"Training final LightGBM...\"); lgb_final = lgb.LGBMClassifier(n_estimators=2000, learning_rate=0.01, max_depth=7, num_leaves=31, min_child_samples=20, subsample=0.7, colsample_bytree=0.6, reg_alpha=0.4, reg_lambda=0.4, random_state=RANDOM_SEED, verbose=-1, n_jobs=-1)\n        lgb_final.fit(X_train_full, y_train_full); test_preds['lgb'] = lgb_final.predict_proba(X_test)[:, 1]\n        print(\"Training final CatBoost...\"); cat_final = CatBoostClassifier(iterations=2200, learning_rate=0.02, depth=6, l2_leaf_reg=5, random_seed=RANDOM_SEED, verbose=0, task_type=\"CPU\")\n        cat_final.fit(X_train_full, y_train_full); test_preds['cat'] = cat_final.predict_proba(X_test)[:, 1]\n        print(\"Training final Ridge...\"); ridge_scaler = StandardScaler(); X_train_full_scaled = ridge_scaler.fit_transform(X_train_full); X_test_scaled = ridge_scaler.transform(X_test)\n        ridge_final = RidgeClassifier(alpha=1.0, random_state=RANDOM_SEED); ridge_final.fit(X_train_full_scaled, y_train_full)\n        ridge_scores = ridge_final.decision_function(X_test_scaled); test_preds['ridge'] = 1 / (1 + np.exp(-ridge_scores))\n        print(\"Applying Classical L2 Meta-Model...\"); X_meta_test = np.stack(list(test_preds.values()), axis=1)\n        final_proba = self.meta_model.predict_proba(X_meta_test)[:, 1]\n        print(\"Applying Classical Isotonic Calibration...\"); calibrated_proba = self.calibrator.transform(final_proba)\n        calibrated_proba = np.clip(calibrated_proba, 0.001, 0.999)\n        return calibrated_proba, final_proba\n\n# --- (Fallback model - unchanged) ---\ndef champion_fallback():\n    \"\"\"Fallback using basic LGBM, basic features.\"\"\"\n    print(\"CRITICAL ERROR: Main pipeline failed. Using CHAMPION FALLBACK model...\")\n    try:\n        from sklearn.preprocessing import StandardScaler; from scipy.sparse import hstack, csr_matrix;\n        train_df = pd.read_csv('/kaggle/input/mercor-ai-detection/train.csv'); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv')\n        feature_extractor = ChampionFeatureExtractor(); train_base_features = feature_extractor.extract_base_features(train_df.drop(columns=['llm_pred_desklib', 'llm_pred_abhi'], errors='ignore'))\n        test_base_features = feature_extractor.extract_base_features(test_df.drop(columns=['llm_pred_desklib', 'llm_pred_abhi'], errors='ignore'))\n        train_topic_dummies = pd.get_dummies(train_df['topic'], prefix='topic', dtype=int); fallback_topic_columns = train_topic_dummies.columns; test_topic_dummies = pd.get_dummies(test_df['topic'], prefix='topic', dtype=int).reindex(columns=fallback_topic_columns, fill_value=0)\n        train_dense_features = pd.concat([train_base_features, train_topic_dummies], axis=1); test_dense_features = pd.concat([test_base_features, test_topic_dummies], axis=1).reindex(columns=train_dense_features.columns, fill_value=0)\n        scaler = StandardScaler(); train_dense_scaled = scaler.fit_transform(train_dense_features); test_dense_scaled = scaler.transform(test_dense_features)\n        tfidf = TfidfVectorizer(max_features=2500, ngram_range=(1, 3), stop_words='english'); train_tfidf = tfidf.fit_transform(train_df['answer'].fillna('')); test_tfidf = tfidf.transform(test_df['answer'].fillna(''))\n        X_train = hstack([csr_matrix(train_dense_scaled), train_tfidf]).tocsr(); X_test = hstack([csr_matrix(test_dense_scaled), test_tfidf]).tocsr(); y_train = train_df['is_cheating']\n        model = lgb.LGBMClassifier(n_estimators=1000, learning_rate=0.01, max_depth=7, num_leaves=63, random_state=RANDOM_SEED, verbose=-1, n_jobs=-1, reg_alpha=0.1, reg_lambda=0.1)\n        model.fit(X_train, y_train); test_proba = model.predict_proba(X_test.toarray())[:, 1]\n        submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': test_proba}); submission.to_csv('submission.csv', index=False, float_format='%.10f')\n        print(\"Fallback submission.csv created successfully.\"); return submission\n    except Exception as e:\n        print(f\"Fallback model ALSO failed: {e}\"); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv'); submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': 0.5}); submission.to_csv('submission.csv', index=False, float_format='%.10f'); print(\"CRITICAL: Created dummy submission.csv with 0.5 probability.\"); return submission\n\n# -------------------------------------------------------------------\n# 4. MAIN EXECUTION (Calibrates BOTH LLM Preds, uses L2 Stack)\n# -------------------------------------------------------------------\ndef champion_main():\n    \"\"\"Main function: Runs 2 LLM inferences, calibrates features, runs classical L2 stack.\"\"\"\n    print(\"=\" * 60); print(f\"  Mercor AI Text Detection - CHAMPION v27 (Dual LLM + L2 Stack)\"); print(\"=\" * 60)\n    train_df = pd.read_csv('/kaggle/input/mercor-ai-detection/train.csv'); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv')\n    print(f\"Training data shape: {train_df.shape}\"); print(f\"Test data shape: {test_df.shape}\")\n    train_df['answer'] = train_df['answer'].fillna(''); test_df['answer'] = test_df['answer'].fillna('')\n    all_texts = pd.concat([train_df['answer'], test_df['answer']])\n    \n    # --- Step 1A: Generate Desklib predictions ---\n    llm_preds_all_1 = get_desklib_predictions(all_texts, LLM_MODEL_NAME_1)\n    llm_oof_preds_1 = llm_preds_all_1[:len(train_df)]; llm_test_preds_1 = llm_preds_all_1[len(train_df):]\n    print(\"\\nCalibrating Desklib predictions...\"); llm_calibrator_1 = IsotonicRegression(out_of_bounds='clip')\n    llm_calibrator_1.fit(llm_oof_preds_1, train_df['is_cheating'].values)\n    llm_oof_calibrated_1 = llm_calibrator_1.transform(llm_oof_preds_1)\n    # --- *** THE FIX IS HERE *** ---\n    llm_test_calibrated_1 = llm_calibrator_1.transform(llm_test_preds_1) # Fixed typo 'lll_calibrator_1'\n    # --- *** END FIX *** ---\n    print(f\"Calibrated Desklib OOF AUC: {roc_auc_score(train_df['is_cheating'], llm_oof_calibrated_1):.8f}\")\n    train_df['llm_pred_desklib'] = llm_oof_calibrated_1\n    test_df['llm_pred_desklib'] = llm_test_calibrated_1\n    print(\"CALIBRATED Desklib feature added.\")\n\n    # --- Step 1B: Generate Abhi predictions ---\n    llm_preds_all_2 = get_generic_llm_predictions(all_texts, LLM_MODEL_NAME_2) # Use the generic loader\n    llm_oof_preds_2 = llm_preds_all_2[:len(train_df)]; llm_test_preds_2 = llm_preds_all_2[len(train_df):]\n    print(\"\\nCalibrating Abhi predictions...\"); llm_calibrator_2 = IsotonicRegression(out_of_bounds='clip')\n    llm_calibrator_2.fit(llm_oof_preds_2, train_df['is_cheating'].values)\n    llm_oof_calibrated_2 = llm_calibrator_2.transform(llm_oof_preds_2)\n    llm_test_calibrated_2 = llm_calibrator_2.transform(llm_test_preds_2)\n    print(f\"Calibrated Abhi OOF AUC: {roc_auc_score(train_df['is_cheating'], llm_oof_calibrated_2):.8f}\")\n    train_df['llm_pred_abhi'] = llm_oof_calibrated_2\n    test_df['llm_pred_abhi'] = llm_test_calibrated_2\n    print(\"CALIBRATED Abhi feature added.\")\n    \n    # --- Step 2: Run Classical L2 Stack Ensemble ---\n    classical_detector = ChampionAIDetector(n_folds=N_SPLITS_CLASSICAL, n_features=N_FEATURES_CLASSICAL) # L2 Stack class\n    oof_auc_classical, oof_preds_classical = classical_detector.train_champion_ensemble(train_df)\n    print(\"\\nStarting Final Test Set Prediction using L2 Stack...\");\n    calibrated_proba, base_proba = classical_detector.predict_champion(train_df, test_df)\n    \n    # Create submission files\n    submission_calibrated = pd.DataFrame({'id': test_df['id'], 'is_cheating': calibrated_proba}); submission_base = pd.DataFrame({'id': test_df['id'], 'is_cheating': base_proba})\n    submission_calibrated.to_csv('submission.csv', index=False, float_format='%.10f'); submission_base.to_csv('submission_base_uncalibrated.csv', index=False, float_format='%.10f')\n    # Final Output\n    print(\"\\n\" + \"=\"*60); print(\"CHAMPION PIPELINE COMPLETED SUCCESSFULLY!\");\n    print(f\"Final L2 Stack OOF AUC (with DUAL Calibrated LLM Feats): {oof_auc_classical:.8f}\")\n    print(\"\\nSubmission files created:\"); print(f\"  1. submission.csv (RECOMMENDED)\"); print(f\"  2. submission_base_uncalibrated.csv (Backup)\")\n    print(f\"\\nFinal 'submission.csv' prediction statistics:\"); print(f\"  Min: {calibrated_proba.min():.6f}\"); print(f\"  Max: {calibrated_proba.max():.6f}\"); print(f\"  Mean: {calibrated_proba.mean():.6f}\"); print(f\"  Median: {np.median(calibrated_proba):.6f}\")\n    print(\"\\nTop 10 Predictions:\"); print(submission_calibrated.head(10).to_string(index=False)); print(\"=\" * 60)\n\n# --- Run the Champion Pipeline ---\nif __name__ == \"__main__\":\n    start_time = time.time()\n    try:\n        gc.collect(); torch.cuda.empty_cache()\n        champion_main()\n    except Exception as e:\n        print(f\"\\n!!! AN ERROR OCCURRED IN THE MAIN PIPELINE !!!: {e}\")\n        import traceback; traceback.print_exc()\n        champion_fallback()\n    finally:\n        gc.collect(); torch.cuda.empty_cache(); end_time = time.time()\n        print(f\"\\nTotal execution time: {(end_time - start_time) / 60:.2f} minutes\")","metadata":{"execution":{"iopub.status.busy":"2025-11-01T15:56:12.915972Z","iopub.execute_input":"2025-11-01T15:56:12.916340Z","iopub.status.idle":"2025-11-01T16:01:38.722889Z","shell.execute_reply.started":"2025-11-01T15:56:12.916314Z","shell.execute_reply":"2025-11-01T16:01:38.722255Z"}}},{"cell_type":"markdown","source":"Installing/Updating transformers...0.98878\nLibraries installed.\nAll libraries imported successfully!\n============================================================\n  Mercor AI Text Detection - CHAMPION v27 (Dual LLM + L2 Stack)\n============================================================\nTraining data shape: (269, 4)\nTest data shape: (264, 3)\n\n--- Starting LLM Inference (desklib/ai-text-detector-v1.01) ---\nUsing device: cuda\nLoading Tokenizer: desklib/ai-text-detector-v1.01...\nLoading Model (Custom Class): desklib/ai-text-detector-v1.01...\nModel and Tokenizer loaded successfully.\nRunning LLM inference on 533 texts...\nUsing batch size: 16\nLLMâ€‡1â€‡(ai-text-detector-v1.01):â€‡100%\nâ€‡34/34â€‡[00:57<00:00,â€‡â€‡1.45s/it]\nLLM 1 (ai-text-detector-v1.01) prediction feature generated.\n--- Finished LLM 1 Inference ---\n\nCalibrating Desklib predictions...\nCalibrated Desklib OOF AUC: 0.96113527\nCALIBRATED Desklib feature added.\n\n--- Starting LLM Inference (abhi099k/ai-text-detector-deberta-v3-large-h1) ---\nUsing device: cuda\nLoading Tokenizer: abhi099k/ai-text-detector-deberta-v3-large-h1...\nLoading Model (Standard Class): abhi099k/ai-text-detector-deberta-v3-large-h1...\nModel and Tokenizer loaded successfully.\nModel labels found: {'LABEL_0': 0, 'LABEL_1': 1}\nDetermined positive (AI) label ID: 1 ('LABEL_1')\nRunning LLM inference on 533 texts...\nUsing batch size: 16\nLLMâ€‡2â€‡(ai-text-detector-deberta-v3-large-h1):â€‡100%\nâ€‡34/34â€‡[01:01<00:00,â€‡â€‡1.55s/it]\nLLM 2 (ai-text-detector-deberta-v3-large-h1) prediction feature generated.\n--- Finished LLM 2 Inference ---\n\nCalibrating Abhi predictions...\nCalibrated Abhi OOF AUC: 0.51463700\nCALIBRATED Abhi feature added.\n\nClassical Detector (L2 Stack): 5 folds, SelectKBest(k=850).\n\n--- Starting Classical Ensemble Training ---\nTraining set size: 269\nPositive class ratio: 0.546468\nFitting topic columns on full training data...\nIdentified 268 topic columns during fit.\n\nFitting Scalers, TFIDF, and Selector on FULL training data...\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nIdentified 268 topic columns during fit.\nFitting/Transforming dense features (excl. LLM)...\nFitting Word TF-IDF...\nFitting Char TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(269, 3684), Test=(269, 3684)\nFitting SelectKBest (k=850)...\nFinal training feature shape: (269, 850)\nFinal testing feature shape: (269, 850)\n\nStarting K-Fold Training...\n\n==================== Classical Fold 1/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3684), Test=(54, 3684)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 1: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 1 LGB AUC: 0.997222\nFold 1 CAT AUC: 0.998611\nFold 1 RIDGE AUC: 0.986111\n\n==================== Classical Fold 2/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3684), Test=(54, 3684)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 2: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 2 LGB AUC: 1.000000\nFold 2 CAT AUC: 1.000000\nFold 2 RIDGE AUC: 0.998611\n\n==================== Classical Fold 3/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3684), Test=(54, 3684)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 3: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 3 LGB AUC: 0.965517\nFold 3 CAT AUC: 0.961379\nFold 3 RIDGE AUC: 0.982069\n\n==================== Classical Fold 4/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3684), Test=(54, 3684)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 4: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 4 LGB AUC: 0.968276\nFold 4 CAT AUC: 0.983448\nFold 4 RIDGE AUC: 0.976552\n\n==================== Classical Fold 5/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(216, 3684), Test=(53, 3684)\nFinal training feature shape: (216, 850)\nFinal testing feature shape: (53, 850)\nFold 5: Train shape (216, 850), Val shape (53, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 5 LGB AUC: 0.987069\nFold 5 CAT AUC: 0.994253\nFold 5 RIDGE AUC: 0.979885\n\n==================================================\nClassical L1 Model OOF Scores:\n  LGB OOF AUC: 0.98488904\n  CAT OOF AUC: 0.98695216\n  RIDGE OOF AUC: 0.98165496\n\nTraining Classical L2 Meta-Model...\n\nCLASSICAL L2 STACKING OOF AUC: 0.98895952\nTraining Classical Isotonic Calibrator...\n\nStarting Final Test Set Prediction using L2 Stack...\n\n==================================================\nGenerating Final Classical Ensemble Predictions...\n==================================================\nPreparing features for final classical model training and prediction...\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nIdentified 268 topic columns during fit.\nFitting/Transforming dense features (excl. LLM)...\nFitting Word TF-IDF...\nFitting Char TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(269, 3684), Test=(264, 3684)\nFitting SelectKBest (k=850)...\nFinal training feature shape: (269, 850)\nFinal testing feature shape: (264, 850)\nTraining final LightGBM...\nTraining final CatBoost...\nTraining final Ridge...\nApplying Classical L2 Meta-Model...\nApplying Classical Isotonic Calibration...\n\n============================================================\nCHAMPION PIPELINE COMPLETED SUCCESSFULLY!\nFinal L2 Stack OOF AUC (with DUAL Calibrated LLM Feats): 0.98895952\n\nSubmission files created:\n  1. submission.csv (RECOMMENDED)\n  2. submission_base_uncalibrated.csv (Backup)\n\nFinal 'submission.csv' prediction statistics:\n  Min: 0.001000\n  Max: 0.999000\n  Mean: 0.552944\n  Median: 0.969697\n\nTop 10 Predictions:\n              id  is_cheating\nscr_81822029c661     0.909091\nscr_52efb19e0ea9     0.999000\nscr_8fc0f33c559e     0.055556\nscr_bac3f5d3aa12     0.001000\nscr_adfbe009984d     0.001000\nscr_9e08ece19277     0.999000\nscr_0e34514f3cd4     0.969697\nscr_b10d808b5528     0.999000\nscr_2024f1e7bf94     0.173913\nscr_aa0b11f10fff     0.001000\n============================================================\n\nTotal execution time: 5.37 minutes","metadata":{}},{"cell_type":"code","source":"d=pd.read_csv(\"/kaggle/working/submission.csv\")\nd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nMercor AI Text Detection - 0.99+ AUC CHAMPION VERSION\n=====================================================\n\nv27.2: Single *Working* LLM + Max Regularization\n- FIXES the 'llm_oof_calibrated_1' NameError.\n- Uses 'desklib/ai-text-detector-v1.01' loaded with the custom class.\n- KEEPS the RepeatedStratifiedKFold (5 splits, 3 repeats).\n- KEEPS all v25.3 handcrafted features (em dash, stopwords, etc.).\n- KEEPS SelectKBest(k=850).\n- KEEPS L2 Stack (LGBM, CatBoost, Ridge, Meta-LR).\n- INCREASES regularization on all models.\n\"\"\"\n\n# === General Imports ===\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport gc\nimport warnings\nimport time\nimport os\n\n# === Sklearn Imports ===\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold # --- Use Repeated ---\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.utils import shuffle\n\n# === GBDT Imports ===\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\n# === Hugging Face Imports ===\nprint(\"Installing/Updating transformers...\"); os.system('pip install -q transformers torch'); print(\"Libraries installed.\")\ntry:\n    import torch\n    import torch.nn as nn\n    from transformers import AutoTokenizer, AutoModel, AutoConfig, PreTrainedModel, AutoModelForSequenceClassification\n    from tqdm.auto import tqdm\n    import transformers\n    transformers.logging.set_verbosity_error()\nexcept ImportError as e: print(f\"FATAL ERROR during imports: {e}\"); raise e\nprint(\"All libraries imported successfully!\")\n\n# === Configuration ===\nLLM_MODEL_NAME = \"desklib/ai-text-detector-v1.01\" \nN_SPLITS_CLASSICAL = 5\nN_REPEATS_CLASSICAL = 3\nN_FEATURES_CLASSICAL = 850\nMAX_LEN_LLM = 512\nRANDOM_SEED = 42\n\n# -------------------------------------------------------------------\n# 0a. CUSTOM LLM CLASS (for Desklib)\n# -------------------------------------------------------------------\nclass DesklibAIDetectionModel(PreTrainedModel):\n    \"\"\"Custom model class for models with this specific head.\"\"\"\n    config_class = AutoConfig\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = AutoModel.from_config(config)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.init_weights()\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = self.model(input_ids, attention_mask=attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n        pooled_output = sum_embeddings / sum_mask\n        logits = self.classifier(pooled_output)\n        loss = None\n        if labels is not None:\n            loss_fct = nn.BCEWithLogitsLoss()\n            loss = loss_fct(logits.view(-1), labels.float())\n        output = {\"logits\": logits}\n        if loss is not None: output[\"loss\"] = loss\n        return output\n\n# -------------------------------------------------------------------\n# 0b. LLM INFERENCE FUNCTION (for Desklib - WITH BUG FIX)\n# -------------------------------------------------------------------\ndef get_llm_predictions(texts, model_name): # Renamed\n    \"\"\"Generates predictions using the specified CUSTOM Desklib model class.\"\"\"\n    print(f\"\\n--- Starting LLM Inference ({model_name}) ---\"); model = None; tokenizer = None\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n    try:\n        print(f\"Loading Tokenizer: {model_name}...\"); tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        print(f\"Loading Model (Custom Class): {model_name}...\"); \n        model = DesklibAIDetectionModel.from_pretrained(model_name, trust_remote_code=True)\n        model.to(device); model.eval()\n        print(\"Model and Tokenizer loaded successfully.\")\n    except Exception as e:\n        print(f\"Error loading model/tokenizer {model_name}: {e}\\nFalling back to 0.5.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); return np.full(len(texts), 0.5)\n    \n    print(f\"Running LLM inference on {len(texts)} texts...\")\n    batch_size = 16\n    print(f\"Using batch size: {batch_size}\")\n    \n    llm_probs = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), batch_size), desc=f\"LLM ({model_name.split('/')[1]})\"):\n            batch_texts = texts[i : i + batch_size].tolist();\n            if not batch_texts: continue\n            try:\n                inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN_LLM)\n                input_ids = inputs['input_ids'].to(device)\n                attention_mask = inputs['attention_mask'].to(device)\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask) # Fixed call\n                logits = outputs[\"logits\"]\n                probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n                if not isinstance(probs, (np.ndarray, list)): probs = [probs.item()] # Handle single item\n                elif isinstance(probs, np.ndarray): probs = probs.tolist()\n                llm_probs.extend(probs)\n            except Exception as e: \n                print(f\"Error during LLM batch {i // batch_size}: {e}\")\n                if \"out of memory\" in str(e).lower(): print(\"!!! CUDA Out of Memory. Try reducing batch_size. !!!\")\n                llm_probs.extend([0.5] * len(batch_texts))\n            if i % (batch_size * 20) == 0: gc.collect(); torch.cuda.empty_cache()\n            \n    print(f\"LLM ({model_name.split('/')[1]}) prediction feature generated.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); print(\"--- Finished LLM Inference ---\")\n    if len(llm_probs) != len(texts): print(f\"CRITICAL WARNING: Length mismatch {len(llm_probs)} vs {len(texts)}. Padding/Truncating.\"); llm_probs = (llm_probs + [0.5] * len(texts))[:len(texts)]\n    return np.array(llm_probs)\n\n# -------------------------------------------------------------------\n# 1. CHAMPION FEATURE EXTRACTOR (With New Features)\n# -------------------------------------------------------------------\nclass ChampionFeatureExtractor:\n    \"\"\"Extracts base features, includes llm_pred, + new features.\"\"\"\n    def __init__(self):\n        self.ai_connectors = ['in conclusion', 'in summary', 'furthermore', 'moreover', 'additionally', 'however', 'therefore', 'thus', 'consequently', 'as a result', 'on the other hand', 'for instance', 'for example', 'it is important to note', 'it is worth noting', 'that being said']; self.formal_words = ['utilize', 'facilitate', 'implement', 'methodology', 'paradigm', 'leverage', 'robust', 'optimal', 'enhance', 'demonstrate', 'comprehensive', 'articulate']; self.hedging_words = ['may', 'might', 'could', 'possibly', 'perhaps', 'suggests', 'seems', 'appears', 'likely']; self.passive_indicators = ['is made', 'was made', 'is given', 'was given', 'is shown', 'was shown', 'is considered', 'was considered']\n        self.stopwords = set(ENGLISH_STOP_WORDS); self.sentence_splitter = re.compile(r'[.!?]+')\n    def _get_sent_len_var(self, text):\n        sentences = self.sentence_splitter.split(text); sentence_lengths = [len(s.split()) for s in sentences if len(s.split()) > 0]\n        if len(sentence_lengths) > 1: return np.var(sentence_lengths)\n        else: return 0\n    def extract_base_features(self, df):\n        features = pd.DataFrame(index=df.index);\n        if 'llm_pred' in df.columns: features['llm_pred'] = df['llm_pred']\n        answers = df['answer'].fillna(''); words = answers.str.split(); word_counts = words.str.len()\n        features['text_length'] = answers.str.len(); features['word_count'] = word_counts; features['avg_word_length'] = features['text_length'] / (features['word_count'] + 1e-6); features['sentence_count'] = answers.str.count(r'[.!?]+'); features['avg_sentence_length'] = features['word_count'] / (features['sentence_count'] + 1e-6); features['comma_count'] = answers.str.count(','); features['period_count'] = answers.str.count(r'\\.'); features['exclamation_count'] = answers.str.count(r'!'); features['question_count'] = answers.str.count(r'\\?'); features['punctuation_ratio'] = (features['comma_count'] + features['period_count']) / (features['word_count'] + 1e-6); features['unique_words'] = answers.apply(lambda x: len(set(str(x).lower().split()))); features['ttr'] = features['unique_words'] / (features['word_count'] + 1e-6)\n        def count_phrases(text, phrases): text_lower = str(text).lower(); return sum(1 for phrase in phrases if phrase in text_lower)\n        features['ai_connector_count'] = answers.apply(lambda x: count_phrases(x, self.ai_connectors)); features['formal_word_count'] = answers.apply(lambda x: count_phrases(x, self.formal_words)); features['passive_voice_count'] = answers.apply(lambda x: count_phrases(x, self.passive_indicators)); features['hedging_word_count'] = answers.apply(lambda x: count_phrases(x, self.hedging_words)); features['ai_connector_ratio'] = features['ai_connector_count'] / (features['word_count'] + 1e-6); features['formal_word_ratio'] = features['formal_word_count'] / (features['word_count'] + 1e-6); features['passive_voice_ratio'] = features['passive_voice_count'] / (features['word_count'] + 1e-6); features['hedging_ratio'] = features['hedging_word_count'] / (features['word_count'] + 1e-6)\n        features['subordinate_ratio'] = answers.str.count(r'\\b(that|which|who|when|where|while|although|because|if)\\b') / (features['word_count'] + 1e-6); features['paragraph_count'] = answers.str.count(r'\\n\\n') + 1; features['avg_paragraph_len'] = features['word_count'] / (features['paragraph_count'] + 1e-6); features['word_length_std'] = answers.apply(lambda x: np.std([len(w) for w in str(x).split()]) if len(str(x).split()) > 1 else 0)\n        features['em_dash_count'] = answers.str.count('â€”'); features['em_dash_ratio'] = features['em_dash_count'] / (features['word_count'] + 1e-6)\n        features['question_mark_count'] = answers.str.count(r'\\?'); features['exclamation_mark_count'] = answers.str.count(r'!')\n        features['stopword_count'] = words.apply(lambda x: sum(1 for word in x if word.lower() in self.stopwords)); features['stopword_ratio'] = features['stopword_count'] / (features['word_count'] + 1e-6)\n        features['uppercase_word_count'] = words.apply(lambda x: sum(1 for word in x if word.isascii() and word.istitle())); features['uppercase_ratio'] = features['uppercase_word_count'] / (features['word_count'] + 1e-6)\n        features['sentence_length_variance'] = answers.apply(self._get_sent_len_var)\n        features.replace([np.inf, -np.inf], 0, inplace=True); features.fillna(0, inplace=True); return features\n\n# -------------------------------------------------------------------\n# 2. CHAMPION AI DETECTOR (L2 Stack + Repeated CV + Max Reg)\n# -------------------------------------------------------------------\nclass ChampionAIDetector: \n    \"\"\"Trains L2 Stack (LGBM+CatBoost+Ridge) using SelectKBest feats and Repeated CV.\"\"\"\n    def __init__(self, n_splits=N_SPLITS_CLASSICAL, n_repeats=N_REPEATS_CLASSICAL, n_features=N_FEATURES_CLASSICAL):\n        self.feature_extractor = ChampionFeatureExtractor(); self.n_splits = n_splits; self.n_repeats = n_repeats; self.n_features = n_features; self.models = {}; self.meta_model = None; self.calibrator = IsotonicRegression(out_of_bounds='clip'); self.scaler = StandardScaler(); self.tfidf_word = None; self.tfidf_char = None; self.selector = None; self.topic_columns = None; self.is_trained = False\n        print(f\"\\nClassical Detector (L2 Stack + Repeated CV): {n_splits} splits, {n_repeats} repeats, SelectKBest(k={n_features}).\")\n    def _get_consistent_topic_dummies(self, series, fit_columns=False):\n        # (Unchanged)\n        series = series.fillna(\"Unknown_Topic\"); dummies = pd.get_dummies(series, prefix='topic', dtype=int, dummy_na=False)\n        if fit_columns:\n            self.topic_columns = dummies.columns\n            if \"topic_Unknown_Topic\" not in self.topic_columns and series.astype(str).str.contains(\"Unknown_Topic\").any(): self.topic_columns = self.topic_columns.append(pd.Index([\"topic_Unknown_Topic\"]))\n            print(f\"Identified {len(self.topic_columns)} topic columns during fit.\"); return dummies.reindex(columns=self.topic_columns, fill_value=0)\n        elif self.topic_columns is not None: return dummies.reindex(columns=self.topic_columns, fill_value=0)\n        else: raise ValueError(\"Topic columns not fitted.\")\n    def prepare_champion_features(self, train_df, test_df, is_training_fold=False, is_full_training=False):\n        \"\"\"Prepares features. Fits objects if training. Separates llm_pred.\"\"\"\n        # (Unchanged from v25.3)\n        print(\"Extracting base handcrafted + LLM features...\"); train_base_features_df = self.feature_extractor.extract_base_features(train_df); test_base_features_df = self.feature_extractor.extract_base_features(test_df)\n        print(\"Processing topic features...\")\n        fit_topic_cols = (self.topic_columns is None) or is_full_training\n        train_topic_dummies = self._get_consistent_topic_dummies(train_df['topic'], fit_columns=fit_topic_cols); test_topic_dummies = self._get_consistent_topic_dummies(test_df['topic'], fit_columns=False)\n        llm_train_feat = train_base_features_df.pop('llm_pred').values.reshape(-1, 1); llm_test_feat = test_base_features_df.pop('llm_pred').values.reshape(-1, 1)\n        train_dense_features_df = pd.concat([train_base_features_df, train_topic_dummies], axis=1); test_dense_features_df = pd.concat([test_base_features_df, test_topic_dummies], axis=1)\n        test_dense_features_df = test_dense_features_df.reindex(columns=train_dense_features_df.columns, fill_value=0)\n        if is_training_fold or is_full_training: print(\"Fitting/Transforming dense features (excl. LLM)...\"); train_features_scaled = self.scaler.fit_transform(train_dense_features_df); test_features_scaled = self.scaler.transform(test_dense_features_df)\n        else: print(\"Transforming dense features (excl. LLM)...\"); train_features_scaled = self.scaler.transform(train_dense_features_df); test_features_scaled = self.scaler.transform(test_dense_features_df)\n        if is_training_fold or is_full_training:\n            print(\"Fitting Word TF-IDF...\"); self.tfidf_word = TfidfVectorizer(ngram_range=(1, 3), min_df=3, max_df=0.9, sublinear_tf=True, stop_words='english', max_features=2500)\n            train_tfidf_word = self.tfidf_word.fit_transform(train_df['answer'].fillna('')); test_tfidf_word = self.tfidf_word.transform(test_df['answer'].fillna(''))\n            print(\"Fitting Char TF-IDF...\"); self.tfidf_char = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 5), min_df=3, sublinear_tf=True, max_features=1250)\n            train_tfidf_char = self.tfidf_char.fit_transform(train_df['answer'].fillna('')); test_tfidf_char = self.tfidf_char.transform(test_df['answer'].fillna(''))\n        else:\n            if not self.tfidf_word or not self.tfidf_char: raise ValueError(\"TF-IDF vectorizers not fitted.\")\n            print(\"Transforming text using fitted TF-IDF...\"); train_tfidf_word = self.tfidf_word.transform(train_df['answer'].fillna('')); test_tfidf_word = self.tfidf_word.transform(test_df['answer'].fillna(''))\n            train_tfidf_char = self.tfidf_char.transform(train_df['answer'].fillna('')); test_tfidf_char = self.tfidf_char.transform(test_df['answer'].fillna(''))\n        print(\"Combining all feature sets...\"); X_train_full = hstack([csr_matrix(train_features_scaled), train_tfidf_word, train_tfidf_char, csr_matrix(llm_train_feat)]).tocsr()\n        X_test_full = hstack([csr_matrix(test_features_scaled), test_tfidf_word, test_tfidf_char, csr_matrix(llm_test_feat)]).tocsr()\n        print(f\"Shape before feature selection: Train={X_train_full.shape}, Test={X_test_full.shape}\")\n        if is_training_fold or is_full_training:\n            y_train = train_df['is_cheating'].values; print(f\"Fitting SelectKBest (k={self.n_features})...\"); self.selector = SelectKBest(f_classif, k=min(self.n_features, X_train_full.shape[1]))\n            with warnings.catch_warnings(): warnings.filterwarnings('ignore'); self.selector.fit(X_train_full, y_train)\n            X_train_selected = self.selector.transform(X_train_full); X_test_selected = self.selector.transform(X_test_full)\n        else:\n            if not self.selector: raise ValueError(\"SelectKBest not fitted.\"); print(f\"Transforming features using fitted SelectKBest (k={self.n_features})...\")\n            X_train_selected = self.selector.transform(X_train_full); X_test_selected = self.selector.transform(X_test_full)\n        X_train_final = X_train_selected.toarray(); X_test_final = X_test_selected.toarray()\n        print(f\"Final training feature shape: {X_train_final.shape}\"); print(f\"Final testing feature shape: {X_test_final.shape}\")\n        return X_train_final, X_test_final\n    \n    def train_champion_ensemble(self, full_train_df):\n        \"\"\"Trains the L2 Stacking ensemble. Fits Scaler/TFIDF/Selector on first fold.\"\"\"\n        # (Unchanged from v25.6)\n        y_train = full_train_df['is_cheating'].values\n        print(f\"\\n--- Starting Classical Ensemble Training ---\"); print(f\"Training set size: {len(full_train_df)}\"); print(f\"Positive class ratio: {y_train.mean():.6f}\")\n        rskf = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=RANDOM_SEED)\n        oof_preds_lgb = np.zeros((len(full_train_df), self.n_repeats)); oof_preds_cat = np.zeros((len(full_train_df), self.n_repeats)); oof_preds_ridge = np.zeros((len(full_train_df), self.n_repeats))\n        print(\"Fitting topic columns on full training data...\"); _ = self._get_consistent_topic_dummies(full_train_df['topic'], fit_columns=True)\n        print(\"\\nFitting Scalers, TFIDF, and Selector on FULL training data...\")\n        _ = self.prepare_champion_features(full_train_df, full_train_df, is_training_fold=True, is_full_training=True) # Fit preprocessing objects\n        print(\"\\nStarting Repeated K-Fold Training...\")\n        for fold, (train_idx, val_idx) in enumerate(tqdm(rskf.split(full_train_df, y_train), total=self.n_splits * self.n_repeats, desc=\"CV Folds\")):\n            repeat = fold // self.n_splits; split = fold % self.n_splits\n            print(f\"\\n{'='*20} Repeat {repeat+1}/{self.n_repeats}, Split {split+1}/{self.n_splits} {'='*20}\")\n            train_fold_df, val_fold_df = full_train_df.iloc[train_idx], full_train_df.iloc[val_idx]\n            y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n            X_train_fold, X_val_fold = self.prepare_champion_features(train_fold_df, val_fold_df, is_training_fold=False, is_full_training=False)\n            print(f\"Fold {fold+1}: Train shape {X_train_fold.shape}, Val shape {X_val_fold.shape}\")\n            fold_seed = RANDOM_SEED + fold\n            \n            # --- MAX REGULARIZATION ---\n            print(\"Training LightGBM...\"); lgb_model = lgb.LGBMClassifier(n_estimators=2500, learning_rate=0.01, max_depth=7, num_leaves=25, min_child_samples=25, subsample=0.7, colsample_bytree=0.6, reg_alpha=0.5, reg_lambda=0.5, random_state=fold_seed, verbose=-1, n_jobs=-1) # leaves=25, child=25, reg=0.5\n            lgb_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], callbacks=[lgb.early_stopping(200, verbose=False)]); oof_preds_lgb[val_idx, repeat] = lgb_model.predict_proba(X_val_fold)[:, 1]\n            print(\"Training CatBoost...\"); cat_model = CatBoostClassifier(iterations=2500, learning_rate=0.02, depth=6, l2_leaf_reg=7, random_seed=fold_seed, verbose=0, early_stopping_rounds=200, task_type=\"CPU\") # l2=7\n            cat_model.fit(X_train_fold, y_train_fold, eval_set=(X_val_fold, y_val_fold), verbose=False); oof_preds_cat[val_idx, repeat] = cat_model.predict_proba(X_val_fold)[:, 1]\n            print(\"Training Ridge Classifier...\"); ridge_scaler = StandardScaler(); X_train_fold_scaled = ridge_scaler.fit_transform(X_train_fold); X_val_fold_scaled = ridge_scaler.transform(X_val_fold)\n            ridge_model = RidgeClassifier(alpha=1.5, random_state=fold_seed); ridge_model.fit(X_train_fold_scaled, y_train_fold); ridge_scores = ridge_model.decision_function(X_val_fold_scaled); ridge_probs = 1 / (1 + np.exp(-ridge_scores)) # alpha=1.5\n            oof_preds_ridge[val_idx, repeat] = ridge_probs\n            \n            fold_auc_lgb = roc_auc_score(y_val_fold, oof_preds_lgb[val_idx, repeat]); fold_auc_cat = roc_auc_score(y_val_fold, oof_preds_cat[val_idx, repeat]); fold_auc_ridge = roc_auc_score(y_val_fold, oof_preds_ridge[val_idx, repeat])\n            print(f\"Fold {fold+1} LGB AUC: {fold_auc_lgb:.6f}\"); print(f\"Fold {fold+1} CAT AUC: {fold_auc_cat:.6f}\"); print(f\"Fold {fold+1} RIDGE AUC: {fold_auc_ridge:.6f}\")\n        print(\"\\n\" + \"=\"*50); print(\"Averaging OOF Scores across repeats:\")\n        oof_lgb_avg = np.mean(oof_preds_lgb, axis=1); oof_cat_avg = np.mean(oof_preds_cat, axis=1); oof_ridge_avg = np.mean(oof_preds_ridge, axis=1)\n        print(f\"  LGB OOF AUC (Averaged): {roc_auc_score(y_train, oof_lgb_avg):.8f}\")\n        print(f\"  CAT OOF AUC (Averaged): {roc_auc_score(y_train, oof_cat_avg):.8f}\")\n        print(f\"  RIDGE OOF AUC (Averaged): {roc_auc_score(y_train, oof_ridge_avg):.8f}\")\n        print(\"\\nTraining Classical L2 Meta-Model on averaged OOFs...\"); X_meta_train = np.stack([oof_lgb_avg, oof_cat_avg, oof_ridge_avg], axis=1)\n        \n        # --- MAX REGULARIZATION ---\n        self.meta_model = LogisticRegression(C=0.2, solver='liblinear', random_state=RANDOM_SEED); self.meta_model.fit(X_meta_train, y_train) # C=0.2\n        \n        oof_ensemble_preds = self.meta_model.predict_proba(X_meta_train)[:, 1]; oof_ensemble_auc = roc_auc_score(y_train, oof_ensemble_preds)\n        print(f\"\\nCLASSICAL L2 STACKING OOF AUC (Averaged): {oof_ensemble_auc:.8f}\"); print(\"Training Classical Isotonic Calibrator...\"); self.calibrator.fit(oof_ensemble_preds, y_train)\n        self.is_trained = True; return oof_ensemble_auc, oof_ensemble_preds\n    def predict_champion(self, full_train_df, test_df):\n        \"\"\"Generates final predictions using fitted classical components.\"\"\"\n        if not self.is_trained: raise ValueError(\"Classical model not trained!\")\n        print(\"\\n\" + \"=\"*50); print(\"Generating Final Classical Ensemble Predictions...\"); print(\"=\"*50)\n        print(\"Preparing features for final classical model training and prediction...\")\n        X_train_full, X_test = self.prepare_champion_features(full_train_df, test_df, is_training_fold=False, is_full_training=True) # Use is_full_training=True\n        y_train_full = full_train_df['is_cheating'].values; test_preds = {}\n        \n        # --- MAX REGULARIZATION ---\n        print(\"Training final LightGBM...\"); lgb_final = lgb.LGBMClassifier(n_estimators=2500, learning_rate=0.01, max_depth=7, num_leaves=25, min_child_samples=25, subsample=0.7, colsample_bytree=0.6, reg_alpha=0.5, reg_lambda=0.5, random_state=RANDOM_SEED, verbose=-1, n_jobs=-1) # leaves=25, child=25, reg=0.5\n        lgb_final.fit(X_train_full, y_train_full); test_preds['lgb'] = lgb_final.predict_proba(X_test)[:, 1]\n        print(\"Training final CatBoost...\"); cat_final = CatBoostClassifier(iterations=2450, learning_rate=0.02, depth=6, l2_leaf_reg=7, random_seed=RANDOM_SEED, verbose=0, task_type=\"CPU\") # l2=7\n        cat_final.fit(X_train_full, y_train_full); test_preds['cat'] = cat_final.predict_proba(X_test)[:, 1]\n        print(\"Training final Ridge...\"); ridge_scaler = StandardScaler(); X_train_full_scaled = ridge_scaler.fit_transform(X_train_full); X_test_scaled = ridge_scaler.transform(X_test)\n        ridge_final = RidgeClassifier(alpha=1.5, random_state=RANDOM_SEED); ridge_final.fit(X_train_full_scaled, y_train_full) # alpha=1.5\n        ridge_scores = ridge_final.decision_function(X_test_scaled); test_preds['ridge'] = 1 / (1 + np.exp(-ridge_scores))\n        \n        print(\"Applying Classical L2 Meta-Model...\"); X_meta_test = np.stack(list(test_preds.values()), axis=1)\n        final_proba = self.meta_model.predict_proba(X_meta_test)[:, 1]\n        print(\"Applying Classical Isotonic Calibration...\"); calibrated_proba = self.calibrator.transform(final_proba)\n        calibrated_proba = np.clip(calibrated_proba, 0.001, 0.999)\n        return calibrated_proba, final_proba\n\n# --- (Fallback model - unchanged) ---\ndef champion_fallback():\n    \"\"\"Fallback using basic LGBM, basic features.\"\"\"\n    print(\"CRITICAL ERROR: Main pipeline failed. Using CHAMPION FALLBACK model...\")\n    try:\n        from sklearn.preprocessing import StandardScaler; from scipy.sparse import hstack, csr_matrix;\n        train_df = pd.read_csv('/kaggle/input/mercor-ai-detection/train.csv'); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv')\n        feature_extractor = ChampionFeatureExtractor(); train_base_features = feature_extractor.extract_base_features(train_df.drop(columns=['llm_pred'], errors='ignore'))\n        test_base_features = feature_extractor.extract_base_features(test_df.drop(columns=['llm_pred'], errors='ignore'))\n        train_topic_dummies = pd.get_dummies(train_df['topic'], prefix='topic', dtype=int); fallback_topic_columns = train_topic_dummies.columns; test_topic_dummies = pd.get_dummies(test_df['topic'], prefix='topic', dtype=int).reindex(columns=fallback_topic_columns, fill_value=0)\n        train_dense_features = pd.concat([train_base_features, train_topic_dummies], axis=1); test_dense_features = pd.concat([test_base_features, test_topic_dummies], axis=1).reindex(columns=train_dense_features.columns, fill_value=0)\n        scaler = StandardScaler(); train_dense_scaled = scaler.fit_transform(train_dense_features); test_dense_scaled = scaler.transform(test_dense_features)\n        tfidf = TfidfVectorizer(max_features=2500, ngram_range=(1, 3), stop_words='english'); train_tfidf = tfidf.fit_transform(train_df['answer'].fillna('')); test_tfidf = tfidf.transform(test_df['answer'].fillna(''))\n        X_train = hstack([csr_matrix(train_dense_scaled), train_tfidf]).tocsr(); X_test = hstack([csr_matrix(test_dense_scaled), test_tfidf]).tocsr(); y_train = train_df['is_cheating']\n        model = lgb.LGBMClassifier(n_estimators=1000, learning_rate=0.01, max_depth=7, num_leaves=63, random_state=RANDOM_SEED, verbose=-1, n_jobs=-1, reg_alpha=0.1, reg_lambda=0.1)\n        model.fit(X_train, y_train); test_proba = model.predict_proba(X_test.toarray())[:, 1]\n        submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': test_proba}); submission.to_csv('submission.csv', index=False, float_format='%.10f')\n        print(\"Fallback submission.csv created successfully.\"); return submission\n    except Exception as e:\n        print(f\"Fallback model ALSO failed: {e}\"); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv'); submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': 0.5}); submission.to_csv('submission.csv', index=False, float_format='%.10f'); print(\"CRITICAL: Created dummy submission.csv with 0.5 probability.\"); return submission\n\n# -------------------------------------------------------------------\n# 4. MAIN EXECUTION (Calibrates LLM Pred, uses L2 Stack)\n# -------------------------------------------------------------------\ndef champion_main():\n    \"\"\"Main function: Runs LLM inference, calibrates feature, runs classical L2 stack.\"\"\"\n    print(\"=\" * 60); print(f\"  Mercor AI Text Detection - CHAMPION v27.2 (Max Reg)\"); print(\"=\" * 60)\n    train_df = pd.read_csv('/kaggle/input/mercor-ai-detection/train.csv'); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv')\n    print(f\"Training data shape: {train_df.shape}\"); print(f\"Test data shape: {test_df.shape}\")\n    train_df['answer'] = train_df['answer'].fillna(''); test_df['answer'] = test_df['answer'].fillna('')\n    # --- Step 1: Generate LLM predictions ---\n    all_texts = pd.concat([train_df['answer'], test_df['answer']])\n    llm_preds_all = get_llm_predictions(all_texts, LLM_MODEL_NAME) # Using desklib\n    llm_oof_preds_orig = llm_preds_all[:len(train_df)]; llm_test_preds_orig = llm_preds_all[len(train_df):]\n    # --- Step 2: Calibrate LLM predictions ---\n    print(\"\\nCalibrating LLM predictions...\"); llm_calibrator = IsotonicRegression(out_of_bounds='clip')\n    llm_calibrator.fit(llm_oof_preds_orig, train_df['is_cheating'].values)\n    llm_oof_calibrated = llm_calibrator.transform(llm_oof_preds_orig)\n    llm_test_calibrated = llm_calibrator.transform(llm_test_preds_orig) # Fixed typo\n    print(f\"Calibrated Desklib OOF AUC: {roc_auc_score(train_df['is_cheating'], llm_oof_calibrated):.8f}\") # Fixed typo\n    train_df['llm_pred'] = llm_oof_calibrated # Add the single, calibrated LLM feature\n    test_df['llm_pred'] = llm_test_calibrated\n    print(\"CALIBRATED LLM feature ('llm_pred') added to dataframes.\")\n    \n    # --- Step 3: Run Classical L2 Stack Ensemble ---\n    classical_detector = ChampionAIDetector(n_splits=N_SPLITS_CLASSICAL, n_repeats=N_REPEATS_CLASSICAL, n_features=N_FEATURES_CLASSICAL) # L2 Stack class\n    oof_auc_classical, oof_preds_classical = classical_detector.train_champion_ensemble(train_df)\n    print(\"\\nStarting Final Test Set Prediction using L2 Stack...\");\n    calibrated_proba, base_proba = classical_detector.predict_champion(train_df, test_df)\n    \n    # Create submission files\n    submission_calibrated = pd.DataFrame({'id': test_df['id'], 'is_cheating': calibrated_proba}); submission_base = pd.DataFrame({'id': test_df['id'], 'is_cheating': base_proba})\n    submission_calibrated.to_csv('submission.csv', index=False, float_format='%.10f'); submission_base.to_csv('submission_base_uncalibrated.csv', index=False, float_format='%.10f')\n    # Final Output\n    print(\"\\n\" + \"=\"*60); print(\"CHAMPION PIPELINE COMPLETED SUCCESSFULLY!\");\n    print(f\"Final L2 Stack OOF AUC (with Calibrated LLM Feat): {oof_auc_classical:.8f}\")\n    print(\"\\nSubmission files created:\"); print(f\"  1. submission.csv (RECOMMENDED)\"); print(f\"  2. submission_base_uncalibrated.csv (Backup)\")\n    print(f\"\\nFinal 'submission.csv' prediction statistics:\"); print(f\"  Min: {calibrated_proba.min():.6f}\"); print(f\"  Max: {calibrated_proba.max():.6f}\"); print(f\"  Mean: {calibrated_proba.mean():.6f}\"); print(f\"  Median: {np.median(calibrated_proba):.6f}\")\n    print(\"\\nTop 10 Predictions:\"); print(submission_calibrated.head(10).to_string(index=False)); print(\"=\" * 60)\n\n# --- Run the Champion Pipeline ---\nif __name__ == \"__main__\":\n    start_time = time.time()\n    try:\n        gc.collect(); torch.cuda.empty_cache()\n        champion_main()\n    except Exception as e:\n        print(f\"\\n!!! AN ERROR OCCURRED IN THE MAIN PIPELINE !!!: {e}\")\n        import traceback; traceback.print_exc()\n        champion_fallback()\n    finally:\n        gc.collect(); torch.cuda.empty_cache(); end_time = time.time()\n        print(f\"\\nTotal execution time: {(end_time - start_time) / 60:.2f} minutes\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dd=pd.read_csv(\"/kaggle/working/submission.csv\")\ndd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nMercor AI Text Detection - 0.99+ AUC CHAMPION VERSION\n=====================================================\n\nv29: The \"Hybrid Monster\" (0.999+ Push) - FIXED\n- This is an ensemble of two complete, independent 0.99+ models.\n\n- MODEL A (Our 0.99006 Winner):\n    - 'desklib/ai-text-detector-v1.01' (0.96 OOF) as the LLM feature.\n    - RepeatedStratifiedKFold (5 splits, 3 repeats).\n    - All handcrafted features + SelectKBest(k=850).\n    - L3 Ensemble (L2 Stack + L2 Weighted Avg) on (LGBM, CatBoost, Ridge).\n    - Max Regularization.\n    \n- MODEL B (The 0.999+ Notebook's Approach):\n    - Model: 'FacebookAI/roberta-large'.\n    - Feature: Custom Prompt (Topic + Answer).\n    - Method: 5-Fold StratifiedKFold training.\n    - Prediction: Average of the 5 trained models.\n\n- FINAL SUBMISSION:\n    - 0.5 * Model_A_Preds + 0.5 * Model_B_Preds\n\"\"\"\n\n# === General Imports ===\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport gc\nimport warnings\nimport time\nimport os\nfrom tqdm.auto import tqdm, trange\nfrom typing import List, Tuple, Dict\n\n# === Sklearn Imports ===\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.utils import shuffle\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# === GBDT Imports ===\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\n# === Hugging Face Imports ===\nprint(\"Installing/Updating transformers...\"); os.system('pip install -q transformers torch'); print(\"Libraries installed.\")\ntry:\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import Dataset, DataLoader\n    from transformers import AutoTokenizer, AutoModel, AutoConfig, PreTrainedModel, AutoModelForSequenceClassification, DataCollatorWithPadding, get_cosine_schedule_with_warmup, RobertaTokenizer\n    import transformers\n    transformers.logging.set_verbosity_error()\nexcept ImportError as e: print(f\"FATAL ERROR during imports: {e}\"); raise e\nprint(\"All libraries imported successfully!\")\n\n# === Configuration ===\nLLM_MODEL_NAME_A = \"desklib/ai-text-detector-v1.01\" \nLLM_MODEL_NAME_B = \"FacebookAI/roberta-large\" # From the notebook\nN_SPLITS_CLASSICAL = 5\nN_REPEATS_CLASSICAL = 3\nN_SPLITS_ROBERTA = 5\nN_FEATURES_CLASSICAL = 850\nMAX_LEN_LLM = 512\nRANDOM_SEED = 42\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# ===================================================================\n# START: MODEL A (Our 0.99006 Pipeline)\n# ===================================================================\n\n# -------------------------------------------------------------------\n# 0a. CUSTOM LLM CLASS (for Desklib)\n# -------------------------------------------------------------------\nclass DesklibAIDetectionModel(PreTrainedModel):\n    config_class = AutoConfig\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = AutoModel.from_config(config)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.init_weights()\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = self.model(input_ids, attention_mask=attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n        pooled_output = sum_embeddings / sum_mask\n        logits = self.classifier(pooled_output)\n        loss = None\n        if labels is not None:\n            loss_fct = nn.BCEWithLogitsLoss()\n            loss = loss_fct(logits.view(-1), labels.float())\n        output = {\"logits\": logits}\n        if loss is not None: output[\"loss\"] = loss\n        return output\n\n# -------------------------------------------------------------------\n# 0b. LLM INFERENCE FUNCTION (for Desklib)\n# -------------------------------------------------------------------\ndef get_llm_predictions_A(texts, model_name):\n    print(f\"\\n--- Starting LLM Inference (Model A: {model_name}) ---\"); model = None; tokenizer = None\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n    if not torch.cuda.is_available(): print(\"!!! WARNING: No GPU for Model A. !!!\")\n    try:\n        print(f\"Loading Tokenizer: {model_name}...\"); tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        print(f\"Loading Model (Custom Class): {model_name}...\"); \n        model = DesklibAIDetectionModel.from_pretrained(model_name, trust_remote_code=True)\n        model.to(device); model.eval()\n        print(\"Model and Tokenizer loaded successfully.\")\n    except Exception as e:\n        print(f\"Error loading model/tokenizer {model_name}: {e}\\nFalling back to 0.5.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); return np.full(len(texts), 0.5)\n    print(f\"Running LLM inference on {len(texts)} texts...\")\n    batch_size = 16\n    print(f\"Using batch size: {batch_size}\")\n    llm_probs = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), batch_size), desc=f\"LLM 1 ({model_name.split('/')[1]})\"):\n            batch_texts = texts[i : i + batch_size].tolist();\n            if not batch_texts: continue\n            try:\n                inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN_LLM)\n                input_ids = inputs['input_ids'].to(device)\n                attention_mask = inputs['attention_mask'].to(device)\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                logits = outputs[\"logits\"]\n                probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n                if not isinstance(probs, (np.ndarray, list)): probs = [probs.item()]\n                elif isinstance(probs, np.ndarray): probs = probs.tolist()\n                llm_probs.extend(probs)\n            except Exception as e: \n                print(f\"Error during LLM batch {i // batch_size}: {e}\"); llm_probs.extend([0.5] * len(batch_texts))\n            if i % (batch_size * 20) == 0: gc.collect(); torch.cuda.empty_cache()\n    print(f\"LLM 1 ({model_name.split('/')[1]}) prediction feature generated.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); print(\"--- Finished LLM 1 Inference ---\")\n    if len(llm_probs) != len(texts): print(f\"CRITICAL WARNING: Length mismatch. Padding/Truncating.\"); llm_probs = (llm_probs + [0.5] * len(texts))[:len(texts)]\n    return np.array(llm_probs)\n\n# -------------------------------------------------------------------\n# 1. CHAMPION FEATURE EXTRACTOR (Model A)\n# -------------------------------------------------------------------\nclass ChampionFeatureExtractor(Dataset):\n    \"\"\"Extracts base features, includes llm_pred, + new features.\"\"\"\n    def __init__(self):\n        self.ai_connectors = ['in conclusion', 'in summary', 'furthermore', 'moreover', 'additionally', 'however', 'therefore', 'thus', 'consequently', 'as a result', 'on the other hand', 'for instance', 'for example', 'it is important to note', 'it is worth noting', 'that being said']; self.formal_words = ['utilize', 'facilitate', 'implement', 'methodology', 'paradigm', 'leverage', 'robust', 'optimal', 'enhance', 'demonstrate', 'comprehensive', 'articulate']; self.hedging_words = ['may', 'might', 'could', 'possibly', 'perhaps', 'suggests', 'seems', 'appears', 'likely']; self.passive_indicators = ['is made', 'was made', 'is given', 'was given', 'is shown', 'was shown', 'is considered', 'was considered']\n        self.stopwords = set(ENGLISH_STOP_WORDS); self.sentence_splitter = re.compile(r'[.!?]+')\n    def _get_sent_len_var(self, text):\n        sentences = self.sentence_splitter.split(text); sentence_lengths = [len(s.split()) for s in sentences if len(s.split()) > 0]\n        if len(sentence_lengths) > 1: return np.var(sentence_lengths)\n        else: return 0\n    def extract_base_features(self, df):\n        features = pd.DataFrame(index=df.index);\n        if 'llm_pred' in df.columns: features['llm_pred'] = df['llm_pred']\n        answers = df['answer'].fillna(''); words = answers.str.split(); word_counts = words.str.len()\n        features['text_length'] = answers.str.len(); features['word_count'] = word_counts; features['avg_word_length'] = features['text_length'] / (features['word_count'] + 1e-6); features['sentence_count'] = answers.str.count(r'[.!?]+'); features['avg_sentence_length'] = features['word_count'] / (features['sentence_count'] + 1e-6); features['comma_count'] = answers.str.count(','); features['period_count'] = answers.str.count(r'\\.'); features['exclamation_count'] = answers.str.count(r'!'); features['question_count'] = answers.str.count(r'\\?'); features['punctuation_ratio'] = (features['comma_count'] + features['period_count']) / (features['word_count'] + 1e-6); features['unique_words'] = answers.apply(lambda x: len(set(str(x).lower().split()))); features['ttr'] = features['unique_words'] / (features['word_count'] + 1e-6)\n        def count_phrases(text, phrases): text_lower = str(text).lower(); return sum(1 for phrase in phrases if phrase in text_lower)\n        features['ai_connector_count'] = answers.apply(lambda x: count_phrases(x, self.ai_connectors)); features['formal_word_count'] = answers.apply(lambda x: count_phrases(x, self.formal_words)); features['passive_voice_count'] = answers.apply(lambda x: count_phrases(x, self.passive_indicators)); features['hedging_word_count'] = answers.apply(lambda x: count_phrases(x, self.hedging_words)); features['ai_connector_ratio'] = features['ai_connector_count'] / (features['word_count'] + 1e-6); features['formal_word_ratio'] = features['formal_word_count'] / (features['word_count'] + 1e-6); features['passive_voice_ratio'] = features['passive_voice_count'] / (features['word_count'] + 1e-6); features['hedging_ratio'] = features['hedging_word_count'] / (features['word_count'] + 1e-6)\n        features['subordinate_ratio'] = answers.str.count(r'\\b(that|which|who|when|where|while|although|because|if)\\b') / (features['word_count'] + 1e-6); features['paragraph_count'] = answers.str.count(r'\\n\\n') + 1; features['avg_paragraph_len'] = features['word_count'] / (features['paragraph_count'] + 1e-6); features['word_length_std'] = answers.apply(lambda x: np.std([len(w) for w in str(x).split()]) if len(str(x).split()) > 1 else 0)\n        features['em_dash_count'] = answers.str.count('â€”'); features['em_dash_ratio'] = features['em_dash_count'] / (features['word_count'] + 1e-6)\n        features['question_mark_count'] = answers.str.count(r'\\?'); features['exclamation_mark_count'] = answers.str.count(r'!')\n        features['stopword_count'] = words.apply(lambda x: sum(1 for word in x if word.lower() in self.stopwords)); features['stopword_ratio'] = features['stopword_count'] / (features['word_count'] + 1e-6)\n        features['uppercase_word_count'] = words.apply(lambda x: sum(1 for word in x if word.isascii() and word.istitle())); features['uppercase_ratio'] = features['uppercase_word_count'] / (features['word_count'] + 1e-6)\n        features['sentence_length_variance'] = answers.apply(self._get_sent_len_var)\n        features.replace([np.inf, -np.inf], 0, inplace=True); features.fillna(0, inplace=True); return features\n\n# -------------------------------------------------------------------\n# 2. CHAMPION AI DETECTOR (L3 Ensemble) (Model A)\n# -------------------------------------------------------------------\nclass ChampionAIDetector_A: \n    \"\"\"Trains L2 Stack AND L2 Weighted Avg (LGBM, CAT, Ridge), blends them 50/50.\"\"\"\n    def __init__(self, n_splits=N_SPLITS_CLASSICAL, n_repeats=N_REPEATS_CLASSICAL, n_features=N_FEATURES_CLASSICAL):\n        self.feature_extractor = ChampionFeatureExtractor(); self.n_splits = n_splits; self.n_repeats = n_repeats; self.n_features = n_features; self.models = {}; self.meta_model = None; self.calibrator = IsotonicRegression(out_of_bounds='clip'); self.scaler = StandardScaler(); self.tfidf_word = None; self.tfidf_char = None; self.selector = None; self.topic_columns = None; self.is_trained = False\n        self.l1_weights = None # For Model B\n        self.final_l1_models = {'lgb': None, 'cat': None, 'ridge': None} # Store final models\n        print(f\"\\nClassical Detector (L3 Ensemble): {n_splits} splits, {n_repeats} repeats, SelectKBest(k={n_features}).\")\n    def _get_consistent_topic_dummies(self, series, fit_columns=False):\n        series = series.fillna(\"Unknown_Topic\"); dummies = pd.get_dummies(series, prefix='topic', dtype=int, dummy_na=False)\n        if fit_columns:\n            self.topic_columns = dummies.columns\n            if \"topic_Unknown_Topic\" not in self.topic_columns and series.astype(str).str.contains(\"Unknown_Topic\").any(): self.topic_columns = self.topic_columns.append(pd.Index([\"topic_Unknown_Topic\"]))\n            print(f\"Identified {len(self.topic_columns)} topic columns during fit.\"); return dummies.reindex(columns=self.topic_columns, fill_value=0)\n        elif self.topic_columns is not None: return dummies.reindex(columns=self.topic_columns, fill_value=0)\n        else: raise ValueError(\"Topic columns not fitted.\")\n    def prepare_champion_features(self, train_df, test_df, is_training_fold=False, is_full_training=False):\n        \"\"\"Prepares features. Fits objects if training. Separates llm_pred.\"\"\"\n        print(\"Extracting base handcrafted + LLM features...\"); train_base_features_df = self.feature_extractor.extract_base_features(train_df); test_base_features_df = self.feature_extractor.extract_base_features(test_df)\n        print(\"Processing topic features...\")\n        fit_topic_cols = (self.topic_columns is None) or is_full_training\n        train_topic_dummies = self._get_consistent_topic_dummies(train_df['topic'], fit_columns=fit_topic_cols); test_topic_dummies = self._get_consistent_topic_dummies(test_df['topic'], fit_columns=False)\n        llm_train_feat = train_base_features_df.pop('llm_pred').values.reshape(-1, 1); llm_test_feat = test_base_features_df.pop('llm_pred').values.reshape(-1, 1)\n        train_dense_features_df = pd.concat([train_base_features_df, train_topic_dummies], axis=1); test_dense_features_df = pd.concat([test_base_features_df, test_topic_dummies], axis=1)\n        test_dense_features_df = test_dense_features_df.reindex(columns=train_dense_features_df.columns, fill_value=0)\n        if is_training_fold or is_full_training: print(\"Fitting/Transforming dense features (excl. LLM)...\"); train_features_scaled = self.scaler.fit_transform(train_dense_features_df); test_features_scaled = self.scaler.transform(test_dense_features_df)\n        else: print(\"Transforming dense features (excl. LLM)...\"); train_features_scaled = self.scaler.transform(train_dense_features_df); test_features_scaled = self.scaler.transform(test_dense_features_df)\n        if is_training_fold or is_full_training:\n            print(\"Fitting Word TF-IDF...\"); self.tfidf_word = TfidfVectorizer(ngram_range=(1, 3), min_df=3, max_df=0.9, sublinear_tf=True, stop_words='english', max_features=2500)\n            train_tfidf_word = self.tfidf_word.fit_transform(train_df['answer'].fillna('')); test_tfidf_word = self.tfidf_word.transform(test_df['answer'].fillna(''))\n            print(\"Fitting Char TF-IDF...\"); self.tfidf_char = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 5), min_df=3, sublinear_tf=True, max_features=1250)\n            train_tfidf_char = self.tfidf_char.fit_transform(train_df['answer'].fillna('')); test_tfidf_char = self.tfidf_char.transform(test_df['answer'].fillna(''))\n        else:\n            if not self.tfidf_word or not self.tfidf_char: raise ValueError(\"TF-IDF vectorizers not fitted.\")\n            print(\"Transforming text using fitted TF-IDF...\"); train_tfidf_word = self.tfidf_word.transform(train_df['answer'].fillna('')); test_tfidf_word = self.tfidf_word.transform(test_df['answer'].fillna(''))\n            train_tfidf_char = self.tfidf_char.transform(train_df['answer'].fillna('')); test_tfidf_char = self.tfidf_char.transform(test_df['answer'].fillna(''))\n        print(\"Combining all feature sets...\"); X_train_full = hstack([csr_matrix(train_features_scaled), train_tfidf_word, train_tfidf_char, csr_matrix(llm_train_feat)]).tocsr()\n        X_test_full = hstack([csr_matrix(test_features_scaled), test_tfidf_word, test_tfidf_char, csr_matrix(llm_test_feat)]).tocsr()\n        print(f\"Shape before feature selection: Train={X_train_full.shape}, Test={X_test_full.shape}\")\n        if is_training_fold or is_full_training:\n            y_train = train_df['is_cheating'].values; print(f\"Fitting SelectKBest (k={self.n_features})...\"); self.selector = SelectKBest(f_classif, k=min(self.n_features, X_train_full.shape[1]))\n            with warnings.catch_warnings(): warnings.filterwarnings('ignore'); self.selector.fit(X_train_full, y_train)\n            X_train_selected = self.selector.transform(X_train_full); X_test_selected = self.selector.transform(X_test_full)\n        else:\n            if not self.selector: raise ValueError(\"SelectKBest not fitted.\"); print(f\"Transforming features using fitted SelectKBest (k={self.n_features})...\")\n            X_train_selected = self.selector.transform(X_train_full); X_test_selected = self.selector.transform(X_test_full)\n        X_train_final = X_train_selected.toarray(); X_test_final = X_test_selected.toarray()\n        print(f\"Final training feature shape: {X_train_final.shape}\"); print(f\"Final testing feature shape: {X_test_final.shape}\")\n        return X_train_final, X_test_final\n    \n    def train_champion_ensemble(self, full_train_df):\n        \"\"\"Trains L1 models, then trains L2 Stack AND L2 Weighted Avg.\"\"\"\n        y_train = full_train_df['is_cheating'].values\n        print(f\"\\n--- Starting Classical Ensemble Training ---\"); print(f\"Training set size: {len(full_train_df)}\"); print(f\"Positive class ratio: {y_train.mean():.6f}\")\n        rskf = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=RANDOM_SEED)\n        oof_preds_lgb = np.zeros((len(full_train_df), self.n_repeats))\n        oof_preds_cat = np.zeros((len(full_train_df), self.n_repeats))\n        oof_preds_ridge = np.zeros((len(full_train_df), self.n_repeats))\n        print(\"Fitting topic columns on full training data...\"); _ = self._get_consistent_topic_dummies(full_train_df['topic'], fit_columns=True)\n        print(\"\\nFitting Scalers, TFIDF, and Selector on FULL training data...\")\n        _ = self.prepare_champion_features(full_train_df, full_train_df, is_training_fold=True, is_full_training=True)\n        print(\"\\nStarting Repeated K-Fold Training...\")\n        for fold, (train_idx, val_idx) in enumerate(tqdm(rskf.split(full_train_df, y_train), total=self.n_splits * self.n_repeats, desc=\"CV Folds (Model A)\")):\n            repeat = fold // self.n_splits; split = fold % self.n_splits\n            print(f\"\\n{'='*20} Repeat {repeat+1}/{self.n_repeats}, Split {split+1}/{self.n_splits} {'='*20}\")\n            train_fold_df, val_fold_df = full_train_df.iloc[train_idx], full_train_df.iloc[val_idx]\n            y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n            X_train_fold, X_val_fold = self.prepare_champion_features(train_fold_df, val_fold_df, is_training_fold=False, is_full_training=False)\n            print(f\"Fold {fold+1}: Train shape {X_train_fold.shape}, Val shape {X_val_fold.shape}\")\n            fold_seed = RANDOM_SEED + fold\n            print(\"Training LightGBM...\"); lgb_model = lgb.LGBMClassifier(n_estimators=2500, learning_rate=0.01, max_depth=7, num_leaves=25, min_child_samples=25, subsample=0.7, colsample_bytree=0.6, reg_alpha=0.6, reg_lambda=0.6, random_state=fold_seed, verbose=-1, n_jobs=-1) \n            lgb_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], callbacks=[lgb.early_stopping(200, verbose=False)]); oof_preds_lgb[val_idx, repeat] = lgb_model.predict_proba(X_val_fold)[:, 1]\n            print(\"Training CatBoost...\"); cat_model = CatBoostClassifier(iterations=2500, learning_rate=0.02, depth=6, l2_leaf_reg=8, random_seed=fold_seed, verbose=0, early_stopping_rounds=200, task_type=\"CPU\")\n            cat_model.fit(X_train_fold, y_train_fold, eval_set=(X_val_fold, y_val_fold), verbose=False); oof_preds_cat[val_idx, repeat] = cat_model.predict_proba(X_val_fold)[:, 1]\n            print(\"Training Ridge Classifier...\"); ridge_scaler = StandardScaler(); X_train_fold_scaled = ridge_scaler.fit_transform(X_train_fold); X_val_fold_scaled = ridge_scaler.transform(X_val_fold)\n            ridge_model = RidgeClassifier(alpha=2.0, random_state=fold_seed); ridge_model.fit(X_train_fold_scaled, y_train_fold); ridge_scores = ridge_model.decision_function(X_val_fold_scaled); ridge_probs = 1 / (1 + np.exp(-ridge_scores))\n            oof_preds_ridge[val_idx, repeat] = ridge_probs\n            fold_auc_lgb = roc_auc_score(y_val_fold, oof_preds_lgb[val_idx, repeat]); fold_auc_cat = roc_auc_score(y_val_fold, oof_preds_cat[val_idx, repeat]); fold_auc_ridge = roc_auc_score(y_val_fold, oof_preds_ridge[val_idx, repeat])\n            print(f\"Fold {fold+1} LGB AUC: {fold_auc_lgb:.6f}\"); print(f\"Fold {fold+1} CAT AUC: {fold_auc_cat:.6f}\"); print(f\"Fold {fold+1} RIDGE AUC: {fold_auc_ridge:.6f}\")\n        \n        print(\"\\n\" + \"=\"*50); print(\"Averaging OOF Scores across repeats:\")\n        oof_lgb_avg = np.mean(oof_preds_lgb, axis=1); oof_cat_avg = np.mean(oof_preds_cat, axis=1); oof_ridge_avg = np.mean(oof_preds_ridge, axis=1)\n        \n        oof_aucs = {\n            'lgb': roc_auc_score(y_train, oof_lgb_avg),\n            'cat': roc_auc_score(y_train, oof_cat_avg),\n            'ridge': roc_auc_score(y_train, oof_ridge_avg)\n        }\n        print(f\"  LGB OOF AUC (Averaged): {oof_aucs['lgb']:.8f}\")\n        print(f\"  CAT OOF AUC (Averaged): {oof_aucs['cat']:.8f}\")\n        print(f\"  RIDGE OOF AUC (Averaged): {oof_aucs['ridge']:.8f}\")\n        \n        total_auc = sum(oof_aucs.values())\n        self.l1_weights = {name: auc / total_auc for name, auc in oof_aucs.items()}\n        print(\"Calculated L1 Weights (for Model B):\")\n        for name, weight in self.l1_weights.items(): print(f\"  {name.upper()}: {weight:.4f}\")\n\n        oof_weighted_avg = (oof_lgb_avg * self.l1_weights['lgb'] + \n                            oof_cat_avg * self.l1_weights['cat'] + \n                            oof_ridge_avg * self.l1_weights['ridge'])\n        oof_auc_weighted = roc_auc_score(y_train, oof_weighted_avg)\n        print(f\"\\nL2 Weighted Avg OOF AUC (Model B): {oof_auc_weighted:.8f}\")\n\n        print(\"\\nTraining L2 Meta-Model (Model A)...\"); \n        X_meta_train = np.stack([oof_lgb_avg, oof_cat_avg, oof_ridge_avg], axis=1)\n        self.meta_model = LogisticRegression(C=0.1, solver='liblinear', random_state=RANDOM_SEED); \n        self.meta_model.fit(X_meta_train, y_train)\n        oof_stack_preds = self.meta_model.predict_proba(X_meta_train)[:, 1]\n        oof_auc_stack = roc_auc_score(y_train, oof_stack_preds)\n        print(f\"L2 Stack OOF AUC (Model A): {oof_auc_stack:.8f}\")\n        \n        oof_final_blend = 0.5 * oof_stack_preds + 0.5 * oof_weighted_avg\n        oof_auc_final_blend = roc_auc_score(y_train, oof_final_blend)\n        \n        print(f\"\\nFINAL L3 BLEND OOF AUC (Averaged): {oof_auc_final_blend:.8f}\"); \n        print(\"Training Final Isotonic Calibrator on L3 Blend...\"); \n        self.calibrator.fit(oof_final_blend, y_train) \n        \n        self.is_trained = True; \n        \n        print(\"\\nTraining Final L1 Models for Model A prediction...\")\n        X_train_full, _ = self.prepare_champion_features(full_train_df, full_train_df.iloc[:1], is_training_fold=False, is_full_training=True)\n        y_train_full = full_train_df['is_cheating'].values\n        \n        print(\"Training final LightGBM...\"); \n        self.final_l1_models['lgb'] = lgb.LGBMClassifier(n_estimators=2000, learning_rate=0.01, max_depth=7, num_leaves=25, min_child_samples=25, subsample=0.7, colsample_bytree=0.6, reg_alpha=0.6, reg_lambda=0.6, random_state=RANDOM_SEED, verbose=-1, n_jobs=-1) \n        self.final_l1_models['lgb'].fit(X_train_full, y_train_full)\n        \n        print(\"Training final CatBoost...\"); \n        self.final_l1_models['cat'] = CatBoostClassifier(iterations=2200, learning_rate=0.02, depth=6, l2_leaf_reg=8, random_state=RANDOM_SEED, verbose=0, task_type=\"CPU\")\n        self.final_l1_models['cat'].fit(X_train_full, y_train_full)\n        \n        print(\"Training final Ridge...\"); \n        ridge_scaler = StandardScaler(); \n        X_train_full_scaled = ridge_scaler.fit_transform(X_train_full)\n        self.final_l1_models['ridge_scaler'] = ridge_scaler\n        self.final_l1_models['ridge'] = RidgeClassifier(alpha=2.0, random_state=RANDOM_SEED)\n        self.final_l1_models['ridge'].fit(X_train_full_scaled, y_train_full)\n        \n        return oof_auc_final_blend, oof_final_blend\n\n    def predict_champion(self, test_df):\n        \"\"\"Generates final predictions using L3 Blend.\"\"\"\n        if not self.is_trained: raise ValueError(\"Classical model not trained!\")\n        print(\"\\n\" + \"=\"*50); print(\"Generating Final Classical Ensemble Predictions (Model A)...\"); print(\"=\"*50)\n        \n        dummy_train_df = test_df.iloc[:1].copy() \n        dummy_train_df['is_cheating'] = 0\n        \n        print(\"Preparing features for test data...\")\n        _, X_test = self.prepare_champion_features(dummy_train_df, test_df, is_training_fold=False, is_full_training=False)\n        \n        test_preds_l1 = {}\n        \n        print(\"Predicting with final LightGBM...\");\n        test_preds_l1['lgb'] = self.final_l1_models['lgb'].predict_proba(X_test)[:, 1]\n        \n        print(\"Predicting with final CatBoost...\");\n        test_preds_l1['cat'] = self.final_l1_models['cat'].predict_proba(X_test)[:, 1]\n        \n        print(\"Predicting with final Ridge...\");\n        X_test_scaled = self.final_l1_models['ridge_scaler'].transform(X_test)\n        ridge_scores = self.final_l1_models['ridge'].decision_function(X_test_scaled)\n        test_preds_l1['ridge'] = 1 / (1 + np.exp(-ridge_scores))\n        \n        print(\"Applying Classical L2 Meta-Model (Model A)...\"); \n        X_meta_test = np.stack([test_preds_l1['lgb'], test_preds_l1['cat'], test_preds_l1['ridge']], axis=1)\n        preds_stack_test = self.meta_model.predict_proba(X_meta_test)[:, 1]\n        \n        print(\"Applying Classical L2 Weighted Average (Model B)...\"); \n        preds_weighted_test = (test_preds_l1['lgb'] * self.l1_weights['lgb'] +\n                               test_preds_l1['cat'] * self.l1_weights['cat'] +\n                               test_preds_l1['ridge'] * self.l1_weights['ridge'])\n                               \n        print(\"Blending Model A and Model B (50/50)...\")\n        final_proba_uncalibrated = 0.5 * preds_stack_test + 0.5 * preds_weighted_test\n        \n        print(\"Applying Final Isotonic Calibration...\"); \n        calibrated_proba = self.calibrator.transform(final_proba_uncalibrated)\n        calibrated_proba = np.clip(calibrated_proba, 0.001, 0.999)\n        \n        return calibrated_proba, final_proba_uncalibrated\n\n\n# ===================================================================\n# END: MODEL A\n# ===================================================================\n\n\n# ===================================================================\n# START: MODEL B (User's Roberta-Large Notebook)\n# ===================================================================\n\n# --- Custom Prompt Function ---\ndef prompt(topic,answer):\n    return f'''Predict if AI generated text was used:\\nTopic:{topic}\\nAnswer:{answer}\\n    '''\n\n# --- FIXED: Custom Dataset Class ---\nclass getData(Dataset):\n    def __init__(self, df: pd.DataFrame, tokenizer, max_len=512, is_test=False):\n        self.data = df\n        self.texts = self.data.input.tolist()\n        self.is_test = is_test\n        # FIX: Only access is_cheating if not test data\n        if not is_test:\n            self.labels = self.data.is_cheating.tolist()\n        else:\n            self.labels = [0] * len(self.data)  # Dummy labels for test\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.data)\n        \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,       \n            truncation=True,               \n            return_token_type_ids=False,   \n            return_attention_mask=True,    \n        )\n        return {\n            'input_ids': encoding['input_ids'],\n            'attention_mask': encoding['attention_mask'],\n            'labels': label\n        }\n\n# --- KFold Loaders Function ---\ndef get_kfold_loaders(df: pd.DataFrame, tokenizer, batch_sizes: list, num_splits: int = 5, target: str = 'is_cheating', device=DEVICE) -> List[Tuple[DataLoader, DataLoader]]:\n    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=RANDOM_SEED)\n    X = df.drop(columns=[target]) \n    y = df[target] \n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    all_loaders = []\n    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n        print(f\"--- Fold {fold+1}/{num_splits} ---\")\n        train_df = df.iloc[train_index].reset_index(drop=True)\n        val_df = df.iloc[val_index].reset_index(drop=True)\n        weights = compute_class_weight('balanced', classes=np.array([0,1]), y=train_df.is_cheating)\n        train_dataset = getData(train_df, tokenizer, is_test=False)\n        val_dataset = getData(val_df, tokenizer, is_test=False)\n        train_loader = DataLoader(\n            train_dataset, batch_size=batch_sizes[0], shuffle=True,\n            collate_fn=data_collator, num_workers=2, pin_memory=True\n        )\n        val_loader = DataLoader(\n            val_dataset, batch_size=batch_sizes[1], shuffle=False,\n            collate_fn=data_collator, num_workers=2, pin_memory=True\n        )\n        weights_tensor = torch.tensor(weights, dtype=torch.float, device=device)\n        all_loaders.append((train_loader, val_loader, weights_tensor, val_index))\n    return all_loaders\n\n# --- Train/Eval Functions ---\ndef train_one_epoch(model, loader, optimizer, loss_fn, scheduler, DEVICE):\n    model.train()\n    total_loss = 0.0\n    for batch_idx, batch in enumerate(loader):\n        input_ids = batch['input_ids'].to(DEVICE)\n        attention_mask = batch['attention_mask'].to(DEVICE)\n        labels = batch['labels'].to(DEVICE).long()\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits \n        loss = loss_fn(logits, labels) \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n\ndef evaluate_one_epoch(model, loader, loss_fn, DEVICE):\n    model.eval()\n    total_loss = 0.0\n    all_labels = []\n    all_probabilities = []\n    all_predictions = []\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(loader):\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            labels = batch['labels'].to(DEVICE).long()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]\n            loss = loss_fn(logits, labels)\n            total_loss += loss.item()\n            probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n            _, predicted_classes = torch.max(logits, dim=1)\n            all_labels.extend(labels.cpu().numpy())\n            all_probabilities.extend(probabilities)\n            all_predictions.extend(predicted_classes.cpu().numpy())\n    avg_loss = total_loss / len(loader)\n    all_probabilities = np.array(all_probabilities)\n    auc_roc = roc_auc_score(all_labels, all_probabilities[:, 1])\n    acc = accuracy_score(all_labels, all_predictions)\n    return avg_loss, acc, auc_roc, all_probabilities\n\n# --- Main Training/Prediction Function for Model B ---\ndef get_roberta_large_predictions(train_df, test_df):\n    print(\"\\n\" + \"=\"*50); print(\"STARTING MODEL B (RoBERTa-Large 5-Fold) PIPELINE\"); print(\"=\"*50)\n    \n    LR = 5e-5\n    NUM_EPOCHS = 50\n    WARMUPS = 500\n    batches = [8, 16]\n    patience = 5\n    model_id = LLM_MODEL_NAME_B\n    \n    print(\"Applying custom prompt to Model B data...\")\n    train_df['input'] = train_df.apply(lambda row: prompt(row['topic'], row['answer']), axis=1)\n    test_df['input'] = test_df.apply(lambda row: prompt(row['topic'], row['answer']), axis=1)\n    \n    tokenizer = RobertaTokenizer.from_pretrained(model_id)\n    all_loaders = get_kfold_loaders(train_df, tokenizer, batches, num_splits=N_SPLITS_ROBERTA)\n    \n    print(\"Preparing Test Loader for Model B...\")\n    test_dataset = getData(test_df, tokenizer, is_test=True)  # FIX: Pass is_test=True\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    test_loader = DataLoader(\n        test_dataset, batch_size=batches[1], shuffle=False, \n        collate_fn=data_collator, num_workers=2, pin_memory=True\n    )\n    \n    oof_preds = np.zeros(len(train_df))\n    test_preds_all_folds = []\n    \n    for idx, (train_loader, val_loader, weights, val_index) in enumerate(tqdm(all_loaders, desc=\"CV Folds (Model B)\")):\n        torch.cuda.empty_cache()\n        print(f'--- Training Model B, Fold {idx+1}/{N_SPLITS_ROBERTA} ---')\n        model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n        model.to(DEVICE)\n\n        all_steps = len(train_loader) * NUM_EPOCHS\n        optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=WARMUPS, num_training_steps=all_steps)\n        loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n\n        best_metric = float('inf')\n        trigger = 0\n        \n        for epochs in trange(NUM_EPOCHS, desc=f\"Epochs Fold {idx+1}\"):\n            training_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, scheduler, DEVICE)\n            avg_val_loss, val_acc, aur_roc, val_probs = evaluate_one_epoch(model, val_loader, loss_fn, DEVICE)\n            print(f'EPOCH: {epochs+1}, TrainLoss: {training_loss:.4f}, ValLoss: {avg_val_loss:.4f}, AUC: {aur_roc:.4f}, Acc: {val_acc:.4f}')\n\n            if avg_val_loss < best_metric:\n                best_metric = avg_val_loss\n                trigger = 0\n                print(f'Saving Best Model B, Fold {idx+1}')\n                torch.save(model.state_dict(), f'Roberta_Fold{idx+1}.pth')\n                oof_preds[val_index] = val_probs[:, 1]\n            else:\n                trigger += 1\n            if trigger >= patience:\n                print('-----EARLY STOPPING-----')\n                break\n        \n        print(f\"Loading best Model B, Fold {idx+1} for test prediction...\")\n        model.load_state_dict(torch.load(f'Roberta_Fold{idx+1}.pth'))\n        model.eval()\n        fold_test_probs = []\n        with torch.no_grad():\n            for batch in tqdm(test_loader, desc=f\"Test Pred Fold {idx+1}\"):\n                input_ids = batch['input_ids'].to(DEVICE)\n                attention_mask = batch['attention_mask'].to(DEVICE)\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                logits = outputs.logits\n                probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n                fold_test_probs.append(probabilities)\n        test_preds_all_folds.append(np.concatenate(fold_test_probs, axis=0))\n\n    print(\"Averaging Model B test predictions across all 5 folds...\")\n    avg_test_preds = np.mean(np.stack(test_preds_all_folds, axis=0), axis=0)[:, 1]\n    \n    print(\"=\"*50); print(\"COMPLETED MODEL B PIPELINE\"); print(\"=\"*50)\n    return oof_preds, avg_test_preds\n\n# ===================================================================\n# END: MODEL B\n# ===================================================================\n\n\n# -------------------------------------------------------------------\n# 4. MAIN EXECUTION (Hybrid Model A + Model B)\n# -------------------------------------------------------------------\ndef champion_main():\n    \"\"\"Main function: Runs BOTH pipelines and blends them.\"\"\"\n    print(\"=\" * 60); print(f\"  Mercor AI Text Detection - CHAMPION v29 (Hybrid Monster)\"); print(\"=\" * 60)\n    train_df = pd.read_csv('/kaggle/input/mercor-ai-detection/train.csv'); \n    test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv')\n    print(f\"Training data shape: {train_df.shape}\"); print(f\"Test data shape: {test_df.shape}\")\n    train_df['answer'] = train_df['answer'].fillna(''); test_df['answer'] = test_df['answer'].fillna('')\n    \n    all_texts_A = pd.concat([train_df['answer'], test_df['answer']])\n    llm_preds_all_A = get_llm_predictions_A(all_texts_A, LLM_MODEL_NAME_A)\n    llm_oof_A = llm_preds_all_A[:len(train_df)]; llm_test_A = llm_preds_all_A[len(train_df):]\n    \n    print(\"\\nCalibrating Model A (Desklib) predictions...\"); \n    calibrator_A = IsotonicRegression(out_of_bounds='clip')\n    calibrator_A.fit(llm_oof_A, train_df['is_cheating'].values)\n    llm_oof_calibrated_A = calibrator_A.transform(llm_oof_A)\n    llm_test_calibrated_A = calibrator_A.transform(llm_test_A)\n    print(f\"Calibrated Model A OOF AUC: {roc_auc_score(train_df['is_cheating'], llm_oof_calibrated_A):.8f}\")\n    \n    train_df_A = train_df.copy()\n    test_df_A = test_df.copy()\n    train_df_A['llm_pred'] = llm_oof_calibrated_A\n    test_df_A['llm_pred'] = llm_test_calibrated_A\n    \n    classical_detector = ChampionAIDetector_A(n_splits=N_SPLITS_CLASSICAL, n_repeats=N_REPEATS_CLASSICAL, n_features=N_FEATURES_CLASSICAL)\n    oof_auc_A, oof_preds_A = classical_detector.train_champion_ensemble(train_df_A)\n    print(\"\\nStarting Final Test Set Prediction for Model A...\");\n    preds_A, _ = classical_detector.predict_champion(test_df_A)\n    \n    train_df_B = train_df.copy()\n    test_df_B = test_df.copy()\n    oof_preds_B_uncalibrated, test_preds_B_uncalibrated = get_roberta_large_predictions(train_df_B, test_df_B)\n    \n    print(\"\\nCalibrating Model B (RoBERTa) predictions...\"); \n    calibrator_B = IsotonicRegression(out_of_bounds='clip')\n    calibrator_B.fit(oof_preds_B_uncalibrated, train_df['is_cheating'].values)\n    oof_preds_B = calibrator_B.transform(oof_preds_B_uncalibrated)\n    preds_B = calibrator_B.transform(test_preds_B_uncalibrated)\n    print(f\"Calibrated Model B OOF AUC: {roc_auc_score(train_df['is_cheating'], oof_preds_B):.8f}\")\n    \n    print(\"\\n\" + \"=\"*50); print(\"Blending Model A and Model B (50/50)...\"); print(\"=\"*50)\n    final_blend_oof = 0.5 * oof_preds_A + 0.5 * oof_preds_B\n    final_blend_test = 0.5 * preds_A + 0.5 * preds_B\n    \n    final_oof_auc = roc_auc_score(train_df['is_cheating'], final_blend_oof)\n    \n    print(\"Training Final Isotonic Calibrator on the 50/50 blend...\"); \n    final_calibrator = IsotonicRegression(out_of_bounds='clip')\n    final_calibrator.fit(final_blend_oof, train_df['is_cheating'].values)\n    final_preds_calibrated = final_calibrator.transform(final_blend_test)\n    final_preds_calibrated = np.clip(final_preds_calibrated, 0.001, 0.999)\n    \n    submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': final_preds_calibrated})\n    submission.to_csv('submission.csv', index=False, float_format='%.10f')\n    \n    print(\"\\n\" + \"=\"*60); print(\"CHAMPION HYBRID PIPELINE COMPLETED SUCCESSFULLY!\");\n    print(f\"Model A (L3 Stack) OOF AUC: {oof_auc_A:.8f}\")\n    print(f\"Model B (RoBERTa) OOF AUC: {roc_auc_score(train_df['is_cheating'], oof_preds_B):.8f}\")\n    print(f\"FINAL L4 BLEND OOF AUC: {final_oof_auc:.8f}\")\n    print(\"\\nSubmission file created: submission.csv (RECOMMENDED)\")\n    print(f\"\\nFinal 'submission.csv' prediction statistics:\"); print(f\"  Min: {final_preds_calibrated.min():.6f}\"); print(f\"  Max: {final_preds_calibrated.max():.6f}\"); print(f\"  Mean: {final_preds_calibrated.mean():.6f}\"); print(f\"  Median: {np.median(final_preds_calibrated):.6f}\")\n    print(\"\\nTop 10 Predictions:\"); print(submission.head(10).to_string(index=False)); print(\"=\" * 60)\n\n# --- (Fallback model - simplified from Model A) ---\ndef champion_fallback():\n    \"\"\"Fallback using basic LGBM, basic features.\"\"\"\n    print(\"CRITICAL ERROR: Main pipeline failed. Using CHAMPION FALLBACK model...\")\n    try:\n        from sklearn.preprocessing import StandardScaler; from scipy.sparse import hstack, csr_matrix;\n        train_df = pd.read_csv('/kaggle/input/mercor-ai-detection/train.csv'); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv')\n        feature_extractor = ChampionFeatureExtractor(); train_base_features = feature_extractor.extract_base_features(train_df.drop(columns=['llm_pred'], errors='ignore'))\n        test_base_features = feature_extractor.extract_base_features(test_df.drop(columns=['llm_pred'], errors='ignore'))\n        train_topic_dummies = pd.get_dummies(train_df['topic'], prefix='topic', dtype=int); fallback_topic_columns = train_topic_dummies.columns; test_topic_dummies = pd.get_dummies(test_df['topic'], prefix='topic', dtype=int).reindex(columns=fallback_topic_columns, fill_value=0)\n        train_dense_features = pd.concat([train_base_features, train_topic_dummies], axis=1); test_dense_features = pd.concat([test_base_features, test_topic_dummies], axis=1).reindex(columns=train_dense_features.columns, fill_value=0)\n        scaler = StandardScaler(); train_dense_scaled = scaler.fit_transform(train_dense_features); test_dense_scaled = scaler.transform(test_dense_features)\n        tfidf = TfidfVectorizer(max_features=2500, ngram_range=(1, 3), stop_words='english'); train_tfidf = tfidf.fit_transform(train_df['answer'].fillna('')); test_tfidf = tfidf.transform(test_df['answer'].fillna(''))\n        X_train = hstack([csr_matrix(train_dense_scaled), train_tfidf]).tocsr(); X_test = hstack([csr_matrix(test_dense_scaled), test_tfidf]).tocsr(); y_train = train_df['is_cheating']\n        model = lgb.LGBMClassifier(n_estimators=1000, learning_rate=0.01, max_depth=7, num_leaves=63, random_state=RANDOM_SEED, verbose=-1, n_jobs=-1, reg_alpha=0.1, reg_lambda=0.1)\n        model.fit(X_train, y_train); test_proba = model.predict_proba(X_test.toarray())[:, 1]\n        submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': test_proba}); submission.to_csv('submission.csv', index=False, float_format='%.10f')\n        print(\"Fallback submission.csv created successfully.\"); return submission\n    except Exception as e:\n        print(f\"Fallback model ALSO failed: {e}\"); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv'); submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': 0.5}); submission.to_csv('submission.csv', index=False, float_format='%.10f'); print(\"CRITICAL: Created dummy submission.csv with 0.5 probability.\"); return submission\n\n# --- Run the Champion Pipeline ---\nif __name__ == \"__main__\":\n    start_time = time.time()\n    try:\n        gc.collect(); torch.cuda.empty_cache()\n        champion_main()\n    except Exception as e:\n        print(f\"\\n!!! AN ERROR OCCURRED IN THE MAIN PIPELINE !!!: {e}\")\n        import traceback; traceback.print_exc()\n        champion_fallback()\n    finally:\n        gc.collect(); torch.cuda.empty_cache(); end_time = time.time()\n        print(f\"\\nTotal execution time: {(end_time - start_time) / 60:.2f} minutes\")","metadata":{"execution":{"iopub.status.busy":"2025-11-10T09:50:32.376028Z","iopub.execute_input":"2025-11-10T09:50:32.376617Z","iopub.status.idle":"2025-11-10T10:52:40.360427Z","shell.execute_reply.started":"2025-11-10T09:50:32.376589Z","shell.execute_reply":"2025-11-10T10:52:40.359687Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Installing/Updating transformers...\nLibraries installed.\nAll libraries imported successfully!\n============================================================\n  Mercor AI Text Detection - CHAMPION v29 (Hybrid Monster)\n============================================================\nTraining data shape: (269, 4)\nTest data shape: (264, 3)\n\n--- Starting LLM Inference (Model A: desklib/ai-text-detector-v1.01) ---\nUsing device: cuda\nLoading Tokenizer: desklib/ai-text-detector-v1.01...\nLoading Model (Custom Class): desklib/ai-text-detector-v1.01...\nModel and Tokenizer loaded successfully.\nRunning LLM inference on 533 texts...\nUsing batch size: 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"LLM 1 (ai-text-detector-v1.01):   0%|          | 0/34 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad938ba188e449869663b0d34a149055"}},"metadata":{}},{"name":"stdout","text":"LLM 1 (ai-text-detector-v1.01) prediction feature generated.\n--- Finished LLM 1 Inference ---\n\nCalibrating Model A (Desklib) predictions...\nCalibrated Model A OOF AUC: 0.96113527\n\nClassical Detector (L3 Ensemble): 5 splits, 3 repeats, SelectKBest(k=850).\n\n--- Starting Classical Ensemble Training ---\nTraining set size: 269\nPositive class ratio: 0.546468\nFitting topic columns on full training data...\nIdentified 268 topic columns during fit.\n\nFitting Scalers, TFIDF, and Selector on FULL training data...\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nIdentified 268 topic columns during fit.\nFitting/Transforming dense features (excl. LLM)...\nFitting Word TF-IDF...\nFitting Char TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(269, 3683), Test=(269, 3683)\nFitting SelectKBest (k=850)...\nFinal training feature shape: (269, 850)\nFinal testing feature shape: (269, 850)\n\nStarting Repeated K-Fold Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"CV Folds (Model A):   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33fa79d97393498089362459eea6c24f"}},"metadata":{}},{"name":"stdout","text":"\n==================== Repeat 1/3, Split 1/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 1: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 1 LGB AUC: 0.995833\nFold 1 CAT AUC: 0.998611\nFold 1 RIDGE AUC: 0.986111\n\n==================== Repeat 1/3, Split 2/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 2: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 2 LGB AUC: 1.000000\nFold 2 CAT AUC: 0.998611\nFold 2 RIDGE AUC: 0.998611\n\n==================== Repeat 1/3, Split 3/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 3: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 3 LGB AUC: 0.966897\nFold 3 CAT AUC: 0.954483\nFold 3 RIDGE AUC: 0.983448\n\n==================== Repeat 1/3, Split 4/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 4: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 4 LGB AUC: 0.977931\nFold 4 CAT AUC: 0.984828\nFold 4 RIDGE AUC: 0.976552\n\n==================== Repeat 1/3, Split 5/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(216, 3683), Test=(53, 3683)\nFinal training feature shape: (216, 850)\nFinal testing feature shape: (53, 850)\nFold 5: Train shape (216, 850), Val shape (53, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 5 LGB AUC: 0.988506\nFold 5 CAT AUC: 0.991379\nFold 5 RIDGE AUC: 0.979885\n\n==================== Repeat 2/3, Split 1/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 6: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 6 LGB AUC: 1.000000\nFold 6 CAT AUC: 1.000000\nFold 6 RIDGE AUC: 0.987500\n\n==================== Repeat 2/3, Split 2/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 7: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 7 LGB AUC: 1.000000\nFold 7 CAT AUC: 1.000000\nFold 7 RIDGE AUC: 0.983333\n\n==================== Repeat 2/3, Split 3/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 8: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 8 LGB AUC: 0.965517\nFold 8 CAT AUC: 0.968276\nFold 8 RIDGE AUC: 0.986207\n\n==================== Repeat 2/3, Split 4/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 9: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 9 LGB AUC: 0.953103\nFold 9 CAT AUC: 0.936552\nFold 9 RIDGE AUC: 0.925517\n\n==================== Repeat 2/3, Split 5/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(216, 3683), Test=(53, 3683)\nFinal training feature shape: (216, 850)\nFinal testing feature shape: (53, 850)\nFold 10: Train shape (216, 850), Val shape (53, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 10 LGB AUC: 1.000000\nFold 10 CAT AUC: 1.000000\nFold 10 RIDGE AUC: 0.995690\n\n==================== Repeat 3/3, Split 1/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 11: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 11 LGB AUC: 0.973611\nFold 11 CAT AUC: 0.979167\nFold 11 RIDGE AUC: 0.995833\n\n==================== Repeat 3/3, Split 2/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 12: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 12 LGB AUC: 0.997222\nFold 12 CAT AUC: 0.988889\nFold 12 RIDGE AUC: 0.986111\n\n==================== Repeat 3/3, Split 3/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 13: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 13 LGB AUC: 0.947586\nFold 13 CAT AUC: 0.932414\nFold 13 RIDGE AUC: 0.947586\n\n==================== Repeat 3/3, Split 4/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 14: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 14 LGB AUC: 0.998621\nFold 14 CAT AUC: 0.997241\nFold 14 RIDGE AUC: 1.000000\n\n==================== Repeat 3/3, Split 5/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(216, 3683), Test=(53, 3683)\nFinal training feature shape: (216, 850)\nFinal testing feature shape: (53, 850)\nFold 15: Train shape (216, 850), Val shape (53, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 15 LGB AUC: 0.981322\nFold 15 CAT AUC: 0.988506\nFold 15 RIDGE AUC: 0.975575\n\n==================================================\nAveraging OOF Scores across repeats:\n  LGB OOF AUC (Averaged): 0.98566968\n  CAT OOF AUC (Averaged): 0.98522360\n  RIDGE OOF AUC (Averaged): 0.98461024\nCalculated L1 Weights (for Model B):\n  LGB: 0.3335\n  CAT: 0.3334\n  RIDGE: 0.3331\n\nL2 Weighted Avg OOF AUC (Model B): 0.98840192\n\nTraining L2 Meta-Model (Model A)...\nL2 Stack OOF AUC (Model A): 0.98728672\n\nFINAL L3 BLEND OOF AUC (Averaged): 0.98790008\nTraining Final Isotonic Calibrator on L3 Blend...\n\nTraining Final L1 Models for Model A prediction...\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nIdentified 268 topic columns during fit.\nFitting/Transforming dense features (excl. LLM)...\nFitting Word TF-IDF...\nFitting Char TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(269, 3683), Test=(1, 3683)\nFitting SelectKBest (k=850)...\nFinal training feature shape: (269, 850)\nFinal testing feature shape: (1, 850)\nTraining final LightGBM...\nTraining final CatBoost...\nTraining final Ridge...\n\nStarting Final Test Set Prediction for Model A...\n\n==================================================\nGenerating Final Classical Ensemble Predictions (Model A)...\n==================================================\nPreparing features for test data...\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(1, 3683), Test=(264, 3683)\nFinal training feature shape: (1, 850)\nFinal testing feature shape: (264, 850)\nPredicting with final LightGBM...\nPredicting with final CatBoost...\nPredicting with final Ridge...\nApplying Classical L2 Meta-Model (Model A)...\nApplying Classical L2 Weighted Average (Model B)...\nBlending Model A and Model B (50/50)...\nApplying Final Isotonic Calibration...\n\n==================================================\nSTARTING MODEL B (RoBERTa-Large 5-Fold) PIPELINE\n==================================================\nApplying custom prompt to Model B data...\n--- Fold 1/5 ---\n--- Fold 2/5 ---\n--- Fold 3/5 ---\n--- Fold 4/5 ---\n--- Fold 5/5 ---\nPreparing Test Loader for Model B...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"CV Folds (Model B):   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69598f6751c94fce8b5b727f51831c39"}},"metadata":{}},{"name":"stdout","text":"--- Training Model B, Fold 1/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"138da451fbc74ea485dcff241354eb72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epochs Fold 1:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe718844e3274c41b9126df84722a222"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 1, TrainLoss: 0.6956, ValLoss: 0.6329, AUC: 0.8639, Acc: 0.4444\nSaving Best Model B, Fold 1\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 2, TrainLoss: 0.5275, ValLoss: 0.4246, AUC: 0.9847, Acc: 0.9259\nSaving Best Model B, Fold 1\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 3, TrainLoss: 0.3690, ValLoss: 0.2500, AUC: 0.9875, Acc: 0.9259\nSaving Best Model B, Fold 1\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 4, TrainLoss: 0.1559, ValLoss: 0.2956, AUC: 0.9833, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 5, TrainLoss: 0.1075, ValLoss: 0.0931, AUC: 0.9972, Acc: 0.9444\nSaving Best Model B, Fold 1\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 6, TrainLoss: 0.0127, ValLoss: 0.4341, AUC: 0.9903, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 7, TrainLoss: 0.0019, ValLoss: 0.4522, AUC: 0.9931, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 8, TrainLoss: 0.0007, ValLoss: 0.3836, AUC: 0.9931, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 9, TrainLoss: 0.0005, ValLoss: 0.3527, AUC: 0.9944, Acc: 0.9444\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 10, TrainLoss: 0.0004, ValLoss: 0.3345, AUC: 0.9958, Acc: 0.9444\n-----EARLY STOPPING-----\nLoading best Model B, Fold 1 for test prediction...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Pred Fold 1:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"057feb1154244076b1d5ad7e8ea2ecfa"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--- Training Model B, Fold 2/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs Fold 2:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"532b753253ad4b11a56d68632ff872a1"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 1, TrainLoss: 0.6802, ValLoss: 0.6359, AUC: 0.9708, Acc: 0.5556\nSaving Best Model B, Fold 2\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 2, TrainLoss: 0.4883, ValLoss: 0.1836, AUC: 0.9861, Acc: 0.9815\nSaving Best Model B, Fold 2\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 3, TrainLoss: 0.1869, ValLoss: 0.1356, AUC: 0.9917, Acc: 0.9630\nSaving Best Model B, Fold 2\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 4, TrainLoss: 0.1142, ValLoss: 0.2535, AUC: 0.9986, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 5, TrainLoss: 0.0350, ValLoss: 0.1891, AUC: 0.9917, Acc: 0.9074\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 6, TrainLoss: 0.0400, ValLoss: 0.0610, AUC: 0.9958, Acc: 0.9444\nSaving Best Model B, Fold 2\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 7, TrainLoss: 0.0143, ValLoss: 0.0731, AUC: 0.9958, Acc: 0.9444\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 8, TrainLoss: 0.0068, ValLoss: 0.0964, AUC: 0.9972, Acc: 0.9444\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 9, TrainLoss: 0.0018, ValLoss: 0.3306, AUC: 0.9917, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 10, TrainLoss: 0.0006, ValLoss: 0.2753, AUC: 0.9931, Acc: 0.9444\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 11, TrainLoss: 0.0003, ValLoss: 0.3156, AUC: 0.9944, Acc: 0.9259\n-----EARLY STOPPING-----\nLoading best Model B, Fold 2 for test prediction...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Pred Fold 2:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75f0c943bfdc46f3925cc2bb8d479105"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--- Training Model B, Fold 3/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs Fold 3:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da5eb8e452b044ea804ffe8e5c084046"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 1, TrainLoss: 0.6929, ValLoss: 0.6581, AUC: 0.9393, Acc: 0.8519\nSaving Best Model B, Fold 3\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 2, TrainLoss: 0.5641, ValLoss: 0.4159, AUC: 0.9793, Acc: 0.9259\nSaving Best Model B, Fold 3\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 3, TrainLoss: 0.2949, ValLoss: 0.3016, AUC: 0.9931, Acc: 0.8704\nSaving Best Model B, Fold 3\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 4, TrainLoss: 0.1324, ValLoss: 0.3072, AUC: 0.9848, Acc: 0.8889\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 5, TrainLoss: 0.0683, ValLoss: 0.2190, AUC: 0.9890, Acc: 0.9444\nSaving Best Model B, Fold 3\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 6, TrainLoss: 0.1089, ValLoss: 0.3014, AUC: 0.9710, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 7, TrainLoss: 0.0420, ValLoss: 0.3777, AUC: 0.9876, Acc: 0.9444\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 8, TrainLoss: 0.0512, ValLoss: 0.5243, AUC: 0.9655, Acc: 0.8889\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 9, TrainLoss: 0.0158, ValLoss: 0.5644, AUC: 0.9641, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 10, TrainLoss: 0.0021, ValLoss: 0.4490, AUC: 0.9628, Acc: 0.9444\n-----EARLY STOPPING-----\nLoading best Model B, Fold 3 for test prediction...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Pred Fold 3:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c38de6e19254cc592d326bf871e2a0d"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--- Training Model B, Fold 4/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs Fold 4:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d98017de667e4462b37ae262a3e8c40f"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 1, TrainLoss: 0.6692, ValLoss: 0.6117, AUC: 0.9724, Acc: 0.5370\nSaving Best Model B, Fold 4\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 2, TrainLoss: 0.4823, ValLoss: 0.3439, AUC: 0.9972, Acc: 0.8519\nSaving Best Model B, Fold 4\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 3, TrainLoss: 0.2717, ValLoss: 0.2234, AUC: 0.9931, Acc: 0.9444\nSaving Best Model B, Fold 4\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 4, TrainLoss: 0.1866, ValLoss: 0.1679, AUC: 0.9917, Acc: 0.9444\nSaving Best Model B, Fold 4\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 5, TrainLoss: 0.0993, ValLoss: 0.0752, AUC: 0.9972, Acc: 0.9630\nSaving Best Model B, Fold 4\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 6, TrainLoss: 0.0540, ValLoss: 0.0940, AUC: 0.9931, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 7, TrainLoss: 0.0279, ValLoss: 0.2784, AUC: 0.9876, Acc: 0.9074\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 8, TrainLoss: 0.0033, ValLoss: 0.1552, AUC: 0.9917, Acc: 0.9444\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 9, TrainLoss: 0.0018, ValLoss: 0.3268, AUC: 0.9848, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 10, TrainLoss: 0.0010, ValLoss: 0.1918, AUC: 0.9931, Acc: 0.9630\n-----EARLY STOPPING-----\nLoading best Model B, Fold 4 for test prediction...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Pred Fold 4:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b2e2e610a1c4c368d2bc9b15bbf199b"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--- Training Model B, Fold 5/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs Fold 5:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"647625be14f542e48c5e1fcda0624a11"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 1, TrainLoss: 0.6701, ValLoss: 0.6560, AUC: 0.9612, Acc: 0.5472\nSaving Best Model B, Fold 5\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 2, TrainLoss: 0.4842, ValLoss: 0.2044, AUC: 0.9684, Acc: 0.9245\nSaving Best Model B, Fold 5\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 3, TrainLoss: 0.1624, ValLoss: 0.1407, AUC: 0.9885, Acc: 0.9434\nSaving Best Model B, Fold 5\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 4, TrainLoss: 0.1274, ValLoss: 0.2321, AUC: 0.9756, Acc: 0.9057\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 5, TrainLoss: 0.1808, ValLoss: 0.1008, AUC: 0.9885, Acc: 0.9623\nSaving Best Model B, Fold 5\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 6, TrainLoss: 0.0597, ValLoss: 0.1923, AUC: 0.9943, Acc: 0.9434\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 7, TrainLoss: 0.0356, ValLoss: 0.1697, AUC: 0.9928, Acc: 0.9434\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 8, TrainLoss: 0.0218, ValLoss: 0.1931, AUC: 0.9957, Acc: 0.9623\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 9, TrainLoss: 0.0096, ValLoss: 0.1613, AUC: 0.9957, Acc: 0.9434\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 10, TrainLoss: 0.0149, ValLoss: 0.2044, AUC: 0.9957, Acc: 0.9434\n-----EARLY STOPPING-----\nLoading best Model B, Fold 5 for test prediction...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Pred Fold 5:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e96b1c05e75f41c9b52ad33e5d70752b"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Averaging Model B test predictions across all 5 folds...\n==================================================\nCOMPLETED MODEL B PIPELINE\n==================================================\n\nCalibrating Model B (RoBERTa) predictions...\nCalibrated Model B OOF AUC: 0.99453552\n\n==================================================\nBlending Model A and Model B (50/50)...\n==================================================\nTraining Final Isotonic Calibrator on the 50/50 blend...\n\n============================================================\nCHAMPION HYBRID PIPELINE COMPLETED SUCCESSFULLY!\nModel A (L3 Stack) OOF AUC: 0.98790008\nModel B (RoBERTa) OOF AUC: 0.99453552\nFINAL L4 BLEND OOF AUC: 0.99464704\n\nSubmission file created: submission.csv (RECOMMENDED)\n\nFinal 'submission.csv' prediction statistics:\n  Min: 0.001000\n  Max: 0.999000\n  Mean: 0.536869\n  Median: 0.972222\n\nTop 10 Predictions:\n              id  is_cheating\nscr_81822029c661        0.125\nscr_52efb19e0ea9        0.999\nscr_8fc0f33c559e        0.125\nscr_bac3f5d3aa12        0.001\nscr_adfbe009984d        0.001\nscr_9e08ece19277        0.999\nscr_0e34514f3cd4        0.999\nscr_b10d808b5528        0.999\nscr_2024f1e7bf94        0.125\nscr_aa0b11f10fff        0.001\n============================================================\n\nTotal execution time: 62.08 minutes\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}