{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":117171,"databundleVersionId":14089262,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":881.724544,"end_time":"2025-10-13T08:26:55.016349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-13T08:12:13.291805","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"dd=pd.read_csv(\"/kaggle/working/submission.csv\")\ndd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nMercor AI Text Detection \n=====================================================\n\nv29: The \"Hybrid Monster\" \n- This is an ensemble of two complete, independent 0.99+ models.\n\n- MODEL A :\n    - 'desklib/ai-text-detector-v1.01' (0.96 OOF) as the LLM feature.\n    - RepeatedStratifiedKFold (5 splits, 3 repeats).\n    - All handcrafted features + SelectKBest(k=850).\n    - L3 Ensemble (L2 Stack + L2 Weighted Avg) on (LGBM, CatBoost, Ridge).\n    - Max Regularization.\n    \n- MODEL B :\n    - Model: 'FacebookAI/roberta-large'.\n    - Feature: Custom Prompt (Topic + Answer).\n    - Method: 5-Fold StratifiedKFold training.\n    - Prediction: Average of the 5 trained models.\n\n- FINAL SUBMISSION:\n    - 0.3 * Model_A_Preds + 0.7 * Model_B_Preds\n\"\"\"\n\n# === General Imports ===\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport gc\nimport warnings\nimport time\nimport os\nfrom tqdm.auto import tqdm, trange\nfrom typing import List, Tuple, Dict\n\n# === Sklearn Imports ===\nfrom sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.utils import shuffle\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# === GBDT Imports ===\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\n# === Hugging Face Imports ===\nprint(\"Installing/Updating transformers...\"); os.system('pip install -q transformers torch'); print(\"Libraries installed.\")\ntry:\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import Dataset, DataLoader\n    from transformers import AutoTokenizer, AutoModel, AutoConfig, PreTrainedModel, AutoModelForSequenceClassification, DataCollatorWithPadding, get_cosine_schedule_with_warmup, RobertaTokenizer\n    import transformers\n    transformers.logging.set_verbosity_error()\nexcept ImportError as e: print(f\"FATAL ERROR during imports: {e}\"); raise e\nprint(\"All libraries imported successfully!\")\n\n# === Configuration ===\nLLM_MODEL_NAME_A = \"desklib/ai-text-detector-v1.01\" \nLLM_MODEL_NAME_B = \"FacebookAI/roberta-large\" # From the notebook\nN_SPLITS_CLASSICAL = 5\nN_REPEATS_CLASSICAL = 3\nN_SPLITS_ROBERTA = 5\nN_FEATURES_CLASSICAL = 850\nMAX_LEN_LLM = 512\nRANDOM_SEED = 42\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# ===================================================================\n# START: MODEL A (Our 0.99006 Pipeline)\n# ===================================================================\n\n# -------------------------------------------------------------------\n# 0a. CUSTOM LLM CLASS (for Desklib)\n# -------------------------------------------------------------------\nclass DesklibAIDetectionModel(PreTrainedModel):\n    config_class = AutoConfig\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = AutoModel.from_config(config)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.init_weights()\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = self.model(input_ids, attention_mask=attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n        pooled_output = sum_embeddings / sum_mask\n        logits = self.classifier(pooled_output)\n        loss = None\n        if labels is not None:\n            loss_fct = nn.BCEWithLogitsLoss()\n            loss = loss_fct(logits.view(-1), labels.float())\n        output = {\"logits\": logits}\n        if loss is not None: output[\"loss\"] = loss\n        return output\n\n# -------------------------------------------------------------------\n# 0b. LLM INFERENCE FUNCTION (for Desklib)\n# -------------------------------------------------------------------\ndef get_llm_predictions_A(texts, model_name):\n    print(f\"\\n--- Starting LLM Inference (Model A: {model_name}) ---\"); model = None; tokenizer = None\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n    if not torch.cuda.is_available(): print(\"!!! WARNING: No GPU for Model A. !!!\")\n    try:\n        print(f\"Loading Tokenizer: {model_name}...\"); tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        print(f\"Loading Model (Custom Class): {model_name}...\"); \n        model = DesklibAIDetectionModel.from_pretrained(model_name, trust_remote_code=True)\n        model.to(device); model.eval()\n        print(\"Model and Tokenizer loaded successfully.\")\n    except Exception as e:\n        print(f\"Error loading model/tokenizer {model_name}: {e}\\nFalling back to 0.5.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); return np.full(len(texts), 0.5)\n    print(f\"Running LLM inference on {len(texts)} texts...\")\n    batch_size = 16\n    print(f\"Using batch size: {batch_size}\")\n    llm_probs = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), batch_size), desc=f\"LLM 1 ({model_name.split('/')[1]})\"):\n            batch_texts = texts[i : i + batch_size].tolist();\n            if not batch_texts: continue\n            try:\n                inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN_LLM)\n                input_ids = inputs['input_ids'].to(device)\n                attention_mask = inputs['attention_mask'].to(device)\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                logits = outputs[\"logits\"]\n                probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n                if not isinstance(probs, (np.ndarray, list)): probs = [probs.item()]\n                elif isinstance(probs, np.ndarray): probs = probs.tolist()\n                llm_probs.extend(probs)\n            except Exception as e: \n                print(f\"Error during LLM batch {i // batch_size}: {e}\"); llm_probs.extend([0.5] * len(batch_texts))\n            if i % (batch_size * 20) == 0: gc.collect(); torch.cuda.empty_cache()\n    print(f\"LLM 1 ({model_name.split('/')[1]}) prediction feature generated.\"); del model; del tokenizer; gc.collect(); torch.cuda.empty_cache(); print(\"--- Finished LLM 1 Inference ---\")\n    if len(llm_probs) != len(texts): print(f\"CRITICAL WARNING: Length mismatch. Padding/Truncating.\"); llm_probs = (llm_probs + [0.5] * len(texts))[:len(texts)]\n    return np.array(llm_probs)\n\n# -------------------------------------------------------------------\n# 1. CHAMPION FEATURE EXTRACTOR (Model A)\n# -------------------------------------------------------------------\nclass ChampionFeatureExtractor(Dataset):\n    \"\"\"Extracts base features, includes llm_pred, + new features.\"\"\"\n    def __init__(self):\n        self.ai_connectors = ['in conclusion', 'in summary', 'furthermore', 'moreover', 'additionally', 'however', 'therefore', 'thus', 'consequently', 'as a result', 'on the other hand', 'for instance', 'for example', 'it is important to note', 'it is worth noting', 'that being said']; self.formal_words = ['utilize', 'facilitate', 'implement', 'methodology', 'paradigm', 'leverage', 'robust', 'optimal', 'enhance', 'demonstrate', 'comprehensive', 'articulate']; self.hedging_words = ['may', 'might', 'could', 'possibly', 'perhaps', 'suggests', 'seems', 'appears', 'likely']; self.passive_indicators = ['is made', 'was made', 'is given', 'was given', 'is shown', 'was shown', 'is considered', 'was considered']\n        self.stopwords = set(ENGLISH_STOP_WORDS); self.sentence_splitter = re.compile(r'[.!?]+')\n    def _get_sent_len_var(self, text):\n        sentences = self.sentence_splitter.split(text); sentence_lengths = [len(s.split()) for s in sentences if len(s.split()) > 0]\n        if len(sentence_lengths) > 1: return np.var(sentence_lengths)\n        else: return 0\n    def extract_base_features(self, df):\n        features = pd.DataFrame(index=df.index);\n        if 'llm_pred' in df.columns: features['llm_pred'] = df['llm_pred']\n        answers = df['answer'].fillna(''); words = answers.str.split(); word_counts = words.str.len()\n        features['text_length'] = answers.str.len(); features['word_count'] = word_counts; features['avg_word_length'] = features['text_length'] / (features['word_count'] + 1e-6); features['sentence_count'] = answers.str.count(r'[.!?]+'); features['avg_sentence_length'] = features['word_count'] / (features['sentence_count'] + 1e-6); features['comma_count'] = answers.str.count(','); features['period_count'] = answers.str.count(r'\\.'); features['exclamation_count'] = answers.str.count(r'!'); features['question_count'] = answers.str.count(r'\\?'); features['punctuation_ratio'] = (features['comma_count'] + features['period_count']) / (features['word_count'] + 1e-6); features['unique_words'] = answers.apply(lambda x: len(set(str(x).lower().split()))); features['ttr'] = features['unique_words'] / (features['word_count'] + 1e-6)\n        def count_phrases(text, phrases): text_lower = str(text).lower(); return sum(1 for phrase in phrases if phrase in text_lower)\n        features['ai_connector_count'] = answers.apply(lambda x: count_phrases(x, self.ai_connectors)); features['formal_word_count'] = answers.apply(lambda x: count_phrases(x, self.formal_words)); features['passive_voice_count'] = answers.apply(lambda x: count_phrases(x, self.passive_indicators)); features['hedging_word_count'] = answers.apply(lambda x: count_phrases(x, self.hedging_words)); features['ai_connector_ratio'] = features['ai_connector_count'] / (features['word_count'] + 1e-6); features['formal_word_ratio'] = features['formal_word_count'] / (features['word_count'] + 1e-6); features['passive_voice_ratio'] = features['passive_voice_count'] / (features['word_count'] + 1e-6); features['hedging_ratio'] = features['hedging_word_count'] / (features['word_count'] + 1e-6)\n        features['subordinate_ratio'] = answers.str.count(r'\\b(that|which|who|when|where|while|although|because|if)\\b') / (features['word_count'] + 1e-6); features['paragraph_count'] = answers.str.count(r'\\n\\n') + 1; features['avg_paragraph_len'] = features['word_count'] / (features['paragraph_count'] + 1e-6); features['word_length_std'] = answers.apply(lambda x: np.std([len(w) for w in str(x).split()]) if len(str(x).split()) > 1 else 0)\n        features['em_dash_count'] = answers.str.count('—'); features['em_dash_ratio'] = features['em_dash_count'] / (features['word_count'] + 1e-6)\n        features['question_mark_count'] = answers.str.count(r'\\?'); features['exclamation_mark_count'] = answers.str.count(r'!')\n        features['stopword_count'] = words.apply(lambda x: sum(1 for word in x if word.lower() in self.stopwords)); features['stopword_ratio'] = features['stopword_count'] / (features['word_count'] + 1e-6)\n        features['uppercase_word_count'] = words.apply(lambda x: sum(1 for word in x if word.isascii() and word.istitle())); features['uppercase_ratio'] = features['uppercase_word_count'] / (features['word_count'] + 1e-6)\n        features['sentence_length_variance'] = answers.apply(self._get_sent_len_var)\n        features.replace([np.inf, -np.inf], 0, inplace=True); features.fillna(0, inplace=True); return features\n\n# -------------------------------------------------------------------\n# 2. CHAMPION AI DETECTOR (L3 Ensemble) (Model A)\n# -------------------------------------------------------------------\nclass ChampionAIDetector_A: \n    \"\"\"Trains L2 Stack AND L2 Weighted Avg (LGBM, CAT, Ridge), blends them 50/50.\"\"\"\n    def __init__(self, n_splits=N_SPLITS_CLASSICAL, n_repeats=N_REPEATS_CLASSICAL, n_features=N_FEATURES_CLASSICAL):\n        self.feature_extractor = ChampionFeatureExtractor(); self.n_splits = n_splits; self.n_repeats = n_repeats; self.n_features = n_features; self.models = {}; self.meta_model = None; self.calibrator = IsotonicRegression(out_of_bounds='clip'); self.scaler = StandardScaler(); self.tfidf_word = None; self.tfidf_char = None; self.selector = None; self.topic_columns = None; self.is_trained = False\n        self.l1_weights = None # For Model B\n        self.final_l1_models = {'lgb': None, 'cat': None, 'ridge': None} # Store final models\n        print(f\"\\nClassical Detector (L3 Ensemble): {n_splits} splits, {n_repeats} repeats, SelectKBest(k={n_features}).\")\n    def _get_consistent_topic_dummies(self, series, fit_columns=False):\n        series = series.fillna(\"Unknown_Topic\"); dummies = pd.get_dummies(series, prefix='topic', dtype=int, dummy_na=False)\n        if fit_columns:\n            self.topic_columns = dummies.columns\n            if \"topic_Unknown_Topic\" not in self.topic_columns and series.astype(str).str.contains(\"Unknown_Topic\").any(): self.topic_columns = self.topic_columns.append(pd.Index([\"topic_Unknown_Topic\"]))\n            print(f\"Identified {len(self.topic_columns)} topic columns during fit.\"); return dummies.reindex(columns=self.topic_columns, fill_value=0)\n        elif self.topic_columns is not None: return dummies.reindex(columns=self.topic_columns, fill_value=0)\n        else: raise ValueError(\"Topic columns not fitted.\")\n    def prepare_champion_features(self, train_df, test_df, is_training_fold=False, is_full_training=False):\n        \"\"\"Prepares features. Fits objects if training. Separates llm_pred.\"\"\"\n        print(\"Extracting base handcrafted + LLM features...\"); train_base_features_df = self.feature_extractor.extract_base_features(train_df); test_base_features_df = self.feature_extractor.extract_base_features(test_df)\n        print(\"Processing topic features...\")\n        fit_topic_cols = (self.topic_columns is None) or is_full_training\n        train_topic_dummies = self._get_consistent_topic_dummies(train_df['topic'], fit_columns=fit_topic_cols); test_topic_dummies = self._get_consistent_topic_dummies(test_df['topic'], fit_columns=False)\n        llm_train_feat = train_base_features_df.pop('llm_pred').values.reshape(-1, 1); llm_test_feat = test_base_features_df.pop('llm_pred').values.reshape(-1, 1)\n        train_dense_features_df = pd.concat([train_base_features_df, train_topic_dummies], axis=1); test_dense_features_df = pd.concat([test_base_features_df, test_topic_dummies], axis=1)\n        test_dense_features_df = test_dense_features_df.reindex(columns=train_dense_features_df.columns, fill_value=0)\n        if is_training_fold or is_full_training: print(\"Fitting/Transforming dense features (excl. LLM)...\"); train_features_scaled = self.scaler.fit_transform(train_dense_features_df); test_features_scaled = self.scaler.transform(test_dense_features_df)\n        else: print(\"Transforming dense features (excl. LLM)...\"); train_features_scaled = self.scaler.transform(train_dense_features_df); test_features_scaled = self.scaler.transform(test_dense_features_df)\n        if is_training_fold or is_full_training:\n            print(\"Fitting Word TF-IDF...\"); self.tfidf_word = TfidfVectorizer(ngram_range=(1, 3), min_df=3, max_df=0.9, sublinear_tf=True, stop_words='english', max_features=2500)\n            train_tfidf_word = self.tfidf_word.fit_transform(train_df['answer'].fillna('')); test_tfidf_word = self.tfidf_word.transform(test_df['answer'].fillna(''))\n            print(\"Fitting Char TF-IDF...\"); self.tfidf_char = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 5), min_df=3, sublinear_tf=True, max_features=1250)\n            train_tfidf_char = self.tfidf_char.fit_transform(train_df['answer'].fillna('')); test_tfidf_char = self.tfidf_char.transform(test_df['answer'].fillna(''))\n        else:\n            if not self.tfidf_word or not self.tfidf_char: raise ValueError(\"TF-IDF vectorizers not fitted.\")\n            print(\"Transforming text using fitted TF-IDF...\"); train_tfidf_word = self.tfidf_word.transform(train_df['answer'].fillna('')); test_tfidf_word = self.tfidf_word.transform(test_df['answer'].fillna(''))\n            train_tfidf_char = self.tfidf_char.transform(train_df['answer'].fillna('')); test_tfidf_char = self.tfidf_char.transform(test_df['answer'].fillna(''))\n        print(\"Combining all feature sets...\"); X_train_full = hstack([csr_matrix(train_features_scaled), train_tfidf_word, train_tfidf_char, csr_matrix(llm_train_feat)]).tocsr()\n        X_test_full = hstack([csr_matrix(test_features_scaled), test_tfidf_word, test_tfidf_char, csr_matrix(llm_test_feat)]).tocsr()\n        print(f\"Shape before feature selection: Train={X_train_full.shape}, Test={X_test_full.shape}\")\n        if is_training_fold or is_full_training:\n            y_train = train_df['is_cheating'].values; print(f\"Fitting SelectKBest (k={self.n_features})...\"); self.selector = SelectKBest(f_classif, k=min(self.n_features, X_train_full.shape[1]))\n            with warnings.catch_warnings(): warnings.filterwarnings('ignore'); self.selector.fit(X_train_full, y_train)\n            X_train_selected = self.selector.transform(X_train_full); X_test_selected = self.selector.transform(X_test_full)\n        else:\n            if not self.selector: raise ValueError(\"SelectKBest not fitted.\"); print(f\"Transforming features using fitted SelectKBest (k={self.n_features})...\")\n            X_train_selected = self.selector.transform(X_train_full); X_test_selected = self.selector.transform(X_test_full)\n        X_train_final = X_train_selected.toarray(); X_test_final = X_test_selected.toarray()\n        print(f\"Final training feature shape: {X_train_final.shape}\"); print(f\"Final testing feature shape: {X_test_final.shape}\")\n        return X_train_final, X_test_final\n    \n    def train_champion_ensemble(self, full_train_df):\n        \"\"\"Trains L1 models, then trains L2 Stack AND L2 Weighted Avg.\"\"\"\n        y_train = full_train_df['is_cheating'].values\n        print(f\"\\n--- Starting Classical Ensemble Training ---\"); print(f\"Training set size: {len(full_train_df)}\"); print(f\"Positive class ratio: {y_train.mean():.6f}\")\n        rskf = RepeatedStratifiedKFold(n_splits=self.n_splits, n_repeats=self.n_repeats, random_state=RANDOM_SEED)\n        oof_preds_lgb = np.zeros((len(full_train_df), self.n_repeats))\n        oof_preds_cat = np.zeros((len(full_train_df), self.n_repeats))\n        oof_preds_ridge = np.zeros((len(full_train_df), self.n_repeats))\n        print(\"Fitting topic columns on full training data...\"); _ = self._get_consistent_topic_dummies(full_train_df['topic'], fit_columns=True)\n        print(\"\\nFitting Scalers, TFIDF, and Selector on FULL training data...\")\n        _ = self.prepare_champion_features(full_train_df, full_train_df, is_training_fold=True, is_full_training=True)\n        print(\"\\nStarting Repeated K-Fold Training...\")\n        for fold, (train_idx, val_idx) in enumerate(tqdm(rskf.split(full_train_df, y_train), total=self.n_splits * self.n_repeats, desc=\"CV Folds (Model A)\")):\n            repeat = fold // self.n_splits; split = fold % self.n_splits\n            print(f\"\\n{'='*20} Repeat {repeat+1}/{self.n_repeats}, Split {split+1}/{self.n_splits} {'='*20}\")\n            train_fold_df, val_fold_df = full_train_df.iloc[train_idx], full_train_df.iloc[val_idx]\n            y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n            X_train_fold, X_val_fold = self.prepare_champion_features(train_fold_df, val_fold_df, is_training_fold=False, is_full_training=False)\n            print(f\"Fold {fold+1}: Train shape {X_train_fold.shape}, Val shape {X_val_fold.shape}\")\n            fold_seed = RANDOM_SEED + fold\n            print(\"Training LightGBM...\"); lgb_model = lgb.LGBMClassifier(n_estimators=2500, learning_rate=0.01, max_depth=7, num_leaves=25, min_child_samples=25, subsample=0.7, colsample_bytree=0.6, reg_alpha=0.6, reg_lambda=0.6, random_state=fold_seed, verbose=-1, n_jobs=-1) \n            lgb_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], callbacks=[lgb.early_stopping(200, verbose=False)]); oof_preds_lgb[val_idx, repeat] = lgb_model.predict_proba(X_val_fold)[:, 1]\n            print(\"Training CatBoost...\"); cat_model = CatBoostClassifier(iterations=2500, learning_rate=0.02, depth=6, l2_leaf_reg=8, random_seed=fold_seed, verbose=0, early_stopping_rounds=200, task_type=\"CPU\")\n            cat_model.fit(X_train_fold, y_train_fold, eval_set=(X_val_fold, y_val_fold), verbose=False); oof_preds_cat[val_idx, repeat] = cat_model.predict_proba(X_val_fold)[:, 1]\n            print(\"Training Ridge Classifier...\"); ridge_scaler = StandardScaler(); X_train_fold_scaled = ridge_scaler.fit_transform(X_train_fold); X_val_fold_scaled = ridge_scaler.transform(X_val_fold)\n            ridge_model = RidgeClassifier(alpha=2.0, random_state=fold_seed); ridge_model.fit(X_train_fold_scaled, y_train_fold); ridge_scores = ridge_model.decision_function(X_val_fold_scaled); ridge_probs = 1 / (1 + np.exp(-ridge_scores))\n            oof_preds_ridge[val_idx, repeat] = ridge_probs\n            fold_auc_lgb = roc_auc_score(y_val_fold, oof_preds_lgb[val_idx, repeat]); fold_auc_cat = roc_auc_score(y_val_fold, oof_preds_cat[val_idx, repeat]); fold_auc_ridge = roc_auc_score(y_val_fold, oof_preds_ridge[val_idx, repeat])\n            print(f\"Fold {fold+1} LGB AUC: {fold_auc_lgb:.6f}\"); print(f\"Fold {fold+1} CAT AUC: {fold_auc_cat:.6f}\"); print(f\"Fold {fold+1} RIDGE AUC: {fold_auc_ridge:.6f}\")\n        \n        print(\"\\n\" + \"=\"*50); print(\"Averaging OOF Scores across repeats:\")\n        oof_lgb_avg = np.mean(oof_preds_lgb, axis=1); oof_cat_avg = np.mean(oof_preds_cat, axis=1); oof_ridge_avg = np.mean(oof_preds_ridge, axis=1)\n        \n        oof_aucs = {\n            'lgb': roc_auc_score(y_train, oof_lgb_avg),\n            'cat': roc_auc_score(y_train, oof_cat_avg),\n            'ridge': roc_auc_score(y_train, oof_ridge_avg)\n        }\n        print(f\"  LGB OOF AUC (Averaged): {oof_aucs['lgb']:.8f}\")\n        print(f\"  CAT OOF AUC (Averaged): {oof_aucs['cat']:.8f}\")\n        print(f\"  RIDGE OOF AUC (Averaged): {oof_aucs['ridge']:.8f}\")\n        \n        total_auc = sum(oof_aucs.values())\n        self.l1_weights = {name: auc / total_auc for name, auc in oof_aucs.items()}\n        print(\"Calculated L1 Weights (for Model B):\")\n        for name, weight in self.l1_weights.items(): print(f\"  {name.upper()}: {weight:.4f}\")\n\n        oof_weighted_avg = (oof_lgb_avg * self.l1_weights['lgb'] + \n                            oof_cat_avg * self.l1_weights['cat'] + \n                            oof_ridge_avg * self.l1_weights['ridge'])\n        oof_auc_weighted = roc_auc_score(y_train, oof_weighted_avg)\n        print(f\"\\nL2 Weighted Avg OOF AUC (Model B): {oof_auc_weighted:.8f}\")\n\n        print(\"\\nTraining L2 Meta-Model (Model A)...\"); \n        X_meta_train = np.stack([oof_lgb_avg, oof_cat_avg, oof_ridge_avg], axis=1)\n        self.meta_model = LogisticRegression(C=0.1, solver='liblinear', random_state=RANDOM_SEED); \n        self.meta_model.fit(X_meta_train, y_train)\n        oof_stack_preds = self.meta_model.predict_proba(X_meta_train)[:, 1]\n        oof_auc_stack = roc_auc_score(y_train, oof_stack_preds)\n        print(f\"L2 Stack OOF AUC (Model A): {oof_auc_stack:.8f}\")\n        \n        oof_final_blend = 0.5 * oof_stack_preds + 0.5 * oof_weighted_avg\n        oof_auc_final_blend = roc_auc_score(y_train, oof_final_blend)\n        \n        print(f\"\\nFINAL L3 BLEND OOF AUC (Averaged): {oof_auc_final_blend:.8f}\"); \n        print(\"Training Final Isotonic Calibrator on L3 Blend...\"); \n        self.calibrator.fit(oof_final_blend, y_train) \n        \n        self.is_trained = True; \n        \n        print(\"\\nTraining Final L1 Models for Model A prediction...\")\n        X_train_full, _ = self.prepare_champion_features(full_train_df, full_train_df.iloc[:1], is_training_fold=False, is_full_training=True)\n        y_train_full = full_train_df['is_cheating'].values\n        \n        print(\"Training final LightGBM...\"); \n        self.final_l1_models['lgb'] = lgb.LGBMClassifier(n_estimators=2200, learning_rate=0.01, max_depth=7, num_leaves=25, min_child_samples=25, subsample=0.7, colsample_bytree=0.6, reg_alpha=0.6, reg_lambda=0.6, random_state=RANDOM_SEED, verbose=-1, n_jobs=-1) \n        self.final_l1_models['lgb'].fit(X_train_full, y_train_full)\n        \n        print(\"Training final CatBoost...\"); \n        self.final_l1_models['cat'] = CatBoostClassifier(iterations=2200, learning_rate=0.02, depth=6, l2_leaf_reg=8, random_state=RANDOM_SEED, verbose=0, task_type=\"CPU\")\n        self.final_l1_models['cat'].fit(X_train_full, y_train_full)\n        \n        print(\"Training final Ridge...\"); \n        ridge_scaler = StandardScaler(); \n        X_train_full_scaled = ridge_scaler.fit_transform(X_train_full)\n        self.final_l1_models['ridge_scaler'] = ridge_scaler\n        self.final_l1_models['ridge'] = RidgeClassifier(alpha=2.0, random_state=RANDOM_SEED)\n        self.final_l1_models['ridge'].fit(X_train_full_scaled, y_train_full)\n        \n        return oof_auc_final_blend, oof_final_blend\n\n    def predict_champion(self, test_df):\n        \"\"\"Generates final predictions using L3 Blend.\"\"\"\n        if not self.is_trained: raise ValueError(\"Classical model not trained!\")\n        print(\"\\n\" + \"=\"*50); print(\"Generating Final Classical Ensemble Predictions (Model A)...\"); print(\"=\"*50)\n        \n        dummy_train_df = test_df.iloc[:1].copy() \n        dummy_train_df['is_cheating'] = 0\n        \n        print(\"Preparing features for test data...\")\n        _, X_test = self.prepare_champion_features(dummy_train_df, test_df, is_training_fold=False, is_full_training=False)\n        \n        test_preds_l1 = {}\n        \n        print(\"Predicting with final LightGBM...\");\n        test_preds_l1['lgb'] = self.final_l1_models['lgb'].predict_proba(X_test)[:, 1]\n        \n        print(\"Predicting with final CatBoost...\");\n        test_preds_l1['cat'] = self.final_l1_models['cat'].predict_proba(X_test)[:, 1]\n        \n        print(\"Predicting with final Ridge...\");\n        X_test_scaled = self.final_l1_models['ridge_scaler'].transform(X_test)\n        ridge_scores = self.final_l1_models['ridge'].decision_function(X_test_scaled)\n        test_preds_l1['ridge'] = 1 / (1 + np.exp(-ridge_scores))\n        \n        print(\"Applying Classical L2 Meta-Model (Model A)...\"); \n        X_meta_test = np.stack([test_preds_l1['lgb'], test_preds_l1['cat'], test_preds_l1['ridge']], axis=1)\n        preds_stack_test = self.meta_model.predict_proba(X_meta_test)[:, 1]\n        \n        print(\"Applying Classical L2 Weighted Average (Model B)...\"); \n        preds_weighted_test = (test_preds_l1['lgb'] * self.l1_weights['lgb'] +\n                               test_preds_l1['cat'] * self.l1_weights['cat'] +\n                               test_preds_l1['ridge'] * self.l1_weights['ridge'])\n                               \n        print(\"Blending Model A and Model B (50/50)...\")\n        final_proba_uncalibrated = 0.5 * preds_stack_test + 0.5 * preds_weighted_test\n        \n        print(\"Applying Final Isotonic Calibration...\"); \n        calibrated_proba = self.calibrator.transform(final_proba_uncalibrated)\n        calibrated_proba = np.clip(calibrated_proba, 0.001, 0.999)\n        \n        return calibrated_proba, final_proba_uncalibrated\n\n\n# ===================================================================\n# END: MODEL A\n# ===================================================================\n\n\n# ===================================================================\n# START: MODEL B (User's Roberta-Large Notebook)\n# ===================================================================\n\n# --- Custom Prompt Function ---\ndef prompt(topic,answer):\n    return f'''Predict if AI generated text was used:\\nTopic:{topic}\\nAnswer:{answer}\\n    '''\n\n# --- FIXED: Custom Dataset Class ---\nclass getData(Dataset):\n    def __init__(self, df: pd.DataFrame, tokenizer, max_len=512, is_test=False):\n        self.data = df\n        self.texts = self.data.input.tolist()\n        self.is_test = is_test\n        # FIX: Only access is_cheating if not test data\n        if not is_test:\n            self.labels = self.data.is_cheating.tolist()\n        else:\n            self.labels = [0] * len(self.data)  # Dummy labels for test\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.data)\n        \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,       \n            truncation=True,               \n            return_token_type_ids=False,   \n            return_attention_mask=True,    \n        )\n        return {\n            'input_ids': encoding['input_ids'],\n            'attention_mask': encoding['attention_mask'],\n            'labels': label\n        }\n\n# --- KFold Loaders Function ---\ndef get_kfold_loaders(df: pd.DataFrame, tokenizer, batch_sizes: list, num_splits: int = 5, target: str = 'is_cheating', device=DEVICE) -> List[Tuple[DataLoader, DataLoader]]:\n    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=RANDOM_SEED)\n    X = df.drop(columns=[target]) \n    y = df[target] \n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    all_loaders = []\n    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n        print(f\"--- Fold {fold+1}/{num_splits} ---\")\n        train_df = df.iloc[train_index].reset_index(drop=True)\n        val_df = df.iloc[val_index].reset_index(drop=True)\n        weights = compute_class_weight('balanced', classes=np.array([0,1]), y=train_df.is_cheating)\n        train_dataset = getData(train_df, tokenizer, is_test=False)\n        val_dataset = getData(val_df, tokenizer, is_test=False)\n        train_loader = DataLoader(\n            train_dataset, batch_size=batch_sizes[0], shuffle=True,\n            collate_fn=data_collator, num_workers=2, pin_memory=True\n        )\n        val_loader = DataLoader(\n            val_dataset, batch_size=batch_sizes[1], shuffle=False,\n            collate_fn=data_collator, num_workers=2, pin_memory=True\n        )\n        weights_tensor = torch.tensor(weights, dtype=torch.float, device=device)\n        all_loaders.append((train_loader, val_loader, weights_tensor, val_index))\n    return all_loaders\n\n# --- Train/Eval Functions ---\ndef train_one_epoch(model, loader, optimizer, loss_fn, scheduler, DEVICE):\n    model.train()\n    total_loss = 0.0\n    for batch_idx, batch in enumerate(loader):\n        input_ids = batch['input_ids'].to(DEVICE)\n        attention_mask = batch['attention_mask'].to(DEVICE)\n        labels = batch['labels'].to(DEVICE).long()\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits \n        loss = loss_fn(logits, labels) \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)\n\ndef evaluate_one_epoch(model, loader, loss_fn, DEVICE):\n    model.eval()\n    total_loss = 0.0\n    all_labels = []\n    all_probabilities = []\n    all_predictions = []\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(loader):\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            labels = batch['labels'].to(DEVICE).long()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]\n            loss = loss_fn(logits, labels)\n            total_loss += loss.item()\n            probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n            _, predicted_classes = torch.max(logits, dim=1)\n            all_labels.extend(labels.cpu().numpy())\n            all_probabilities.extend(probabilities)\n            all_predictions.extend(predicted_classes.cpu().numpy())\n    avg_loss = total_loss / len(loader)\n    all_probabilities = np.array(all_probabilities)\n    auc_roc = roc_auc_score(all_labels, all_probabilities[:, 1])\n    acc = accuracy_score(all_labels, all_predictions)\n    return avg_loss, acc, auc_roc, all_probabilities\n\n# --- Main Training/Prediction Function for Model B ---\ndef get_roberta_large_predictions(train_df, test_df):\n    print(\"\\n\" + \"=\"*50); print(\"STARTING MODEL B (RoBERTa-Large 5-Fold) PIPELINE\"); print(\"=\"*50)\n    \n    LR = 3e-5\n    NUM_EPOCHS = 50\n    WARMUPS = 750\n    batches = [8, 16]\n    patience = 5\n    model_id = LLM_MODEL_NAME_B\n    \n    print(\"Applying custom prompt to Model B data...\")\n    train_df['input'] = train_df.apply(lambda row: prompt(row['topic'], row['answer']), axis=1)\n    test_df['input'] = test_df.apply(lambda row: prompt(row['topic'], row['answer']), axis=1)\n    \n    tokenizer = RobertaTokenizer.from_pretrained(model_id)\n    all_loaders = get_kfold_loaders(train_df, tokenizer, batches, num_splits=N_SPLITS_ROBERTA)\n    \n    print(\"Preparing Test Loader for Model B...\")\n    test_dataset = getData(test_df, tokenizer, is_test=True)  # FIX: Pass is_test=True\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    test_loader = DataLoader(\n        test_dataset, batch_size=batches[1], shuffle=False, \n        collate_fn=data_collator, num_workers=2, pin_memory=True\n    )\n    \n    oof_preds = np.zeros(len(train_df))\n    test_preds_all_folds = []\n    \n    for idx, (train_loader, val_loader, weights, val_index) in enumerate(tqdm(all_loaders, desc=\"CV Folds (Model B)\")):\n        torch.cuda.empty_cache()\n        print(f'--- Training Model B, Fold {idx+1}/{N_SPLITS_ROBERTA} ---')\n        model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n        model.to(DEVICE)\n\n        all_steps = len(train_loader) * NUM_EPOCHS\n        optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=WARMUPS, num_training_steps=all_steps)\n        loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n\n        best_metric = float('inf')\n        trigger = 0\n        \n        for epochs in trange(NUM_EPOCHS, desc=f\"Epochs Fold {idx+1}\"):\n            training_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, scheduler, DEVICE)\n            avg_val_loss, val_acc, aur_roc, val_probs = evaluate_one_epoch(model, val_loader, loss_fn, DEVICE)\n            print(f'EPOCH: {epochs+1}, TrainLoss: {training_loss:.4f}, ValLoss: {avg_val_loss:.4f}, AUC: {aur_roc:.4f}, Acc: {val_acc:.4f}')\n\n            if avg_val_loss < best_metric:\n                best_metric = avg_val_loss\n                trigger = 0\n                print(f'Saving Best Model B, Fold {idx+1}')\n                torch.save(model.state_dict(), f'Roberta_Fold{idx+1}.pth')\n                oof_preds[val_index] = val_probs[:, 1]\n            else:\n                trigger += 1\n            if trigger >= patience:\n                print('-----EARLY STOPPING-----')\n                break\n        \n        print(f\"Loading best Model B, Fold {idx+1} for test prediction...\")\n        model.load_state_dict(torch.load(f'Roberta_Fold{idx+1}.pth'))\n        model.eval()\n        fold_test_probs = []\n        with torch.no_grad():\n            for batch in tqdm(test_loader, desc=f\"Test Pred Fold {idx+1}\"):\n                input_ids = batch['input_ids'].to(DEVICE)\n                attention_mask = batch['attention_mask'].to(DEVICE)\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                logits = outputs.logits\n                probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n                fold_test_probs.append(probabilities)\n        test_preds_all_folds.append(np.concatenate(fold_test_probs, axis=0))\n\n    print(\"Averaging Model B test predictions across all 5 folds...\")\n    avg_test_preds = np.mean(np.stack(test_preds_all_folds, axis=0), axis=0)[:, 1]\n    \n    print(\"=\"*50); print(\"COMPLETED MODEL B PIPELINE\"); print(\"=\"*50)\n    return oof_preds, avg_test_preds\n\n# ===================================================================\n# END: MODEL B\n# ===================================================================\n\n\n# -------------------------------------------------------------------\n# 4. MAIN EXECUTION (Hybrid Model A + Model B)\n# -------------------------------------------------------------------\ndef champion_main():\n    \"\"\"Main function: Runs BOTH pipelines and blends them.\"\"\"\n    print(\"=\" * 60); print(f\"  Mercor AI Text Detection - CHAMPION v29 (Hybrid Monster)\"); print(\"=\" * 60)\n    train_df = pd.read_csv('/kaggle/input/mercor-ai-detection/train.csv'); \n    test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv')\n    print(f\"Training data shape: {train_df.shape}\"); print(f\"Test data shape: {test_df.shape}\")\n    train_df['answer'] = train_df['answer'].fillna(''); test_df['answer'] = test_df['answer'].fillna('')\n    \n    all_texts_A = pd.concat([train_df['answer'], test_df['answer']])\n    llm_preds_all_A = get_llm_predictions_A(all_texts_A, LLM_MODEL_NAME_A)\n    llm_oof_A = llm_preds_all_A[:len(train_df)]; llm_test_A = llm_preds_all_A[len(train_df):]\n    \n    print(\"\\nCalibrating Model A (Desklib) predictions...\"); \n    calibrator_A = IsotonicRegression(out_of_bounds='clip')\n    calibrator_A.fit(llm_oof_A, train_df['is_cheating'].values)\n    llm_oof_calibrated_A = calibrator_A.transform(llm_oof_A)\n    llm_test_calibrated_A = calibrator_A.transform(llm_test_A)\n    print(f\"Calibrated Model A OOF AUC: {roc_auc_score(train_df['is_cheating'], llm_oof_calibrated_A):.8f}\")\n    \n    train_df_A = train_df.copy()\n    test_df_A = test_df.copy()\n    train_df_A['llm_pred'] = llm_oof_calibrated_A\n    test_df_A['llm_pred'] = llm_test_calibrated_A\n    \n    classical_detector = ChampionAIDetector_A(n_splits=N_SPLITS_CLASSICAL, n_repeats=N_REPEATS_CLASSICAL, n_features=N_FEATURES_CLASSICAL)\n    oof_auc_A, oof_preds_A = classical_detector.train_champion_ensemble(train_df_A)\n    print(\"\\nStarting Final Test Set Prediction for Model A...\");\n    preds_A, _ = classical_detector.predict_champion(test_df_A)\n    \n    train_df_B = train_df.copy()\n    test_df_B = test_df.copy()\n    oof_preds_B_uncalibrated, test_preds_B_uncalibrated = get_roberta_large_predictions(train_df_B, test_df_B)\n    \n    print(\"\\nCalibrating Model B (RoBERTa) predictions...\"); \n    calibrator_B = IsotonicRegression(out_of_bounds='clip')\n    calibrator_B.fit(oof_preds_B_uncalibrated, train_df['is_cheating'].values)\n    oof_preds_B = calibrator_B.transform(oof_preds_B_uncalibrated)\n    preds_B = calibrator_B.transform(test_preds_B_uncalibrated)\n    print(f\"Calibrated Model B OOF AUC: {roc_auc_score(train_df['is_cheating'], oof_preds_B):.8f}\")\n    \n    print(\"\\n\" + \"=\"*50); print(\"Blending Model A and Model B (50/50)...\"); print(\"=\"*50)\n    final_blend_oof = 0.3 * oof_preds_A + 0.7 * oof_preds_B\n    final_blend_test = 0.3 * preds_A + 0.7 * preds_B\n    \n    final_oof_auc = roc_auc_score(train_df['is_cheating'], final_blend_oof)\n    \n    print(\"Training Final Isotonic Calibrator on the 50/50 blend...\"); \n    final_calibrator = IsotonicRegression(out_of_bounds='clip')\n    final_calibrator.fit(final_blend_oof, train_df['is_cheating'].values)\n    final_preds_calibrated = final_calibrator.transform(final_blend_test)\n    final_preds_calibrated = np.clip(final_preds_calibrated, 0.001, 0.999)\n    \n    submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': final_preds_calibrated})\n    submission.to_csv('submission.csv', index=False, float_format='%.10f')\n    \n    print(\"\\n\" + \"=\"*60); print(\"CHAMPION HYBRID PIPELINE COMPLETED SUCCESSFULLY!\");\n    print(f\"Model A (L3 Stack) OOF AUC: {oof_auc_A:.8f}\")\n    print(f\"Model B (RoBERTa) OOF AUC: {roc_auc_score(train_df['is_cheating'], oof_preds_B):.8f}\")\n    print(f\"FINAL L4 BLEND OOF AUC: {final_oof_auc:.8f}\")\n    print(\"\\nSubmission file created: submission.csv (RECOMMENDED)\")\n    print(f\"\\nFinal 'submission.csv' prediction statistics:\"); print(f\"  Min: {final_preds_calibrated.min():.6f}\"); print(f\"  Max: {final_preds_calibrated.max():.6f}\"); print(f\"  Mean: {final_preds_calibrated.mean():.6f}\"); print(f\"  Median: {np.median(final_preds_calibrated):.6f}\")\n    print(\"\\nTop 10 Predictions:\"); print(submission.head(10).to_string(index=False)); print(\"=\" * 60)\n\n# --- (Fallback model - simplified from Model A) ---\ndef champion_fallback():\n    \"\"\"Fallback using basic LGBM, basic features.\"\"\"\n    print(\"CRITICAL ERROR: Main pipeline failed. Using CHAMPION FALLBACK model...\")\n    try:\n        from sklearn.preprocessing import StandardScaler; from scipy.sparse import hstack, csr_matrix;\n        train_df = pd.read_csv('/kaggle/input/mercor-ai-detection/train.csv'); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv')\n        feature_extractor = ChampionFeatureExtractor(); train_base_features = feature_extractor.extract_base_features(train_df.drop(columns=['llm_pred'], errors='ignore'))\n        test_base_features = feature_extractor.extract_base_features(test_df.drop(columns=['llm_pred'], errors='ignore'))\n        train_topic_dummies = pd.get_dummies(train_df['topic'], prefix='topic', dtype=int); fallback_topic_columns = train_topic_dummies.columns; test_topic_dummies = pd.get_dummies(test_df['topic'], prefix='topic', dtype=int).reindex(columns=fallback_topic_columns, fill_value=0)\n        train_dense_features = pd.concat([train_base_features, train_topic_dummies], axis=1); test_dense_features = pd.concat([test_base_features, test_topic_dummies], axis=1).reindex(columns=train_dense_features.columns, fill_value=0)\n        scaler = StandardScaler(); train_dense_scaled = scaler.fit_transform(train_dense_features); test_dense_scaled = scaler.transform(test_dense_features)\n        tfidf = TfidfVectorizer(max_features=2500, ngram_range=(1, 3), stop_words='english'); train_tfidf = tfidf.fit_transform(train_df['answer'].fillna('')); test_tfidf = tfidf.transform(test_df['answer'].fillna(''))\n        X_train = hstack([csr_matrix(train_dense_scaled), train_tfidf]).tocsr(); X_test = hstack([csr_matrix(test_dense_scaled), test_tfidf]).tocsr(); y_train = train_df['is_cheating']\n        model = lgb.LGBMClassifier(n_estimators=1000, learning_rate=0.01, max_depth=7, num_leaves=63, random_state=RANDOM_SEED, verbose=-1, n_jobs=-1, reg_alpha=0.1, reg_lambda=0.1)\n        model.fit(X_train, y_train); test_proba = model.predict_proba(X_test.toarray())[:, 1]\n        submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': test_proba}); submission.to_csv('submission.csv', index=False, float_format='%.10f')\n        print(\"Fallback submission.csv created successfully.\"); return submission\n    except Exception as e:\n        print(f\"Fallback model ALSO failed: {e}\"); test_df = pd.read_csv('/kaggle/input/mercor-ai-detection/test.csv'); submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': 0.5}); submission.to_csv('submission.csv', index=False, float_format='%.10f'); print(\"CRITICAL: Created dummy submission.csv with 0.5 probability.\"); return submission\n\n# --- Run the Champion Pipeline ---\nif __name__ == \"__main__\":\n    start_time = time.time()\n    try:\n        gc.collect(); torch.cuda.empty_cache()\n        champion_main()\n    except Exception as e:\n        print(f\"\\n!!! AN ERROR OCCURRED IN THE MAIN PIPELINE !!!: {e}\")\n        import traceback; traceback.print_exc()\n        champion_fallback()\n    finally:\n        gc.collect(); torch.cuda.empty_cache(); end_time = time.time()\n        print(f\"\\nTotal execution time: {(end_time - start_time) / 60:.2f} minutes\")","metadata":{"execution":{"iopub.status.busy":"2025-11-15T05:17:07.687380Z","iopub.execute_input":"2025-11-15T05:17:07.687777Z","iopub.status.idle":"2025-11-15T06:28:12.036053Z","shell.execute_reply.started":"2025-11-15T05:17:07.687749Z","shell.execute_reply":"2025-11-15T06:28:12.035052Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Installing/Updating transformers...\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 4.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 86.9 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 64.8 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 48.0 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 6.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 30.8 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 13.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 1.9 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 70.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 566.1/566.1 kB 31.9 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"Libraries installed.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-15 05:18:41.713318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763183921.930672      39 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763183921.994350      39 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"All libraries imported successfully!\n============================================================\n  Mercor AI Text Detection - CHAMPION v29 (Hybrid Monster)\n============================================================\nTraining data shape: (269, 4)\nTest data shape: (264, 3)\n\n--- Starting LLM Inference (Model A: desklib/ai-text-detector-v1.01) ---\nUsing device: cuda\nLoading Tokenizer: desklib/ai-text-detector-v1.01...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddfc3017e134471697d7f73dad7d089e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe5e7cd474274c0ebe22bd6065c6afcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"709becd5999f4c43afb8b86aa669593b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d06441b952c40fc9971d2bd6d1546aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/286 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"004b21bd7e8b4e7bab7205e36655d1a7"}},"metadata":{}},{"name":"stdout","text":"Loading Model (Custom Class): desklib/ai-text-detector-v1.01...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8e04d4832f0412db49d25b22ea8f28d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdd01d88ccf940cea17bba0bce0abc18"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model and Tokenizer loaded successfully.\nRunning LLM inference on 533 texts...\nUsing batch size: 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"LLM 1 (ai-text-detector-v1.01):   0%|          | 0/34 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13dfc6310e314a78ad160a2374546c29"}},"metadata":{}},{"name":"stdout","text":"LLM 1 (ai-text-detector-v1.01) prediction feature generated.\n--- Finished LLM 1 Inference ---\n\nCalibrating Model A (Desklib) predictions...\nCalibrated Model A OOF AUC: 0.96113527\n\nClassical Detector (L3 Ensemble): 5 splits, 3 repeats, SelectKBest(k=850).\n\n--- Starting Classical Ensemble Training ---\nTraining set size: 269\nPositive class ratio: 0.546468\nFitting topic columns on full training data...\nIdentified 268 topic columns during fit.\n\nFitting Scalers, TFIDF, and Selector on FULL training data...\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nIdentified 268 topic columns during fit.\nFitting/Transforming dense features (excl. LLM)...\nFitting Word TF-IDF...\nFitting Char TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(269, 3683), Test=(269, 3683)\nFitting SelectKBest (k=850)...\nFinal training feature shape: (269, 850)\nFinal testing feature shape: (269, 850)\n\nStarting Repeated K-Fold Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"CV Folds (Model A):   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4747f36232543c49867cd9bc7e8a9c4"}},"metadata":{}},{"name":"stdout","text":"\n==================== Repeat 1/3, Split 1/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 1: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 1 LGB AUC: 0.995833\nFold 1 CAT AUC: 0.998611\nFold 1 RIDGE AUC: 0.986111\n\n==================== Repeat 1/3, Split 2/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 2: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 2 LGB AUC: 1.000000\nFold 2 CAT AUC: 0.998611\nFold 2 RIDGE AUC: 0.998611\n\n==================== Repeat 1/3, Split 3/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 3: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 3 LGB AUC: 0.966897\nFold 3 CAT AUC: 0.954483\nFold 3 RIDGE AUC: 0.983448\n\n==================== Repeat 1/3, Split 4/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 4: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 4 LGB AUC: 0.977931\nFold 4 CAT AUC: 0.984828\nFold 4 RIDGE AUC: 0.976552\n\n==================== Repeat 1/3, Split 5/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(216, 3683), Test=(53, 3683)\nFinal training feature shape: (216, 850)\nFinal testing feature shape: (53, 850)\nFold 5: Train shape (216, 850), Val shape (53, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 5 LGB AUC: 0.988506\nFold 5 CAT AUC: 0.991379\nFold 5 RIDGE AUC: 0.979885\n\n==================== Repeat 2/3, Split 1/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 6: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 6 LGB AUC: 1.000000\nFold 6 CAT AUC: 1.000000\nFold 6 RIDGE AUC: 0.987500\n\n==================== Repeat 2/3, Split 2/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 7: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 7 LGB AUC: 1.000000\nFold 7 CAT AUC: 1.000000\nFold 7 RIDGE AUC: 0.983333\n\n==================== Repeat 2/3, Split 3/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 8: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 8 LGB AUC: 0.965517\nFold 8 CAT AUC: 0.968276\nFold 8 RIDGE AUC: 0.986207\n\n==================== Repeat 2/3, Split 4/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 9: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 9 LGB AUC: 0.953103\nFold 9 CAT AUC: 0.936552\nFold 9 RIDGE AUC: 0.925517\n\n==================== Repeat 2/3, Split 5/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(216, 3683), Test=(53, 3683)\nFinal training feature shape: (216, 850)\nFinal testing feature shape: (53, 850)\nFold 10: Train shape (216, 850), Val shape (53, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 10 LGB AUC: 1.000000\nFold 10 CAT AUC: 1.000000\nFold 10 RIDGE AUC: 0.995690\n\n==================== Repeat 3/3, Split 1/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 11: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 11 LGB AUC: 0.973611\nFold 11 CAT AUC: 0.979167\nFold 11 RIDGE AUC: 0.995833\n\n==================== Repeat 3/3, Split 2/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 12: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 12 LGB AUC: 0.997222\nFold 12 CAT AUC: 0.988889\nFold 12 RIDGE AUC: 0.986111\n\n==================== Repeat 3/3, Split 3/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 13: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 13 LGB AUC: 0.947586\nFold 13 CAT AUC: 0.932414\nFold 13 RIDGE AUC: 0.947586\n\n==================== Repeat 3/3, Split 4/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(215, 3683), Test=(54, 3683)\nFinal training feature shape: (215, 850)\nFinal testing feature shape: (54, 850)\nFold 14: Train shape (215, 850), Val shape (54, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 14 LGB AUC: 0.998621\nFold 14 CAT AUC: 0.997241\nFold 14 RIDGE AUC: 1.000000\n\n==================== Repeat 3/3, Split 5/5 ====================\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(216, 3683), Test=(53, 3683)\nFinal training feature shape: (216, 850)\nFinal testing feature shape: (53, 850)\nFold 15: Train shape (216, 850), Val shape (53, 850)\nTraining LightGBM...\nTraining CatBoost...\nTraining Ridge Classifier...\nFold 15 LGB AUC: 0.981322\nFold 15 CAT AUC: 0.988506\nFold 15 RIDGE AUC: 0.975575\n\n==================================================\nAveraging OOF Scores across repeats:\n  LGB OOF AUC (Averaged): 0.98566968\n  CAT OOF AUC (Averaged): 0.98522360\n  RIDGE OOF AUC (Averaged): 0.98461024\nCalculated L1 Weights (for Model B):\n  LGB: 0.3335\n  CAT: 0.3334\n  RIDGE: 0.3331\n\nL2 Weighted Avg OOF AUC (Model B): 0.98840192\n\nTraining L2 Meta-Model (Model A)...\nL2 Stack OOF AUC (Model A): 0.98728672\n\nFINAL L3 BLEND OOF AUC (Averaged): 0.98790008\nTraining Final Isotonic Calibrator on L3 Blend...\n\nTraining Final L1 Models for Model A prediction...\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nIdentified 268 topic columns during fit.\nFitting/Transforming dense features (excl. LLM)...\nFitting Word TF-IDF...\nFitting Char TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(269, 3683), Test=(1, 3683)\nFitting SelectKBest (k=850)...\nFinal training feature shape: (269, 850)\nFinal testing feature shape: (1, 850)\nTraining final LightGBM...\nTraining final CatBoost...\nTraining final Ridge...\n\nStarting Final Test Set Prediction for Model A...\n\n==================================================\nGenerating Final Classical Ensemble Predictions (Model A)...\n==================================================\nPreparing features for test data...\nExtracting base handcrafted + LLM features...\nProcessing topic features...\nTransforming dense features (excl. LLM)...\nTransforming text using fitted TF-IDF...\nCombining all feature sets...\nShape before feature selection: Train=(1, 3683), Test=(264, 3683)\nFinal training feature shape: (1, 850)\nFinal testing feature shape: (264, 850)\nPredicting with final LightGBM...\nPredicting with final CatBoost...\nPredicting with final Ridge...\nApplying Classical L2 Meta-Model (Model A)...\nApplying Classical L2 Weighted Average (Model B)...\nBlending Model A and Model B (50/50)...\nApplying Final Isotonic Calibration...\n\n==================================================\nSTARTING MODEL B (RoBERTa-Large 5-Fold) PIPELINE\n==================================================\nApplying custom prompt to Model B data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb68e74a95ca4e1bb4809da5bcefb9c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d3a296de5ea4312853b91ec5e4f3c1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47e34dc92de24cb6b1510a58787c404c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57e6bad48ab64bf79a472327679580a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60e145e5036e43debd82d1b2999c8d46"}},"metadata":{}},{"name":"stdout","text":"--- Fold 1/5 ---\n--- Fold 2/5 ---\n--- Fold 3/5 ---\n--- Fold 4/5 ---\n--- Fold 5/5 ---\nPreparing Test Loader for Model B...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"CV Folds (Model B):   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db467b5447eb4f7aad7312ddaec1b87d"}},"metadata":{}},{"name":"stdout","text":"--- Training Model B, Fold 1/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7e3ad2cf9cf4b2fbe77feb612712539"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epochs Fold 1:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a234ff8eece4d42ace8a699a0286210"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 1, TrainLoss: 0.7563, ValLoss: 0.6891, AUC: 0.8222, Acc: 0.4444\nSaving Best Model B, Fold 1\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 2, TrainLoss: 0.6367, ValLoss: 0.5104, AUC: 0.9847, Acc: 0.7778\nSaving Best Model B, Fold 1\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 3, TrainLoss: 0.4460, ValLoss: 0.2720, AUC: 0.9972, Acc: 0.9815\nSaving Best Model B, Fold 1\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 4, TrainLoss: 0.2605, ValLoss: 0.1205, AUC: 1.0000, Acc: 0.9444\nSaving Best Model B, Fold 1\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 5, TrainLoss: 0.0950, ValLoss: 0.1482, AUC: 1.0000, Acc: 0.9444\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 6, TrainLoss: 0.0443, ValLoss: 0.3664, AUC: 1.0000, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 7, TrainLoss: 0.0317, ValLoss: 0.0255, AUC: 1.0000, Acc: 1.0000\nSaving Best Model B, Fold 1\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 8, TrainLoss: 0.0219, ValLoss: 0.0692, AUC: 1.0000, Acc: 0.9815\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 9, TrainLoss: 0.0040, ValLoss: 0.4603, AUC: 1.0000, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 10, TrainLoss: 0.0016, ValLoss: 0.4359, AUC: 1.0000, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 11, TrainLoss: 0.0013, ValLoss: 0.4688, AUC: 1.0000, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 12, TrainLoss: 0.0014, ValLoss: 0.4423, AUC: 1.0000, Acc: 0.9259\n-----EARLY STOPPING-----\nLoading best Model B, Fold 1 for test prediction...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Pred Fold 1:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"828068e5ab93499bafab35f43d664a98"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--- Training Model B, Fold 2/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs Fold 2:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"249e9d0642a848f29d143371d38354e0"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 1, TrainLoss: 0.6904, ValLoss: 0.6649, AUC: 0.8403, Acc: 0.7593\nSaving Best Model B, Fold 2\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 2, TrainLoss: 0.6205, ValLoss: 0.5199, AUC: 0.9500, Acc: 0.7963\nSaving Best Model B, Fold 2\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 3, TrainLoss: 0.4320, ValLoss: 0.2108, AUC: 0.9875, Acc: 0.9444\nSaving Best Model B, Fold 2\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 4, TrainLoss: 0.1853, ValLoss: 0.1031, AUC: 0.9917, Acc: 0.9630\nSaving Best Model B, Fold 2\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 5, TrainLoss: 0.0909, ValLoss: 0.1119, AUC: 0.9917, Acc: 0.9630\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 6, TrainLoss: 0.0414, ValLoss: 0.1677, AUC: 0.9889, Acc: 0.9074\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 7, TrainLoss: 0.0225, ValLoss: 0.1274, AUC: 0.9972, Acc: 0.9630\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 8, TrainLoss: 0.1270, ValLoss: 0.2169, AUC: 0.9833, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 9, TrainLoss: 0.0846, ValLoss: 0.0929, AUC: 0.9944, Acc: 0.9630\nSaving Best Model B, Fold 2\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 10, TrainLoss: 0.0070, ValLoss: 0.2279, AUC: 0.9917, Acc: 0.9074\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 11, TrainLoss: 0.0021, ValLoss: 0.3117, AUC: 0.9931, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 12, TrainLoss: 0.0022, ValLoss: 0.3073, AUC: 0.9917, Acc: 0.9074\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 13, TrainLoss: 0.0008, ValLoss: 0.3082, AUC: 0.9917, Acc: 0.9074\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 14, TrainLoss: 0.0006, ValLoss: 0.3150, AUC: 0.9917, Acc: 0.9074\n-----EARLY STOPPING-----\nLoading best Model B, Fold 2 for test prediction...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Pred Fold 2:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"297e5f883829460dac14b93df926278a"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--- Training Model B, Fold 3/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs Fold 3:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ad72204303646fbbe171bdded51d0d2"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 1, TrainLoss: 0.6943, ValLoss: 0.6794, AUC: 0.8290, Acc: 0.7222\nSaving Best Model B, Fold 3\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 2, TrainLoss: 0.6532, ValLoss: 0.5463, AUC: 0.9655, Acc: 0.9074\nSaving Best Model B, Fold 3\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 3, TrainLoss: 0.4438, ValLoss: 0.2724, AUC: 0.9903, Acc: 0.9444\nSaving Best Model B, Fold 3\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 4, TrainLoss: 0.1886, ValLoss: 0.2088, AUC: 0.9876, Acc: 0.9444\nSaving Best Model B, Fold 3\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 5, TrainLoss: 0.0801, ValLoss: 0.3332, AUC: 0.9862, Acc: 0.8704\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 6, TrainLoss: 0.0150, ValLoss: 0.4128, AUC: 0.9876, Acc: 0.8519\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 7, TrainLoss: 0.0053, ValLoss: 0.3632, AUC: 0.9876, Acc: 0.9444\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 8, TrainLoss: 0.0025, ValLoss: 0.3926, AUC: 0.9890, Acc: 0.9259\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 9, TrainLoss: 0.0017, ValLoss: 0.4326, AUC: 0.9890, Acc: 0.9259\n-----EARLY STOPPING-----\nLoading best Model B, Fold 3 for test prediction...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Pred Fold 3:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b52e8e033c4a461b955493fdcc44b7f3"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--- Training Model B, Fold 4/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs Fold 4:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2b49a73c3ad449e81cefb4d4b95c7fd"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 1, TrainLoss: 0.7165, ValLoss: 0.6600, AUC: 0.8538, Acc: 0.6111\nSaving Best Model B, Fold 4\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 2, TrainLoss: 0.6066, ValLoss: 0.4410, AUC: 0.9834, Acc: 0.9259\nSaving Best Model B, Fold 4\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 3, TrainLoss: 0.4591, ValLoss: 0.3310, AUC: 0.9890, Acc: 0.9444\nSaving Best Model B, Fold 4\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 4, TrainLoss: 0.2927, ValLoss: 0.1734, AUC: 0.9917, Acc: 0.9630\nSaving Best Model B, Fold 4\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 5, TrainLoss: 0.1443, ValLoss: 0.1059, AUC: 0.9876, Acc: 0.9444\nSaving Best Model B, Fold 4\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 6, TrainLoss: 0.1039, ValLoss: 0.0637, AUC: 0.9959, Acc: 0.9630\nSaving Best Model B, Fold 4\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 7, TrainLoss: 0.0559, ValLoss: 0.2393, AUC: 0.9931, Acc: 0.9074\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 8, TrainLoss: 0.0203, ValLoss: 0.1074, AUC: 0.9890, Acc: 0.9630\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 9, TrainLoss: 0.0035, ValLoss: 0.0800, AUC: 0.9945, Acc: 0.9815\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 10, TrainLoss: 0.0025, ValLoss: 0.1197, AUC: 0.9903, Acc: 0.9630\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 11, TrainLoss: 0.0015, ValLoss: 0.1064, AUC: 0.9890, Acc: 0.9815\n-----EARLY STOPPING-----\nLoading best Model B, Fold 4 for test prediction...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Pred Fold 4:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d88ff739c5c4eca9e66a29e20be4935"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--- Training Model B, Fold 5/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs Fold 5:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6122f69ed85e4386a9fc531e95028af0"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 1, TrainLoss: 0.7055, ValLoss: 0.6791, AUC: 0.6336, Acc: 0.4528\nSaving Best Model B, Fold 5\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 2, TrainLoss: 0.6398, ValLoss: 0.6006, AUC: 0.9540, Acc: 0.6415\nSaving Best Model B, Fold 5\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 3, TrainLoss: 0.4589, ValLoss: 0.2776, AUC: 0.9612, Acc: 0.9245\nSaving Best Model B, Fold 5\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 4, TrainLoss: 0.2077, ValLoss: 0.1296, AUC: 0.9842, Acc: 0.9623\nSaving Best Model B, Fold 5\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 5, TrainLoss: 0.1167, ValLoss: 0.1163, AUC: 0.9928, Acc: 0.9623\nSaving Best Model B, Fold 5\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 6, TrainLoss: 0.0548, ValLoss: 0.1275, AUC: 0.9871, Acc: 0.9623\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 7, TrainLoss: 0.0497, ValLoss: 0.1849, AUC: 0.9641, Acc: 0.9245\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 8, TrainLoss: 0.0199, ValLoss: 0.2004, AUC: 0.9871, Acc: 0.9434\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 9, TrainLoss: 0.0404, ValLoss: 0.3694, AUC: 0.9871, Acc: 0.9057\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"EPOCH: 10, TrainLoss: 0.0257, ValLoss: 0.1822, AUC: 0.9828, Acc: 0.9434\n-----EARLY STOPPING-----\nLoading best Model B, Fold 5 for test prediction...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test Pred Fold 5:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"397d9b7477ae4f2a8821e2ce165f6777"}},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Averaging Model B test predictions across all 5 folds...\n==================================================\nCOMPLETED MODEL B PIPELINE\n==================================================\n\nCalibrating Model B (RoBERTa) predictions...\nCalibrated Model B OOF AUC: 0.99620832\n\n==================================================\nBlending Model A and Model B (50/50)...\n==================================================\nTraining Final Isotonic Calibrator on the 50/50 blend...\n\n============================================================\nCHAMPION HYBRID PIPELINE COMPLETED SUCCESSFULLY!\nModel A (L3 Stack) OOF AUC: 0.98790008\nModel B (RoBERTa) OOF AUC: 0.99620832\nFINAL L4 BLEND OOF AUC: 0.99654288\n\nSubmission file created: submission.csv (RECOMMENDED)\n\nFinal 'submission.csv' prediction statistics:\n  Min: 0.001000\n  Max: 0.999000\n  Mean: 0.544941\n  Median: 0.916667\n\nTop 10 Predictions:\n              id  is_cheating\nscr_81822029c661        0.400\nscr_52efb19e0ea9        0.999\nscr_8fc0f33c559e        0.400\nscr_bac3f5d3aa12        0.001\nscr_adfbe009984d        0.001\nscr_9e08ece19277        0.999\nscr_0e34514f3cd4        0.999\nscr_b10d808b5528        0.999\nscr_2024f1e7bf94        0.400\nscr_aa0b11f10fff        0.001\n============================================================\n\nTotal execution time: 69.29 minutes\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}